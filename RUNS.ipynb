{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "S_8UqOlTZ95Y",
        "Sj3k7gEfaFIz"
      ],
      "history_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-xmaywqZB2-",
        "outputId": "bc991917-7c6b-4bb7-bc06-3fcbc47bf1e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/FinalProjectGNN/NAGphormer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jabhxP8OZToD",
        "outputId": "7b3548cf-9285-4aa2-f0bc-7e6b4555ad1a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ogb\n",
        "!pip install dgl==0.9\n",
        "!pip install pytorch==1.11"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FL6jUar_btYV",
        "outputId": "083e9487-0afc-48ec-915b-6db7ac0a8c8c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ogb in /usr/local/lib/python3.10/dist-packages (1.3.6)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.1.0+cu118)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.23.5)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (4.66.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.2.2)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.5.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.0.7)\n",
            "Requirement already satisfied: outdated>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (0.2.2)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (67.7.2)\n",
            "Requirement already satisfied: littleutils in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (0.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (2.31.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2023.3.post1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->ogb) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n",
            "Requirement already satisfied: dgl==0.9 in /usr/local/lib/python3.10/dist-packages (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl==0.9) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl==0.9) (1.11.4)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl==0.9) (3.2.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl==0.9) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl==0.9) (4.66.1)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl==0.9) (5.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl==0.9) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl==0.9) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl==0.9) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl==0.9) (2023.11.17)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pytorch==1.11 (from versions: 0.1.2, 1.0.2)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pytorch==1.11\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --dataset pubmed --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 7  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack dice --attperc 60 --TrainTest train --weight_decay=1e-05\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doZDv4ri_E-H",
        "outputId": "7d550ec0-ac7f-4693-b672-61d3f8b51a35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "from dgl.data import CoraGraphDataset, CiteseerGraphDataset, PubmedGraphDataset\n",
        "\n",
        "graph1 = CoraGraphDataset()[0]\n",
        "graph2 = CiteseerGraphDataset()[0]\n",
        "graph3 = PubmedGraphDataset()[0]\n",
        "import dgl\n",
        "import numpy as np\n",
        "\n",
        "def find_hub_nodes(graph, threshold=0.90):\n",
        "    centrality_scores = graph.in_degrees().float().numpy()\n",
        "    centrality_scores /= np.sum(centrality_scores)\n",
        "    threshold *= np.mean(centrality_scores)\n",
        "    hub_nodes = [node for node, centrality in enumerate(centrality_scores) if centrality > threshold]\n",
        "    return len(hub_nodes)/graph.num_nodes()\n",
        "\n",
        "def calculate_other_statistics(graph):\n",
        "    num_nodes = graph.num_nodes()\n",
        "    num_edges = graph.num_edges()\n",
        "    avg_degree = num_edges / num_nodes  # Assuming a homogeneous graph\n",
        "    density = num_edges / (num_nodes * (num_nodes - 1)) * 100\n",
        "\n",
        "    return {\n",
        "        'Number of Nodes': num_nodes,\n",
        "        'Number of Edges': num_edges,\n",
        "        'Average Degree': avg_degree,\n",
        "        'Density': density\n",
        "    }\n",
        "\n",
        "# Convert DGLHeteroGraph to DGLGraph\n",
        "graph1 = dgl.to_homogeneous(graph1)\n",
        "# Example usage:\n",
        "hub_nodes_1 = find_hub_nodes(graph1)\n",
        "statistics_1 = calculate_other_statistics(graph1)\n",
        "# Print or use the results as needed\n",
        "print(\"Graph 1 Hub Nodes:\", hub_nodes_1)\n",
        "print(\"Graph 1 Statistics:\", statistics_1)\n",
        "\n",
        "graph2 = dgl.to_homogeneous(graph2)\n",
        "hub_nodes_2 = find_hub_nodes(graph2)\n",
        "statistics_2 = calculate_other_statistics(graph2)\n",
        "print(\"Graph 2 Hub Nodes:\", hub_nodes_2)\n",
        "print(\"Graph 2 Statistics:\", statistics_2)\n",
        "\n",
        "graph3 = dgl.to_homogeneous(graph3)\n",
        "hub_nodes_3 = find_hub_nodes(graph3)\n",
        "statistics_3 = calculate_other_statistics(graph3)\n",
        "print(\"Graph 3 Hub Nodes:\", hub_nodes_3)\n",
        "print(\"Graph 3 Statistics:\", statistics_3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4H1AZVWR3TLj",
        "outputId": "8a248d2e-8064-43a7-9e68-d0492ce3fa22"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "  NumNodes: 3327\n",
            "  NumEdges: 9228\n",
            "  NumFeats: 3703\n",
            "  NumClasses: 6\n",
            "  NumTrainingSamples: 120\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "Graph 1 Hub Nodes: 0.40140324963072377\n",
            "Graph 1 Statistics: {'Number of Nodes': 2708, 'Number of Edges': 10556, 'Average Degree': 3.8980797636632203, 'Density': 0.14399999126942079}\n",
            "Graph 2 Hub Nodes: 0.35166816952209196\n",
            "Graph 2 Statistics: {'Number of Nodes': 3327, 'Number of Edges': 9228, 'Average Degree': 2.7736699729486025, 'Density': 0.08339356503152744}\n",
            "Graph 3 Hub Nodes: 0.24182177816097783\n",
            "Graph 3 Statistics: {'Number of Nodes': 19717, 'Number of Edges': 88651, 'Average Degree': 4.496170817061419, 'Density': 0.022804680549104377}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(graph.ndata['feat']).sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hkqcXHKnxLS",
        "outputId": "e91965d3-a9e6-4671-9395-9e0de8dc0047"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2708.)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "graph = CoraGraphDataset()[0]\n",
        "adj=graph.adj()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "eyhkoZfSnigE",
        "outputId": "a2570424-0413-4a9b-b1ca-990fa2fd81b3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-88c29da43b00>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoraGraphDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'CoraGraphDataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(graph.adj()-adj).sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYgoD90BoOSy",
        "outputId": "cdb45c3c-3d38-468b-ac32-1b5aa409b1a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.)"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import utils\n",
        "from dgl.data import CoraGraphDataset, CiteseerGraphDataset, PubmedGraphDataset\n",
        "import torch\n",
        "import dgl"
      ],
      "metadata": {
        "id": "saZUGCPjfuCU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=\"cora\"\n",
        "file_path = \"dataset/\"+dataset+\".pt\"\n",
        "data_list = torch.load(file_path)\n",
        "\n",
        "# data_list = [adj, features, labels, idx_train, idx_val, idx_test]\n",
        "adj = data_list[0]\n",
        "features = data_list[1]\n",
        "labels = data_list[2]\n",
        "\n",
        "idx_train = data_list[3]\n",
        "idx_val = data_list[4]\n",
        "idx_test = data_list[5]\n",
        "\n",
        "graph = CoraGraphDataset()[0]\n",
        "print(graph)\n",
        "graph = dgl.to_bidirected(graph)\n",
        "print(graph)\n",
        "\n",
        "lpe = utils.laplacian_positional_encoding(graph, 15)\n",
        "\n",
        "features = torch.cat((features, lpe), dim=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZE_IlTCfW7U",
        "outputId": "2f7d4062-0c9d-45e7-9716-566097a9d10a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "Graph(num_nodes=2708, num_edges=10556,\n",
            "      ndata_schemes={'feat': Scheme(shape=(1433,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64), 'test_mask': Scheme(shape=(), dtype=torch.bool), 'val_mask': Scheme(shape=(), dtype=torch.bool), 'train_mask': Scheme(shape=(), dtype=torch.bool)}\n",
            "      edata_schemes={})\n",
            "Graph(num_nodes=2708, num_edges=10556,\n",
            "      ndata_schemes={}\n",
            "      edata_schemes={})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "eZW8ow436rF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzWeXAEmlAq2",
        "outputId": "a8a32b89-cb02-4884-ff5d-e2645f910d7f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Graph(num_nodes=2708, num_edges=10556,\n",
              "      ndata_schemes={}\n",
              "      edata_schemes={})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(idx_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Pqf0FfEkZp4",
        "outputId": "07d1ce8e-9791-4c90-f2bf-e1666a738e0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1626"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "graph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZmvbFF0keH5",
        "outputId": "d91ea053-8a60-4928-ca2e-9e89ecdaf3f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Graph(num_nodes=2708, num_edges=10556,\n",
              "      ndata_schemes={}\n",
              "      edata_schemes={})"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "8pXXjCKW7uo-",
        "outputId": "d31dd904-bcec-46ef-df54-6d2ea681b44d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot open directory '.': Transport endpoint is not connected\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --dataset citeseer --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 7  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack dice --attperc 20 --TrainTest train --weight_decay=1e-05"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrYuKcd_Zcx8",
        "outputId": "5d740852-74b5-43bc-c846-04120a34c404"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading /root/.dgl/citeseer.zip from https://data.dgl.ai/dataset/citeseer.zip...\n",
            "Extracting file to /root/.dgl/citeseer\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/data/citation_graph.py:287: RuntimeWarning: divide by zero encountered in power\n",
            "  r_inv = np.power(rowsum, -1).flatten()\n",
            "Finished data loading and preprocessing.\n",
            "  NumNodes: 3327\n",
            "  NumEdges: 9228\n",
            "  NumFeats: 3703\n",
            "  NumClasses: 6\n",
            "  NumTrainingSamples: 120\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done saving data into cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:92: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=3718, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=6, bias=True)\n",
            ")\n",
            "total params: 4141832\n",
            "Epoch: 0001 loss_train: 1.8196 acc_train: 0.0896 loss_val: 1.8128 acc_val: 0.0781\n",
            "Epoch: 0002 loss_train: 1.8157 acc_train: 0.0986 loss_val: 1.8048 acc_val: 0.1471\n",
            "Epoch: 0003 loss_train: 1.8082 acc_train: 0.1237 loss_val: 1.7939 acc_val: 0.2102\n",
            "Epoch: 0004 loss_train: 1.7984 acc_train: 0.1923 loss_val: 1.7813 acc_val: 0.2102\n",
            "Epoch: 0005 loss_train: 1.7866 acc_train: 0.2093 loss_val: 1.7684 acc_val: 0.2102\n",
            "Epoch: 0006 loss_train: 1.7748 acc_train: 0.2113 loss_val: 1.7570 acc_val: 0.2102\n",
            "Epoch: 0007 loss_train: 1.7621 acc_train: 0.2133 loss_val: 1.7484 acc_val: 0.2102\n",
            "Epoch: 0008 loss_train: 1.7544 acc_train: 0.2133 loss_val: 1.7423 acc_val: 0.2102\n",
            "Epoch: 0009 loss_train: 1.7481 acc_train: 0.2138 loss_val: 1.7388 acc_val: 0.2147\n",
            "Epoch: 0010 loss_train: 1.7436 acc_train: 0.2238 loss_val: 1.7373 acc_val: 0.2553\n",
            "Epoch: 0011 loss_train: 1.7401 acc_train: 0.2509 loss_val: 1.7358 acc_val: 0.3498\n",
            "Epoch: 0012 loss_train: 1.7399 acc_train: 0.2699 loss_val: 1.7335 acc_val: 0.3664\n",
            "Epoch: 0013 loss_train: 1.7376 acc_train: 0.2834 loss_val: 1.7300 acc_val: 0.3844\n",
            "Epoch: 0014 loss_train: 1.7369 acc_train: 0.2914 loss_val: 1.7249 acc_val: 0.3874\n",
            "Epoch: 0015 loss_train: 1.7334 acc_train: 0.3170 loss_val: 1.7179 acc_val: 0.3784\n",
            "Epoch: 0016 loss_train: 1.7306 acc_train: 0.3420 loss_val: 1.7085 acc_val: 0.3589\n",
            "Epoch: 0017 loss_train: 1.7227 acc_train: 0.3455 loss_val: 1.6958 acc_val: 0.3288\n",
            "Epoch: 0018 loss_train: 1.7140 acc_train: 0.3565 loss_val: 1.6792 acc_val: 0.3213\n",
            "Epoch: 0019 loss_train: 1.7027 acc_train: 0.3400 loss_val: 1.6574 acc_val: 0.3318\n",
            "Epoch: 0020 loss_train: 1.6869 acc_train: 0.3495 loss_val: 1.6285 acc_val: 0.3559\n",
            "Epoch: 0021 loss_train: 1.6689 acc_train: 0.3585 loss_val: 1.5933 acc_val: 0.4279\n",
            "Epoch: 0022 loss_train: 1.6434 acc_train: 0.4091 loss_val: 1.5493 acc_val: 0.5811\n",
            "Epoch: 0023 loss_train: 1.6130 acc_train: 0.5168 loss_val: 1.4933 acc_val: 0.6772\n",
            "Epoch: 0024 loss_train: 1.5774 acc_train: 0.5829 loss_val: 1.4203 acc_val: 0.7267\n",
            "Epoch: 0025 loss_train: 1.5276 acc_train: 0.6089 loss_val: 1.3240 acc_val: 0.7823\n",
            "Epoch: 0026 loss_train: 1.4603 acc_train: 0.6430 loss_val: 1.2048 acc_val: 0.8649\n",
            "Epoch: 0027 loss_train: 1.3841 acc_train: 0.7021 loss_val: 1.0877 acc_val: 0.8679\n",
            "Epoch: 0028 loss_train: 1.3076 acc_train: 0.7151 loss_val: 0.9889 acc_val: 0.8769\n",
            "Epoch: 0029 loss_train: 1.2392 acc_train: 0.7261 loss_val: 0.8948 acc_val: 0.8874\n",
            "Epoch: 0030 loss_train: 1.1799 acc_train: 0.7321 loss_val: 0.8119 acc_val: 0.8859\n",
            "Epoch: 0031 loss_train: 1.1162 acc_train: 0.7376 loss_val: 0.7653 acc_val: 0.8934\n",
            "Epoch: 0032 loss_train: 1.0864 acc_train: 0.7456 loss_val: 0.7543 acc_val: 0.8258\n",
            "Epoch: 0033 loss_train: 1.0652 acc_train: 0.6925 loss_val: 0.6170 acc_val: 0.9084\n",
            "Epoch: 0034 loss_train: 0.9702 acc_train: 0.7521 loss_val: 0.6015 acc_val: 0.9009\n",
            "Epoch: 0035 loss_train: 0.9591 acc_train: 0.7491 loss_val: 0.5135 acc_val: 0.9084\n",
            "Epoch: 0036 loss_train: 0.8920 acc_train: 0.7501 loss_val: 0.5072 acc_val: 0.8919\n",
            "Epoch: 0037 loss_train: 0.8842 acc_train: 0.7396 loss_val: 0.4228 acc_val: 0.9114\n",
            "Epoch: 0038 loss_train: 0.8288 acc_train: 0.7601 loss_val: 0.4187 acc_val: 0.9099\n",
            "Epoch: 0039 loss_train: 0.8218 acc_train: 0.7581 loss_val: 0.3484 acc_val: 0.9189\n",
            "Epoch: 0040 loss_train: 0.7674 acc_train: 0.7661 loss_val: 0.3617 acc_val: 0.9144\n",
            "Epoch: 0041 loss_train: 0.7668 acc_train: 0.7576 loss_val: 0.2998 acc_val: 0.9279\n",
            "Epoch: 0042 loss_train: 0.7198 acc_train: 0.7752 loss_val: 0.2961 acc_val: 0.9324\n",
            "Epoch: 0043 loss_train: 0.7141 acc_train: 0.7702 loss_val: 0.2673 acc_val: 0.9339\n",
            "Epoch: 0044 loss_train: 0.6825 acc_train: 0.7777 loss_val: 0.2571 acc_val: 0.9429\n",
            "Epoch: 0045 loss_train: 0.6637 acc_train: 0.7857 loss_val: 0.2443 acc_val: 0.9474\n",
            "Epoch: 0046 loss_train: 0.6452 acc_train: 0.7842 loss_val: 0.2153 acc_val: 0.9535\n",
            "Epoch: 0047 loss_train: 0.6206 acc_train: 0.7882 loss_val: 0.2163 acc_val: 0.9565\n",
            "Epoch: 0048 loss_train: 0.6115 acc_train: 0.7912 loss_val: 0.1941 acc_val: 0.9640\n",
            "Epoch: 0049 loss_train: 0.5833 acc_train: 0.7987 loss_val: 0.1912 acc_val: 0.9640\n",
            "Epoch: 0050 loss_train: 0.5694 acc_train: 0.8022 loss_val: 0.1775 acc_val: 0.9700\n",
            "Epoch: 0051 loss_train: 0.5510 acc_train: 0.8037 loss_val: 0.1581 acc_val: 0.9700\n",
            "Epoch: 0052 loss_train: 0.5324 acc_train: 0.8077 loss_val: 0.1517 acc_val: 0.9760\n",
            "Epoch: 0053 loss_train: 0.5197 acc_train: 0.8092 loss_val: 0.1413 acc_val: 0.9790\n",
            "Epoch: 0054 loss_train: 0.5000 acc_train: 0.8112 loss_val: 0.1355 acc_val: 0.9805\n",
            "Epoch: 0055 loss_train: 0.4874 acc_train: 0.8192 loss_val: 0.1280 acc_val: 0.9835\n",
            "Epoch: 0056 loss_train: 0.4735 acc_train: 0.8167 loss_val: 0.1165 acc_val: 0.9820\n",
            "Epoch: 0057 loss_train: 0.4604 acc_train: 0.8157 loss_val: 0.1098 acc_val: 0.9805\n",
            "Epoch: 0058 loss_train: 0.4497 acc_train: 0.8207 loss_val: 0.1028 acc_val: 0.9880\n",
            "Epoch: 0059 loss_train: 0.4354 acc_train: 0.8282 loss_val: 0.0962 acc_val: 0.9865\n",
            "Epoch: 0060 loss_train: 0.4249 acc_train: 0.8247 loss_val: 0.0905 acc_val: 0.9850\n",
            "Epoch: 0061 loss_train: 0.4165 acc_train: 0.8267 loss_val: 0.0862 acc_val: 0.9850\n",
            "Epoch: 0062 loss_train: 0.4065 acc_train: 0.8317 loss_val: 0.0815 acc_val: 0.9880\n",
            "Epoch: 0063 loss_train: 0.3959 acc_train: 0.8363 loss_val: 0.0743 acc_val: 0.9895\n",
            "Epoch: 0064 loss_train: 0.3868 acc_train: 0.8368 loss_val: 0.0685 acc_val: 0.9895\n",
            "Epoch: 0065 loss_train: 0.3792 acc_train: 0.8327 loss_val: 0.0637 acc_val: 0.9910\n",
            "Epoch: 0066 loss_train: 0.3692 acc_train: 0.8358 loss_val: 0.0623 acc_val: 0.9910\n",
            "Epoch: 0067 loss_train: 0.3623 acc_train: 0.8458 loss_val: 0.0607 acc_val: 0.9925\n",
            "Epoch: 0068 loss_train: 0.3558 acc_train: 0.8538 loss_val: 0.0561 acc_val: 0.9940\n",
            "Epoch: 0069 loss_train: 0.3480 acc_train: 0.8553 loss_val: 0.0505 acc_val: 0.9940\n",
            "Epoch: 0070 loss_train: 0.3400 acc_train: 0.8593 loss_val: 0.0479 acc_val: 0.9940\n",
            "Epoch: 0071 loss_train: 0.3344 acc_train: 0.8673 loss_val: 0.0465 acc_val: 0.9925\n",
            "Epoch: 0072 loss_train: 0.3260 acc_train: 0.8808 loss_val: 0.0457 acc_val: 0.9940\n",
            "Epoch: 0073 loss_train: 0.3177 acc_train: 0.8968 loss_val: 0.0454 acc_val: 0.9940\n",
            "Epoch: 0074 loss_train: 0.3104 acc_train: 0.9064 loss_val: 0.0441 acc_val: 0.9940\n",
            "Epoch: 0075 loss_train: 0.3021 acc_train: 0.9234 loss_val: 0.0429 acc_val: 0.9940\n",
            "Epoch: 0076 loss_train: 0.2939 acc_train: 0.9359 loss_val: 0.0426 acc_val: 0.9940\n",
            "Epoch: 0077 loss_train: 0.2841 acc_train: 0.9494 loss_val: 0.0448 acc_val: 0.9925\n",
            "Epoch: 0078 loss_train: 0.2743 acc_train: 0.9569 loss_val: 0.0464 acc_val: 0.9925\n",
            "Epoch: 0079 loss_train: 0.2643 acc_train: 0.9629 loss_val: 0.0444 acc_val: 0.9925\n",
            "Epoch: 0080 loss_train: 0.2541 acc_train: 0.9680 loss_val: 0.0426 acc_val: 0.9925\n",
            "Epoch: 0081 loss_train: 0.2422 acc_train: 0.9780 loss_val: 0.0424 acc_val: 0.9925\n",
            "Epoch: 0082 loss_train: 0.2320 acc_train: 0.9775 loss_val: 0.0420 acc_val: 0.9925\n",
            "Epoch: 0083 loss_train: 0.2206 acc_train: 0.9800 loss_val: 0.0431 acc_val: 0.9910\n",
            "Epoch: 0084 loss_train: 0.2107 acc_train: 0.9780 loss_val: 0.0430 acc_val: 0.9910\n",
            "Epoch: 0085 loss_train: 0.1983 acc_train: 0.9805 loss_val: 0.0410 acc_val: 0.9895\n",
            "Epoch: 0086 loss_train: 0.1868 acc_train: 0.9835 loss_val: 0.0385 acc_val: 0.9910\n",
            "Epoch: 0087 loss_train: 0.1753 acc_train: 0.9855 loss_val: 0.0399 acc_val: 0.9925\n",
            "Epoch: 0088 loss_train: 0.1645 acc_train: 0.9845 loss_val: 0.0414 acc_val: 0.9910\n",
            "Epoch: 0089 loss_train: 0.1543 acc_train: 0.9855 loss_val: 0.0409 acc_val: 0.9880\n",
            "Epoch: 0090 loss_train: 0.1439 acc_train: 0.9870 loss_val: 0.0385 acc_val: 0.9895\n",
            "Epoch: 0091 loss_train: 0.1331 acc_train: 0.9905 loss_val: 0.0383 acc_val: 0.9910\n",
            "Epoch: 0092 loss_train: 0.1217 acc_train: 0.9905 loss_val: 0.0389 acc_val: 0.9910\n",
            "Epoch: 0093 loss_train: 0.1114 acc_train: 0.9945 loss_val: 0.0378 acc_val: 0.9910\n",
            "Epoch: 0094 loss_train: 0.1020 acc_train: 0.9945 loss_val: 0.0359 acc_val: 0.9895\n",
            "Epoch: 0095 loss_train: 0.0930 acc_train: 0.9970 loss_val: 0.0380 acc_val: 0.9880\n",
            "Epoch: 0096 loss_train: 0.0834 acc_train: 0.9975 loss_val: 0.0417 acc_val: 0.9865\n",
            "Epoch: 0097 loss_train: 0.0751 acc_train: 0.9980 loss_val: 0.0434 acc_val: 0.9850\n",
            "Epoch: 0098 loss_train: 0.0672 acc_train: 0.9985 loss_val: 0.0435 acc_val: 0.9850\n",
            "Epoch: 0099 loss_train: 0.0590 acc_train: 0.9995 loss_val: 0.0435 acc_val: 0.9850\n",
            "Epoch: 0100 loss_train: 0.0532 acc_train: 1.0000 loss_val: 0.0419 acc_val: 0.9835\n",
            "Epoch: 0101 loss_train: 0.0470 acc_train: 1.0000 loss_val: 0.0446 acc_val: 0.9820\n",
            "Epoch: 0102 loss_train: 0.0422 acc_train: 1.0000 loss_val: 0.0487 acc_val: 0.9790\n",
            "Epoch: 0103 loss_train: 0.0374 acc_train: 1.0000 loss_val: 0.0533 acc_val: 0.9790\n",
            "Epoch: 0104 loss_train: 0.0333 acc_train: 1.0000 loss_val: 0.0575 acc_val: 0.9775\n",
            "Epoch: 0105 loss_train: 0.0291 acc_train: 1.0000 loss_val: 0.0581 acc_val: 0.9775\n",
            "Epoch: 0106 loss_train: 0.0265 acc_train: 0.9995 loss_val: 0.0507 acc_val: 0.9820\n",
            "Epoch: 0107 loss_train: 0.0239 acc_train: 1.0000 loss_val: 0.0474 acc_val: 0.9820\n",
            "Epoch: 0108 loss_train: 0.0211 acc_train: 1.0000 loss_val: 0.0481 acc_val: 0.9820\n",
            "Epoch: 0109 loss_train: 0.0185 acc_train: 1.0000 loss_val: 0.0517 acc_val: 0.9820\n",
            "Epoch: 0110 loss_train: 0.0169 acc_train: 1.0000 loss_val: 0.0561 acc_val: 0.9820\n",
            "Epoch: 0111 loss_train: 0.0152 acc_train: 1.0000 loss_val: 0.0614 acc_val: 0.9805\n",
            "Epoch: 0112 loss_train: 0.0137 acc_train: 1.0000 loss_val: 0.0641 acc_val: 0.9790\n",
            "Epoch: 0113 loss_train: 0.0123 acc_train: 1.0000 loss_val: 0.0617 acc_val: 0.9790\n",
            "Epoch: 0114 loss_train: 0.0112 acc_train: 1.0000 loss_val: 0.0573 acc_val: 0.9805\n",
            "Epoch: 0115 loss_train: 0.0100 acc_train: 1.0000 loss_val: 0.0532 acc_val: 0.9805\n",
            "Epoch: 0116 loss_train: 0.0094 acc_train: 1.0000 loss_val: 0.0523 acc_val: 0.9820\n",
            "Epoch: 0117 loss_train: 0.0088 acc_train: 1.0000 loss_val: 0.0529 acc_val: 0.9820\n",
            "Epoch: 0118 loss_train: 0.0078 acc_train: 1.0000 loss_val: 0.0555 acc_val: 0.9805\n",
            "Epoch: 0119 loss_train: 0.0072 acc_train: 1.0000 loss_val: 0.0581 acc_val: 0.9805\n",
            "Epoch: 0120 loss_train: 0.0068 acc_train: 1.0000 loss_val: 0.0616 acc_val: 0.9805\n",
            "Epoch: 0121 loss_train: 0.0062 acc_train: 1.0000 loss_val: 0.0671 acc_val: 0.9790\n",
            "Epoch: 0122 loss_train: 0.0058 acc_train: 1.0000 loss_val: 0.0696 acc_val: 0.9790\n",
            "Epoch: 0123 loss_train: 0.0055 acc_train: 1.0000 loss_val: 0.0673 acc_val: 0.9790\n",
            "Epoch: 0124 loss_train: 0.0050 acc_train: 1.0000 loss_val: 0.0606 acc_val: 0.9790\n",
            "Epoch: 0125 loss_train: 0.0046 acc_train: 1.0000 loss_val: 0.0542 acc_val: 0.9820\n",
            "Epoch: 0126 loss_train: 0.0042 acc_train: 1.0000 loss_val: 0.0489 acc_val: 0.9835\n",
            "Epoch: 0127 loss_train: 0.0040 acc_train: 1.0000 loss_val: 0.0468 acc_val: 0.9865\n",
            "Epoch: 0128 loss_train: 0.0038 acc_train: 1.0000 loss_val: 0.0484 acc_val: 0.9865\n",
            "Epoch: 0129 loss_train: 0.0036 acc_train: 1.0000 loss_val: 0.0526 acc_val: 0.9835\n",
            "Epoch: 0130 loss_train: 0.0033 acc_train: 1.0000 loss_val: 0.0582 acc_val: 0.9805\n",
            "Epoch: 0131 loss_train: 0.0031 acc_train: 1.0000 loss_val: 0.0638 acc_val: 0.9790\n",
            "Epoch: 0132 loss_train: 0.0030 acc_train: 1.0000 loss_val: 0.0670 acc_val: 0.9775\n",
            "Epoch: 0133 loss_train: 0.0027 acc_train: 1.0000 loss_val: 0.0686 acc_val: 0.9775\n",
            "Epoch: 0134 loss_train: 0.0026 acc_train: 1.0000 loss_val: 0.0683 acc_val: 0.9775\n",
            "Epoch: 0135 loss_train: 0.0025 acc_train: 1.0000 loss_val: 0.0658 acc_val: 0.9790\n",
            "Epoch: 0136 loss_train: 0.0024 acc_train: 1.0000 loss_val: 0.0621 acc_val: 0.9805\n",
            "Epoch: 0137 loss_train: 0.0023 acc_train: 1.0000 loss_val: 0.0581 acc_val: 0.9820\n",
            "Epoch: 0138 loss_train: 0.0022 acc_train: 1.0000 loss_val: 0.0541 acc_val: 0.9850\n",
            "Epoch: 0139 loss_train: 0.0021 acc_train: 1.0000 loss_val: 0.0517 acc_val: 0.9850\n",
            "Epoch: 0140 loss_train: 0.0020 acc_train: 1.0000 loss_val: 0.0513 acc_val: 0.9850\n",
            "Epoch: 0141 loss_train: 0.0019 acc_train: 1.0000 loss_val: 0.0521 acc_val: 0.9850\n",
            "Epoch: 0142 loss_train: 0.0019 acc_train: 1.0000 loss_val: 0.0539 acc_val: 0.9835\n",
            "Epoch: 0143 loss_train: 0.0019 acc_train: 1.0000 loss_val: 0.0557 acc_val: 0.9835\n",
            "Epoch: 0144 loss_train: 0.0017 acc_train: 1.0000 loss_val: 0.0576 acc_val: 0.9835\n",
            "Optimization Finished!\n",
            "Train cost: 56.6995s\n",
            "Loading 76th epoch\n",
            "Test set results: loss= 0.0533 accuracy= 0.9819\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --dataset citeseer --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 7  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack none --attperc 20 --TrainTest train --weight_decay=1e-05"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fr1DmoDnDAbD",
        "outputId": "f7f317ad-b622-4f31-c7c7-71858e0e2a7b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NumNodes: 3327\n",
            "  NumEdges: 9228\n",
            "  NumFeats: 3703\n",
            "  NumClasses: 6\n",
            "  NumTrainingSamples: 120\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:92: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=3718, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=6, bias=True)\n",
            ")\n",
            "total params: 4141832\n",
            "Epoch: 0001 loss_train: 1.8150 acc_train: 0.1282 loss_val: 1.8131 acc_val: 0.1186\n",
            "Epoch: 0002 loss_train: 1.8111 acc_train: 0.1277 loss_val: 1.8018 acc_val: 0.1291\n",
            "Epoch: 0003 loss_train: 1.8005 acc_train: 0.1352 loss_val: 1.7852 acc_val: 0.1336\n",
            "Epoch: 0004 loss_train: 1.7856 acc_train: 0.1487 loss_val: 1.7635 acc_val: 0.1607\n",
            "Epoch: 0005 loss_train: 1.7654 acc_train: 0.1763 loss_val: 1.7372 acc_val: 0.2492\n",
            "Epoch: 0006 loss_train: 1.7398 acc_train: 0.2649 loss_val: 1.7069 acc_val: 0.3709\n",
            "Epoch: 0007 loss_train: 1.7108 acc_train: 0.3540 loss_val: 1.6733 acc_val: 0.4595\n",
            "Epoch: 0008 loss_train: 1.6768 acc_train: 0.4557 loss_val: 1.6374 acc_val: 0.5030\n",
            "Epoch: 0009 loss_train: 1.6438 acc_train: 0.5103 loss_val: 1.6001 acc_val: 0.5435\n",
            "Epoch: 0010 loss_train: 1.6064 acc_train: 0.5443 loss_val: 1.5625 acc_val: 0.5616\n",
            "Epoch: 0011 loss_train: 1.5677 acc_train: 0.5729 loss_val: 1.5245 acc_val: 0.5721\n",
            "Epoch: 0012 loss_train: 1.5280 acc_train: 0.5899 loss_val: 1.4863 acc_val: 0.5736\n",
            "Epoch: 0013 loss_train: 1.4891 acc_train: 0.5949 loss_val: 1.4478 acc_val: 0.5856\n",
            "Epoch: 0014 loss_train: 1.4490 acc_train: 0.6119 loss_val: 1.4088 acc_val: 0.5901\n",
            "Epoch: 0015 loss_train: 1.4090 acc_train: 0.6124 loss_val: 1.3698 acc_val: 0.6006\n",
            "Epoch: 0016 loss_train: 1.3676 acc_train: 0.6284 loss_val: 1.3310 acc_val: 0.6111\n",
            "Epoch: 0017 loss_train: 1.3260 acc_train: 0.6450 loss_val: 1.2928 acc_val: 0.6291\n",
            "Epoch: 0018 loss_train: 1.2848 acc_train: 0.6505 loss_val: 1.2555 acc_val: 0.6366\n",
            "Epoch: 0019 loss_train: 1.2447 acc_train: 0.6635 loss_val: 1.2190 acc_val: 0.6426\n",
            "Epoch: 0020 loss_train: 1.2040 acc_train: 0.6725 loss_val: 1.1837 acc_val: 0.6486\n",
            "Epoch: 0021 loss_train: 1.1641 acc_train: 0.6825 loss_val: 1.1497 acc_val: 0.6562\n",
            "Epoch: 0022 loss_train: 1.1260 acc_train: 0.6890 loss_val: 1.1171 acc_val: 0.6637\n",
            "Epoch: 0023 loss_train: 1.0881 acc_train: 0.6965 loss_val: 1.0863 acc_val: 0.6697\n",
            "Epoch: 0024 loss_train: 1.0529 acc_train: 0.7051 loss_val: 1.0573 acc_val: 0.6742\n",
            "Epoch: 0025 loss_train: 1.0174 acc_train: 0.7106 loss_val: 1.0304 acc_val: 0.6802\n",
            "Epoch: 0026 loss_train: 0.9825 acc_train: 0.7171 loss_val: 1.0053 acc_val: 0.6832\n",
            "Epoch: 0027 loss_train: 0.9507 acc_train: 0.7231 loss_val: 0.9819 acc_val: 0.6892\n",
            "Epoch: 0028 loss_train: 0.9189 acc_train: 0.7296 loss_val: 0.9600 acc_val: 0.6892\n",
            "Epoch: 0029 loss_train: 0.8883 acc_train: 0.7371 loss_val: 0.9396 acc_val: 0.6952\n",
            "Epoch: 0030 loss_train: 0.8563 acc_train: 0.7436 loss_val: 0.9207 acc_val: 0.6967\n",
            "Epoch: 0031 loss_train: 0.8287 acc_train: 0.7481 loss_val: 0.9035 acc_val: 0.7027\n",
            "Epoch: 0032 loss_train: 0.8008 acc_train: 0.7606 loss_val: 0.8878 acc_val: 0.7102\n",
            "Epoch: 0033 loss_train: 0.7751 acc_train: 0.7651 loss_val: 0.8737 acc_val: 0.7102\n",
            "Epoch: 0034 loss_train: 0.7463 acc_train: 0.7707 loss_val: 0.8616 acc_val: 0.7087\n",
            "Epoch: 0035 loss_train: 0.7243 acc_train: 0.7772 loss_val: 0.8517 acc_val: 0.7117\n",
            "Epoch: 0036 loss_train: 0.7008 acc_train: 0.7817 loss_val: 0.8439 acc_val: 0.7132\n",
            "Epoch: 0037 loss_train: 0.6769 acc_train: 0.7862 loss_val: 0.8379 acc_val: 0.7192\n",
            "Epoch: 0038 loss_train: 0.6536 acc_train: 0.7922 loss_val: 0.8337 acc_val: 0.7192\n",
            "Epoch: 0039 loss_train: 0.6302 acc_train: 0.7967 loss_val: 0.8311 acc_val: 0.7237\n",
            "Epoch: 0040 loss_train: 0.6067 acc_train: 0.8032 loss_val: 0.8302 acc_val: 0.7297\n",
            "Epoch: 0041 loss_train: 0.5851 acc_train: 0.8092 loss_val: 0.8312 acc_val: 0.7297\n",
            "Epoch: 0042 loss_train: 0.5627 acc_train: 0.8187 loss_val: 0.8338 acc_val: 0.7372\n",
            "Epoch: 0043 loss_train: 0.5425 acc_train: 0.8202 loss_val: 0.8378 acc_val: 0.7402\n",
            "Epoch: 0044 loss_train: 0.5219 acc_train: 0.8232 loss_val: 0.8430 acc_val: 0.7402\n",
            "Epoch: 0045 loss_train: 0.5018 acc_train: 0.8302 loss_val: 0.8488 acc_val: 0.7432\n",
            "Epoch: 0046 loss_train: 0.4849 acc_train: 0.8353 loss_val: 0.8555 acc_val: 0.7357\n",
            "Epoch: 0047 loss_train: 0.4673 acc_train: 0.8398 loss_val: 0.8631 acc_val: 0.7327\n",
            "Epoch: 0048 loss_train: 0.4488 acc_train: 0.8463 loss_val: 0.8710 acc_val: 0.7327\n",
            "Epoch: 0049 loss_train: 0.4327 acc_train: 0.8448 loss_val: 0.8783 acc_val: 0.7312\n",
            "Epoch: 0050 loss_train: 0.4128 acc_train: 0.8528 loss_val: 0.8861 acc_val: 0.7312\n",
            "Epoch: 0051 loss_train: 0.3955 acc_train: 0.8568 loss_val: 0.8961 acc_val: 0.7372\n",
            "Epoch: 0052 loss_train: 0.3771 acc_train: 0.8698 loss_val: 0.9079 acc_val: 0.7402\n",
            "Epoch: 0053 loss_train: 0.3595 acc_train: 0.8688 loss_val: 0.9206 acc_val: 0.7402\n",
            "Epoch: 0054 loss_train: 0.3425 acc_train: 0.8788 loss_val: 0.9330 acc_val: 0.7372\n",
            "Epoch: 0055 loss_train: 0.3231 acc_train: 0.8828 loss_val: 0.9470 acc_val: 0.7357\n",
            "Epoch: 0056 loss_train: 0.3074 acc_train: 0.8863 loss_val: 0.9626 acc_val: 0.7387\n",
            "Epoch: 0057 loss_train: 0.2919 acc_train: 0.8913 loss_val: 0.9807 acc_val: 0.7342\n",
            "Epoch: 0058 loss_train: 0.2711 acc_train: 0.9104 loss_val: 1.0011 acc_val: 0.7342\n",
            "Epoch: 0059 loss_train: 0.2554 acc_train: 0.9109 loss_val: 1.0221 acc_val: 0.7327\n",
            "Epoch: 0060 loss_train: 0.2397 acc_train: 0.9169 loss_val: 1.0444 acc_val: 0.7282\n",
            "Epoch: 0061 loss_train: 0.2246 acc_train: 0.9234 loss_val: 1.0672 acc_val: 0.7297\n",
            "Epoch: 0062 loss_train: 0.2104 acc_train: 0.9294 loss_val: 1.0915 acc_val: 0.7312\n",
            "Epoch: 0063 loss_train: 0.1917 acc_train: 0.9364 loss_val: 1.1146 acc_val: 0.7342\n",
            "Epoch: 0064 loss_train: 0.1787 acc_train: 0.9409 loss_val: 1.1404 acc_val: 0.7252\n",
            "Epoch: 0065 loss_train: 0.1653 acc_train: 0.9464 loss_val: 1.1691 acc_val: 0.7222\n",
            "Epoch: 0066 loss_train: 0.1508 acc_train: 0.9499 loss_val: 1.1967 acc_val: 0.7207\n",
            "Epoch: 0067 loss_train: 0.1350 acc_train: 0.9574 loss_val: 1.2235 acc_val: 0.7237\n",
            "Epoch: 0068 loss_train: 0.1224 acc_train: 0.9634 loss_val: 1.2522 acc_val: 0.7207\n",
            "Epoch: 0069 loss_train: 0.1107 acc_train: 0.9690 loss_val: 1.2825 acc_val: 0.7222\n",
            "Epoch: 0070 loss_train: 0.0987 acc_train: 0.9750 loss_val: 1.3105 acc_val: 0.7192\n",
            "Epoch: 0071 loss_train: 0.0900 acc_train: 0.9765 loss_val: 1.3377 acc_val: 0.7222\n",
            "Epoch: 0072 loss_train: 0.0818 acc_train: 0.9775 loss_val: 1.3663 acc_val: 0.7207\n",
            "Epoch: 0073 loss_train: 0.0725 acc_train: 0.9800 loss_val: 1.3947 acc_val: 0.7222\n",
            "Epoch: 0074 loss_train: 0.0663 acc_train: 0.9815 loss_val: 1.4226 acc_val: 0.7267\n",
            "Epoch: 0075 loss_train: 0.0589 acc_train: 0.9850 loss_val: 1.4505 acc_val: 0.7252\n",
            "Epoch: 0076 loss_train: 0.0539 acc_train: 0.9880 loss_val: 1.4787 acc_val: 0.7237\n",
            "Epoch: 0077 loss_train: 0.0501 acc_train: 0.9880 loss_val: 1.5060 acc_val: 0.7222\n",
            "Epoch: 0078 loss_train: 0.0438 acc_train: 0.9915 loss_val: 1.5309 acc_val: 0.7252\n",
            "Epoch: 0079 loss_train: 0.0383 acc_train: 0.9915 loss_val: 1.5558 acc_val: 0.7237\n",
            "Epoch: 0080 loss_train: 0.0361 acc_train: 0.9905 loss_val: 1.5841 acc_val: 0.7237\n",
            "Epoch: 0081 loss_train: 0.0330 acc_train: 0.9925 loss_val: 1.6085 acc_val: 0.7252\n",
            "Epoch: 0082 loss_train: 0.0303 acc_train: 0.9930 loss_val: 1.6327 acc_val: 0.7267\n",
            "Epoch: 0083 loss_train: 0.0271 acc_train: 0.9940 loss_val: 1.6590 acc_val: 0.7252\n",
            "Epoch: 0084 loss_train: 0.0248 acc_train: 0.9935 loss_val: 1.6874 acc_val: 0.7237\n",
            "Epoch: 0085 loss_train: 0.0245 acc_train: 0.9955 loss_val: 1.7118 acc_val: 0.7267\n",
            "Epoch: 0086 loss_train: 0.0217 acc_train: 0.9945 loss_val: 1.7299 acc_val: 0.7237\n",
            "Epoch: 0087 loss_train: 0.0199 acc_train: 0.9945 loss_val: 1.7525 acc_val: 0.7222\n",
            "Epoch: 0088 loss_train: 0.0192 acc_train: 0.9925 loss_val: 1.7763 acc_val: 0.7267\n",
            "Epoch: 0089 loss_train: 0.0168 acc_train: 0.9950 loss_val: 1.7910 acc_val: 0.7282\n",
            "Epoch: 0090 loss_train: 0.0157 acc_train: 0.9965 loss_val: 1.8036 acc_val: 0.7207\n",
            "Epoch: 0091 loss_train: 0.0155 acc_train: 0.9940 loss_val: 1.8288 acc_val: 0.7312\n",
            "Epoch: 0092 loss_train: 0.0127 acc_train: 0.9980 loss_val: 1.8518 acc_val: 0.7297\n",
            "Epoch: 0093 loss_train: 0.0134 acc_train: 0.9970 loss_val: 1.8636 acc_val: 0.7252\n",
            "Epoch: 0094 loss_train: 0.0117 acc_train: 0.9970 loss_val: 1.8723 acc_val: 0.7207\n",
            "Epoch: 0095 loss_train: 0.0102 acc_train: 0.9980 loss_val: 1.8859 acc_val: 0.7267\n",
            "Optimization Finished!\n",
            "Train cost: 38.7821s\n",
            "Loading 45th epoch\n",
            "Test set results: loss= 0.6811 accuracy= 0.7937\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --dataset cora --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 7  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack dice --attperc 20 --TrainTest train --weight_decay=1e-05"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwE6lq1dehTm",
        "outputId": "fccc2506-6582-4fc8-ecdc-d1bd55845d61"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:92: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=1448, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=7, bias=True)\n",
            ")\n",
            "total params: 2979849\n",
            "Epoch: 0001 loss_train: 1.9703 acc_train: 0.1156 loss_val: 1.9655 acc_val: 0.1144\n",
            "Epoch: 0002 loss_train: 1.9629 acc_train: 0.1199 loss_val: 1.9551 acc_val: 0.1273\n",
            "Epoch: 0003 loss_train: 1.9549 acc_train: 0.1347 loss_val: 1.9403 acc_val: 0.1365\n",
            "Epoch: 0004 loss_train: 1.9438 acc_train: 0.1125 loss_val: 1.9220 acc_val: 0.1347\n",
            "Epoch: 0005 loss_train: 1.9274 acc_train: 0.1507 loss_val: 1.9020 acc_val: 0.3026\n",
            "Epoch: 0006 loss_train: 1.9088 acc_train: 0.2792 loss_val: 1.8824 acc_val: 0.3026\n",
            "Epoch: 0007 loss_train: 1.8894 acc_train: 0.3020 loss_val: 1.8642 acc_val: 0.3026\n",
            "Epoch: 0008 loss_train: 1.8723 acc_train: 0.3020 loss_val: 1.8492 acc_val: 0.3026\n",
            "Epoch: 0009 loss_train: 1.8581 acc_train: 0.3020 loss_val: 1.8383 acc_val: 0.3026\n",
            "Epoch: 0010 loss_train: 1.8462 acc_train: 0.3020 loss_val: 1.8308 acc_val: 0.3026\n",
            "Epoch: 0011 loss_train: 1.8380 acc_train: 0.3020 loss_val: 1.8267 acc_val: 0.3026\n",
            "Epoch: 0012 loss_train: 1.8327 acc_train: 0.3020 loss_val: 1.8252 acc_val: 0.3026\n",
            "Epoch: 0013 loss_train: 1.8288 acc_train: 0.3020 loss_val: 1.8245 acc_val: 0.3026\n",
            "Epoch: 0014 loss_train: 1.8279 acc_train: 0.3020 loss_val: 1.8235 acc_val: 0.3026\n",
            "Epoch: 0015 loss_train: 1.8265 acc_train: 0.3020 loss_val: 1.8216 acc_val: 0.3026\n",
            "Epoch: 0016 loss_train: 1.8240 acc_train: 0.3020 loss_val: 1.8188 acc_val: 0.3026\n",
            "Epoch: 0017 loss_train: 1.8229 acc_train: 0.3020 loss_val: 1.8152 acc_val: 0.3026\n",
            "Epoch: 0018 loss_train: 1.8184 acc_train: 0.3020 loss_val: 1.8115 acc_val: 0.3026\n",
            "Epoch: 0019 loss_train: 1.8186 acc_train: 0.3020 loss_val: 1.8080 acc_val: 0.3026\n",
            "Epoch: 0020 loss_train: 1.8170 acc_train: 0.3020 loss_val: 1.8046 acc_val: 0.3026\n",
            "Epoch: 0021 loss_train: 1.8133 acc_train: 0.3020 loss_val: 1.8009 acc_val: 0.3026\n",
            "Epoch: 0022 loss_train: 1.8111 acc_train: 0.3020 loss_val: 1.7962 acc_val: 0.3026\n",
            "Epoch: 0023 loss_train: 1.8093 acc_train: 0.3020 loss_val: 1.7898 acc_val: 0.3026\n",
            "Epoch: 0024 loss_train: 1.8023 acc_train: 0.3020 loss_val: 1.7818 acc_val: 0.3026\n",
            "Epoch: 0025 loss_train: 1.7975 acc_train: 0.3020 loss_val: 1.7722 acc_val: 0.3026\n",
            "Epoch: 0026 loss_train: 1.7896 acc_train: 0.3020 loss_val: 1.7596 acc_val: 0.3026\n",
            "Epoch: 0027 loss_train: 1.7816 acc_train: 0.3020 loss_val: 1.7412 acc_val: 0.3026\n",
            "Epoch: 0028 loss_train: 1.7666 acc_train: 0.3020 loss_val: 1.7137 acc_val: 0.3026\n",
            "Epoch: 0029 loss_train: 1.7465 acc_train: 0.3020 loss_val: 1.6777 acc_val: 0.3026\n",
            "Epoch: 0030 loss_train: 1.7210 acc_train: 0.3026 loss_val: 1.6355 acc_val: 0.3487\n",
            "Epoch: 0031 loss_train: 1.6918 acc_train: 0.3413 loss_val: 1.5806 acc_val: 0.4170\n",
            "Epoch: 0032 loss_train: 1.6512 acc_train: 0.4028 loss_val: 1.5123 acc_val: 0.4188\n",
            "Epoch: 0033 loss_train: 1.6034 acc_train: 0.3875 loss_val: 1.4253 acc_val: 0.4926\n",
            "Epoch: 0034 loss_train: 1.5441 acc_train: 0.4465 loss_val: 1.3285 acc_val: 0.6476\n",
            "Epoch: 0035 loss_train: 1.4844 acc_train: 0.5535 loss_val: 1.2397 acc_val: 0.6624\n",
            "Epoch: 0036 loss_train: 1.4195 acc_train: 0.5677 loss_val: 1.1604 acc_val: 0.6771\n",
            "Epoch: 0037 loss_train: 1.3668 acc_train: 0.5793 loss_val: 1.0915 acc_val: 0.7085\n",
            "Epoch: 0038 loss_train: 1.3191 acc_train: 0.6027 loss_val: 1.0266 acc_val: 0.7214\n",
            "Epoch: 0039 loss_train: 1.2737 acc_train: 0.6101 loss_val: 0.9633 acc_val: 0.7638\n",
            "Epoch: 0040 loss_train: 1.2200 acc_train: 0.6538 loss_val: 0.8931 acc_val: 0.7804\n",
            "Epoch: 0041 loss_train: 1.1647 acc_train: 0.6630 loss_val: 0.8302 acc_val: 0.7915\n",
            "Epoch: 0042 loss_train: 1.1143 acc_train: 0.6734 loss_val: 0.7783 acc_val: 0.8081\n",
            "Epoch: 0043 loss_train: 1.0733 acc_train: 0.6888 loss_val: 0.7262 acc_val: 0.8266\n",
            "Epoch: 0044 loss_train: 1.0268 acc_train: 0.7030 loss_val: 0.6773 acc_val: 0.8450\n",
            "Epoch: 0045 loss_train: 0.9852 acc_train: 0.7109 loss_val: 0.6190 acc_val: 0.8450\n",
            "Epoch: 0046 loss_train: 0.9423 acc_train: 0.7214 loss_val: 0.5949 acc_val: 0.8598\n",
            "Epoch: 0047 loss_train: 0.9076 acc_train: 0.7269 loss_val: 0.5311 acc_val: 0.8819\n",
            "Epoch: 0048 loss_train: 0.8620 acc_train: 0.7423 loss_val: 0.4770 acc_val: 0.8893\n",
            "Epoch: 0049 loss_train: 0.8142 acc_train: 0.7552 loss_val: 0.4734 acc_val: 0.8690\n",
            "Epoch: 0050 loss_train: 0.7952 acc_train: 0.7608 loss_val: 0.4127 acc_val: 0.9004\n",
            "Epoch: 0051 loss_train: 0.7465 acc_train: 0.7632 loss_val: 0.3942 acc_val: 0.9022\n",
            "Epoch: 0052 loss_train: 0.7243 acc_train: 0.7755 loss_val: 0.3640 acc_val: 0.9059\n",
            "Epoch: 0053 loss_train: 0.6830 acc_train: 0.7878 loss_val: 0.3599 acc_val: 0.9096\n",
            "Epoch: 0054 loss_train: 0.6612 acc_train: 0.7909 loss_val: 0.3290 acc_val: 0.9225\n",
            "Epoch: 0055 loss_train: 0.6274 acc_train: 0.8001 loss_val: 0.3113 acc_val: 0.9188\n",
            "Epoch: 0056 loss_train: 0.6080 acc_train: 0.8044 loss_val: 0.2993 acc_val: 0.9170\n",
            "Epoch: 0057 loss_train: 0.5811 acc_train: 0.8075 loss_val: 0.2872 acc_val: 0.9317\n",
            "Epoch: 0058 loss_train: 0.5516 acc_train: 0.8100 loss_val: 0.2851 acc_val: 0.9244\n",
            "Epoch: 0059 loss_train: 0.5341 acc_train: 0.8216 loss_val: 0.2676 acc_val: 0.9225\n",
            "Epoch: 0060 loss_train: 0.5052 acc_train: 0.8260 loss_val: 0.2572 acc_val: 0.9299\n",
            "Epoch: 0061 loss_train: 0.4866 acc_train: 0.8309 loss_val: 0.2578 acc_val: 0.9244\n",
            "Epoch: 0062 loss_train: 0.4661 acc_train: 0.8358 loss_val: 0.2483 acc_val: 0.9225\n",
            "Epoch: 0063 loss_train: 0.4453 acc_train: 0.8370 loss_val: 0.2489 acc_val: 0.9225\n",
            "Epoch: 0064 loss_train: 0.4243 acc_train: 0.8456 loss_val: 0.2578 acc_val: 0.9133\n",
            "Epoch: 0065 loss_train: 0.4032 acc_train: 0.8561 loss_val: 0.2612 acc_val: 0.9004\n",
            "Epoch: 0066 loss_train: 0.3839 acc_train: 0.8641 loss_val: 0.2588 acc_val: 0.9096\n",
            "Epoch: 0067 loss_train: 0.3664 acc_train: 0.8745 loss_val: 0.2679 acc_val: 0.9096\n",
            "Epoch: 0068 loss_train: 0.3503 acc_train: 0.8862 loss_val: 0.2828 acc_val: 0.9022\n",
            "Epoch: 0069 loss_train: 0.3303 acc_train: 0.8930 loss_val: 0.2997 acc_val: 0.8948\n",
            "Epoch: 0070 loss_train: 0.3164 acc_train: 0.8961 loss_val: 0.3156 acc_val: 0.8911\n",
            "Epoch: 0071 loss_train: 0.2984 acc_train: 0.9114 loss_val: 0.3360 acc_val: 0.8764\n",
            "Epoch: 0072 loss_train: 0.2791 acc_train: 0.9231 loss_val: 0.3516 acc_val: 0.8727\n",
            "Epoch: 0073 loss_train: 0.2633 acc_train: 0.9293 loss_val: 0.3685 acc_val: 0.8672\n",
            "Epoch: 0074 loss_train: 0.2474 acc_train: 0.9373 loss_val: 0.3810 acc_val: 0.8764\n",
            "Epoch: 0075 loss_train: 0.2312 acc_train: 0.9434 loss_val: 0.3941 acc_val: 0.8708\n",
            "Epoch: 0076 loss_train: 0.2137 acc_train: 0.9496 loss_val: 0.4024 acc_val: 0.8635\n",
            "Epoch: 0077 loss_train: 0.1995 acc_train: 0.9594 loss_val: 0.4142 acc_val: 0.8653\n",
            "Epoch: 0078 loss_train: 0.1856 acc_train: 0.9643 loss_val: 0.4286 acc_val: 0.8653\n",
            "Epoch: 0079 loss_train: 0.1680 acc_train: 0.9668 loss_val: 0.4439 acc_val: 0.8653\n",
            "Epoch: 0080 loss_train: 0.1551 acc_train: 0.9717 loss_val: 0.4542 acc_val: 0.8653\n",
            "Epoch: 0081 loss_train: 0.1427 acc_train: 0.9772 loss_val: 0.4646 acc_val: 0.8598\n",
            "Epoch: 0082 loss_train: 0.1273 acc_train: 0.9822 loss_val: 0.4805 acc_val: 0.8579\n",
            "Epoch: 0083 loss_train: 0.1168 acc_train: 0.9846 loss_val: 0.4863 acc_val: 0.8524\n",
            "Epoch: 0084 loss_train: 0.1047 acc_train: 0.9920 loss_val: 0.4974 acc_val: 0.8561\n",
            "Epoch: 0085 loss_train: 0.0922 acc_train: 0.9957 loss_val: 0.5106 acc_val: 0.8616\n",
            "Epoch: 0086 loss_train: 0.0828 acc_train: 0.9957 loss_val: 0.5232 acc_val: 0.8616\n",
            "Epoch: 0087 loss_train: 0.0726 acc_train: 0.9957 loss_val: 0.5252 acc_val: 0.8598\n",
            "Epoch: 0088 loss_train: 0.0660 acc_train: 0.9969 loss_val: 0.5248 acc_val: 0.8524\n",
            "Epoch: 0089 loss_train: 0.0553 acc_train: 0.9975 loss_val: 0.5370 acc_val: 0.8635\n",
            "Epoch: 0090 loss_train: 0.0484 acc_train: 0.9988 loss_val: 0.5468 acc_val: 0.8579\n",
            "Epoch: 0091 loss_train: 0.0401 acc_train: 0.9994 loss_val: 0.5551 acc_val: 0.8579\n",
            "Epoch: 0092 loss_train: 0.0358 acc_train: 0.9988 loss_val: 0.5572 acc_val: 0.8598\n",
            "Epoch: 0093 loss_train: 0.0322 acc_train: 0.9982 loss_val: 0.5548 acc_val: 0.8635\n",
            "Epoch: 0094 loss_train: 0.0283 acc_train: 0.9988 loss_val: 0.5517 acc_val: 0.8672\n",
            "Epoch: 0095 loss_train: 0.0232 acc_train: 0.9994 loss_val: 0.5532 acc_val: 0.8690\n",
            "Epoch: 0096 loss_train: 0.0200 acc_train: 1.0000 loss_val: 0.5552 acc_val: 0.8690\n",
            "Epoch: 0097 loss_train: 0.0170 acc_train: 1.0000 loss_val: 0.5602 acc_val: 0.8708\n",
            "Epoch: 0098 loss_train: 0.0146 acc_train: 1.0000 loss_val: 0.5674 acc_val: 0.8672\n",
            "Epoch: 0099 loss_train: 0.0135 acc_train: 1.0000 loss_val: 0.5710 acc_val: 0.8708\n",
            "Epoch: 0100 loss_train: 0.0123 acc_train: 0.9994 loss_val: 0.5711 acc_val: 0.8690\n",
            "Epoch: 0101 loss_train: 0.0102 acc_train: 1.0000 loss_val: 0.5733 acc_val: 0.8690\n",
            "Epoch: 0102 loss_train: 0.0089 acc_train: 1.0000 loss_val: 0.5764 acc_val: 0.8782\n",
            "Epoch: 0103 loss_train: 0.0077 acc_train: 1.0000 loss_val: 0.5771 acc_val: 0.8782\n",
            "Epoch: 0104 loss_train: 0.0070 acc_train: 1.0000 loss_val: 0.5759 acc_val: 0.8745\n",
            "Epoch: 0105 loss_train: 0.0068 acc_train: 0.9994 loss_val: 0.5774 acc_val: 0.8745\n",
            "Epoch: 0106 loss_train: 0.0056 acc_train: 1.0000 loss_val: 0.5815 acc_val: 0.8745\n",
            "Epoch: 0107 loss_train: 0.0061 acc_train: 1.0000 loss_val: 0.5850 acc_val: 0.8764\n",
            "Epoch: 0108 loss_train: 0.0047 acc_train: 1.0000 loss_val: 0.5901 acc_val: 0.8764\n",
            "Epoch: 0109 loss_train: 0.0043 acc_train: 1.0000 loss_val: 0.5968 acc_val: 0.8782\n",
            "Epoch: 0110 loss_train: 0.0041 acc_train: 1.0000 loss_val: 0.6035 acc_val: 0.8819\n",
            "Epoch: 0111 loss_train: 0.0044 acc_train: 1.0000 loss_val: 0.6053 acc_val: 0.8801\n",
            "Epoch: 0112 loss_train: 0.0035 acc_train: 1.0000 loss_val: 0.6051 acc_val: 0.8856\n",
            "Optimization Finished!\n",
            "Train cost: 19.2411s\n",
            "Loading 57th epoch\n",
            "Test set results: loss= 0.2645 accuracy= 0.9370\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --dataset cora --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 7  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack none --attperc 20 --TrainTest train --weight_decay=1e-05"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kdrw3upe_U8Z",
        "outputId": "a6c321b8-729e-4dc8-e70a-d9d505e6bacb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:92: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=1448, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=7, bias=True)\n",
            ")\n",
            "total params: 2979849\n",
            "Epoch: 0001 loss_train: 1.9728 acc_train: 0.1181 loss_val: 1.9696 acc_val: 0.1292\n",
            "Epoch: 0002 loss_train: 1.9676 acc_train: 0.1224 loss_val: 1.9576 acc_val: 0.1328\n",
            "Epoch: 0003 loss_train: 1.9555 acc_train: 0.1335 loss_val: 1.9396 acc_val: 0.1494\n",
            "Epoch: 0004 loss_train: 1.9401 acc_train: 0.1408 loss_val: 1.9160 acc_val: 0.1900\n",
            "Epoch: 0005 loss_train: 1.9207 acc_train: 0.1691 loss_val: 1.8871 acc_val: 0.2841\n",
            "Epoch: 0006 loss_train: 1.8928 acc_train: 0.2737 loss_val: 1.8534 acc_val: 0.4391\n",
            "Epoch: 0007 loss_train: 1.8617 acc_train: 0.3868 loss_val: 1.8154 acc_val: 0.4889\n",
            "Epoch: 0008 loss_train: 1.8266 acc_train: 0.4643 loss_val: 1.7737 acc_val: 0.4926\n",
            "Epoch: 0009 loss_train: 1.7898 acc_train: 0.4803 loss_val: 1.7294 acc_val: 0.5000\n",
            "Epoch: 0010 loss_train: 1.7486 acc_train: 0.4852 loss_val: 1.6833 acc_val: 0.4982\n",
            "Epoch: 0011 loss_train: 1.7045 acc_train: 0.4951 loss_val: 1.6359 acc_val: 0.5074\n",
            "Epoch: 0012 loss_train: 1.6584 acc_train: 0.5025 loss_val: 1.5869 acc_val: 0.5258\n",
            "Epoch: 0013 loss_train: 1.6110 acc_train: 0.5123 loss_val: 1.5362 acc_val: 0.5369\n",
            "Epoch: 0014 loss_train: 1.5621 acc_train: 0.5277 loss_val: 1.4841 acc_val: 0.5443\n",
            "Epoch: 0015 loss_train: 1.5086 acc_train: 0.5400 loss_val: 1.4308 acc_val: 0.5517\n",
            "Epoch: 0016 loss_train: 1.4569 acc_train: 0.5590 loss_val: 1.3765 acc_val: 0.5664\n",
            "Epoch: 0017 loss_train: 1.4009 acc_train: 0.5720 loss_val: 1.3209 acc_val: 0.5886\n",
            "Epoch: 0018 loss_train: 1.3440 acc_train: 0.5867 loss_val: 1.2648 acc_val: 0.6162\n",
            "Epoch: 0019 loss_train: 1.2865 acc_train: 0.6187 loss_val: 1.2096 acc_val: 0.6476\n",
            "Epoch: 0020 loss_train: 1.2281 acc_train: 0.6667 loss_val: 1.1571 acc_val: 0.6716\n",
            "Epoch: 0021 loss_train: 1.1716 acc_train: 0.6980 loss_val: 1.1082 acc_val: 0.6845\n",
            "Epoch: 0022 loss_train: 1.1186 acc_train: 0.7140 loss_val: 1.0629 acc_val: 0.6863\n",
            "Epoch: 0023 loss_train: 1.0670 acc_train: 0.7294 loss_val: 1.0204 acc_val: 0.6937\n",
            "Epoch: 0024 loss_train: 1.0172 acc_train: 0.7374 loss_val: 0.9796 acc_val: 0.7011\n",
            "Epoch: 0025 loss_train: 0.9698 acc_train: 0.7448 loss_val: 0.9404 acc_val: 0.7140\n",
            "Epoch: 0026 loss_train: 0.9250 acc_train: 0.7528 loss_val: 0.9028 acc_val: 0.7140\n",
            "Epoch: 0027 loss_train: 0.8804 acc_train: 0.7571 loss_val: 0.8673 acc_val: 0.7196\n",
            "Epoch: 0028 loss_train: 0.8415 acc_train: 0.7681 loss_val: 0.8345 acc_val: 0.7306\n",
            "Epoch: 0029 loss_train: 0.8027 acc_train: 0.7860 loss_val: 0.8041 acc_val: 0.7509\n",
            "Epoch: 0030 loss_train: 0.7666 acc_train: 0.8038 loss_val: 0.7756 acc_val: 0.7509\n",
            "Epoch: 0031 loss_train: 0.7349 acc_train: 0.8081 loss_val: 0.7482 acc_val: 0.7601\n",
            "Epoch: 0032 loss_train: 0.7022 acc_train: 0.8167 loss_val: 0.7220 acc_val: 0.7712\n",
            "Epoch: 0033 loss_train: 0.6729 acc_train: 0.8223 loss_val: 0.6974 acc_val: 0.7768\n",
            "Epoch: 0034 loss_train: 0.6426 acc_train: 0.8290 loss_val: 0.6744 acc_val: 0.7804\n",
            "Epoch: 0035 loss_train: 0.6143 acc_train: 0.8339 loss_val: 0.6515 acc_val: 0.7878\n",
            "Epoch: 0036 loss_train: 0.5888 acc_train: 0.8358 loss_val: 0.6277 acc_val: 0.7934\n",
            "Epoch: 0037 loss_train: 0.5615 acc_train: 0.8444 loss_val: 0.6027 acc_val: 0.8007\n",
            "Epoch: 0038 loss_train: 0.5367 acc_train: 0.8481 loss_val: 0.5775 acc_val: 0.8063\n",
            "Epoch: 0039 loss_train: 0.5121 acc_train: 0.8567 loss_val: 0.5526 acc_val: 0.8155\n",
            "Epoch: 0040 loss_train: 0.4885 acc_train: 0.8604 loss_val: 0.5287 acc_val: 0.8155\n",
            "Epoch: 0041 loss_train: 0.4642 acc_train: 0.8641 loss_val: 0.5062 acc_val: 0.8192\n",
            "Epoch: 0042 loss_train: 0.4419 acc_train: 0.8727 loss_val: 0.4858 acc_val: 0.8284\n",
            "Epoch: 0043 loss_train: 0.4212 acc_train: 0.8807 loss_val: 0.4661 acc_val: 0.8358\n",
            "Epoch: 0044 loss_train: 0.4022 acc_train: 0.8838 loss_val: 0.4460 acc_val: 0.8598\n",
            "Epoch: 0045 loss_train: 0.3811 acc_train: 0.8856 loss_val: 0.4268 acc_val: 0.8708\n",
            "Epoch: 0046 loss_train: 0.3592 acc_train: 0.8985 loss_val: 0.4109 acc_val: 0.8727\n",
            "Epoch: 0047 loss_train: 0.3401 acc_train: 0.8985 loss_val: 0.3993 acc_val: 0.8782\n",
            "Epoch: 0048 loss_train: 0.3222 acc_train: 0.9114 loss_val: 0.3918 acc_val: 0.8782\n",
            "Epoch: 0049 loss_train: 0.3052 acc_train: 0.9170 loss_val: 0.3864 acc_val: 0.8801\n",
            "Epoch: 0050 loss_train: 0.2876 acc_train: 0.9237 loss_val: 0.3802 acc_val: 0.8838\n",
            "Epoch: 0051 loss_train: 0.2712 acc_train: 0.9293 loss_val: 0.3756 acc_val: 0.8764\n",
            "Epoch: 0052 loss_train: 0.2548 acc_train: 0.9367 loss_val: 0.3746 acc_val: 0.8801\n",
            "Epoch: 0053 loss_train: 0.2357 acc_train: 0.9397 loss_val: 0.3766 acc_val: 0.8764\n",
            "Epoch: 0054 loss_train: 0.2187 acc_train: 0.9459 loss_val: 0.3791 acc_val: 0.8819\n",
            "Epoch: 0055 loss_train: 0.2021 acc_train: 0.9496 loss_val: 0.3802 acc_val: 0.8819\n",
            "Epoch: 0056 loss_train: 0.1876 acc_train: 0.9557 loss_val: 0.3797 acc_val: 0.8801\n",
            "Epoch: 0057 loss_train: 0.1714 acc_train: 0.9588 loss_val: 0.3814 acc_val: 0.8801\n",
            "Epoch: 0058 loss_train: 0.1570 acc_train: 0.9606 loss_val: 0.3851 acc_val: 0.8764\n",
            "Epoch: 0059 loss_train: 0.1457 acc_train: 0.9625 loss_val: 0.3911 acc_val: 0.8727\n",
            "Epoch: 0060 loss_train: 0.1333 acc_train: 0.9662 loss_val: 0.3964 acc_val: 0.8764\n",
            "Epoch: 0061 loss_train: 0.1227 acc_train: 0.9717 loss_val: 0.4023 acc_val: 0.8782\n",
            "Epoch: 0062 loss_train: 0.1113 acc_train: 0.9760 loss_val: 0.4099 acc_val: 0.8764\n",
            "Epoch: 0063 loss_train: 0.1029 acc_train: 0.9785 loss_val: 0.4203 acc_val: 0.8727\n",
            "Epoch: 0064 loss_train: 0.0919 acc_train: 0.9834 loss_val: 0.4294 acc_val: 0.8745\n",
            "Epoch: 0065 loss_train: 0.0847 acc_train: 0.9852 loss_val: 0.4345 acc_val: 0.8745\n",
            "Epoch: 0066 loss_train: 0.0759 acc_train: 0.9871 loss_val: 0.4387 acc_val: 0.8764\n",
            "Epoch: 0067 loss_train: 0.0705 acc_train: 0.9865 loss_val: 0.4446 acc_val: 0.8764\n",
            "Epoch: 0068 loss_train: 0.0631 acc_train: 0.9883 loss_val: 0.4549 acc_val: 0.8764\n",
            "Epoch: 0069 loss_train: 0.0579 acc_train: 0.9895 loss_val: 0.4656 acc_val: 0.8727\n",
            "Epoch: 0070 loss_train: 0.0517 acc_train: 0.9914 loss_val: 0.4741 acc_val: 0.8727\n",
            "Epoch: 0071 loss_train: 0.0478 acc_train: 0.9926 loss_val: 0.4778 acc_val: 0.8672\n",
            "Epoch: 0072 loss_train: 0.0429 acc_train: 0.9926 loss_val: 0.4800 acc_val: 0.8635\n",
            "Epoch: 0073 loss_train: 0.0389 acc_train: 0.9938 loss_val: 0.4849 acc_val: 0.8672\n",
            "Epoch: 0074 loss_train: 0.0352 acc_train: 0.9938 loss_val: 0.4926 acc_val: 0.8653\n",
            "Epoch: 0075 loss_train: 0.0314 acc_train: 0.9938 loss_val: 0.5027 acc_val: 0.8672\n",
            "Epoch: 0076 loss_train: 0.0287 acc_train: 0.9957 loss_val: 0.5130 acc_val: 0.8672\n",
            "Epoch: 0077 loss_train: 0.0266 acc_train: 0.9969 loss_val: 0.5193 acc_val: 0.8653\n",
            "Epoch: 0078 loss_train: 0.0238 acc_train: 0.9982 loss_val: 0.5221 acc_val: 0.8672\n",
            "Epoch: 0079 loss_train: 0.0216 acc_train: 0.9988 loss_val: 0.5228 acc_val: 0.8672\n",
            "Epoch: 0080 loss_train: 0.0199 acc_train: 0.9982 loss_val: 0.5258 acc_val: 0.8690\n",
            "Epoch: 0081 loss_train: 0.0179 acc_train: 0.9975 loss_val: 0.5325 acc_val: 0.8672\n",
            "Epoch: 0082 loss_train: 0.0161 acc_train: 0.9988 loss_val: 0.5423 acc_val: 0.8672\n",
            "Epoch: 0083 loss_train: 0.0142 acc_train: 0.9994 loss_val: 0.5545 acc_val: 0.8635\n",
            "Epoch: 0084 loss_train: 0.0127 acc_train: 0.9994 loss_val: 0.5660 acc_val: 0.8635\n",
            "Epoch: 0085 loss_train: 0.0118 acc_train: 0.9994 loss_val: 0.5745 acc_val: 0.8653\n",
            "Epoch: 0086 loss_train: 0.0101 acc_train: 0.9994 loss_val: 0.5798 acc_val: 0.8672\n",
            "Epoch: 0087 loss_train: 0.0100 acc_train: 0.9994 loss_val: 0.5826 acc_val: 0.8653\n",
            "Epoch: 0088 loss_train: 0.0086 acc_train: 0.9994 loss_val: 0.5862 acc_val: 0.8653\n",
            "Epoch: 0089 loss_train: 0.0080 acc_train: 0.9994 loss_val: 0.5914 acc_val: 0.8653\n",
            "Epoch: 0090 loss_train: 0.0071 acc_train: 0.9994 loss_val: 0.5987 acc_val: 0.8672\n",
            "Epoch: 0091 loss_train: 0.0066 acc_train: 0.9994 loss_val: 0.6060 acc_val: 0.8690\n",
            "Epoch: 0092 loss_train: 0.0060 acc_train: 1.0000 loss_val: 0.6127 acc_val: 0.8708\n",
            "Epoch: 0093 loss_train: 0.0054 acc_train: 1.0000 loss_val: 0.6192 acc_val: 0.8672\n",
            "Epoch: 0094 loss_train: 0.0050 acc_train: 1.0000 loss_val: 0.6254 acc_val: 0.8635\n",
            "Epoch: 0095 loss_train: 0.0047 acc_train: 1.0000 loss_val: 0.6320 acc_val: 0.8635\n",
            "Epoch: 0096 loss_train: 0.0041 acc_train: 1.0000 loss_val: 0.6377 acc_val: 0.8635\n",
            "Epoch: 0097 loss_train: 0.0039 acc_train: 1.0000 loss_val: 0.6425 acc_val: 0.8635\n",
            "Epoch: 0098 loss_train: 0.0037 acc_train: 1.0000 loss_val: 0.6470 acc_val: 0.8635\n",
            "Epoch: 0099 loss_train: 0.0034 acc_train: 1.0000 loss_val: 0.6508 acc_val: 0.8635\n",
            "Epoch: 0100 loss_train: 0.0031 acc_train: 1.0000 loss_val: 0.6542 acc_val: 0.8653\n",
            "Epoch: 0101 loss_train: 0.0030 acc_train: 1.0000 loss_val: 0.6573 acc_val: 0.8672\n",
            "Epoch: 0102 loss_train: 0.0029 acc_train: 1.0000 loss_val: 0.6602 acc_val: 0.8672\n",
            "Optimization Finished!\n",
            "Train cost: 17.2951s\n",
            "Loading 50th epoch\n",
            "Test set results: loss= 0.3530 accuracy= 0.9000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computer.pt"
      ],
      "metadata": {
        "id": "kFS2-4bSIoUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --dataset computer --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 7  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack dice --attperc 20 --TrainTest train --weight_decay=1e-05"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUgIrwWYHjkj",
        "outputId": "4455b234-d9a5-4c6f-8fe9-e6e839b2b544"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading /root/.dgl/amazon_co_buy_computer.zip from https://data.dgl.ai/dataset/amazon_co_buy_computer.zip...\n",
            "Extracting file to /root/.dgl/amazon_co_buy_computer\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/train.py\", line 98, in <module>\n",
            "    adj, features, labels, idx_train, idx_val, idx_test = get_dataset(args.dataset, args.pe_dim, \n",
            "  File \"/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/data.py\", line 112, in get_dataset\n",
            "    DICE_attack(graph, node, delta=int((attperc/100)*len(idx_train)))\n",
            "  File \"/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/data.py\", line 23, in DICE_attack\n",
            "    different_class_nodes = [node for node in graph.nodes() if node not in neighbors and graph.ndata['label'][node] != graph.ndata['label'][target_node]]\n",
            "  File \"/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/data.py\", line 23, in <listcomp>\n",
            "    different_class_nodes = [node for node in graph.nodes() if node not in neighbors and graph.ndata['label'][node] != graph.ndata['label'][target_node]]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py\", line 1850, in ndata\n",
            "    ntid = self.get_ntype_id(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py\", line 1158, in get_ntype_id\n",
            "    def get_ntype_id(self, ntype):\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --dataset computer --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 7  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack none --attperc 20 --TrainTest train --weight_decay=1e-05"
      ],
      "metadata": {
        "id": "XmrceydzIswc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "photo.pt"
      ],
      "metadata": {
        "id": "ic280SQkJAjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --dataset photo --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 7  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack dice --attperc 20 --TrainTest train --weight_decay=1e-05"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bCnt4p3JB8D",
        "outputId": "57c9990b-c56b-4aa3-e265-b7260a411672"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading /root/.dgl/amazon_co_buy_photo.zip from https://data.dgl.ai/dataset/amazon_co_buy_photo.zip...\n",
            "Extracting file to /root/.dgl/amazon_co_buy_photo\n",
            "\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nwwY1uDWJD1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "cora.pt\n"
      ],
      "metadata": {
        "id": "-ACboFQAJR5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --dataset cora --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 7  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack dice --attperc 20 --TrainTest train --weight_decay=1e-05"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TP2JZcTgJTiz",
        "outputId": "0228e3e2-2b6e-4621-c930-9a9a39c2d3c8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=1448, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=7, bias=True)\n",
            ")\n",
            "total params: 2979849\n",
            "Epoch: 0001 loss_train: 1.9703 acc_train: 0.1156 loss_val: 1.9655 acc_val: 0.1144\n",
            "Epoch: 0002 loss_train: 1.9629 acc_train: 0.1199 loss_val: 1.9551 acc_val: 0.1273\n",
            "Epoch: 0003 loss_train: 1.9549 acc_train: 0.1347 loss_val: 1.9403 acc_val: 0.1365\n",
            "Epoch: 0004 loss_train: 1.9438 acc_train: 0.1125 loss_val: 1.9220 acc_val: 0.1347\n",
            "Epoch: 0005 loss_train: 1.9274 acc_train: 0.1507 loss_val: 1.9020 acc_val: 0.3026\n",
            "Epoch: 0006 loss_train: 1.9088 acc_train: 0.2792 loss_val: 1.8824 acc_val: 0.3026\n",
            "Epoch: 0007 loss_train: 1.8894 acc_train: 0.3020 loss_val: 1.8642 acc_val: 0.3026\n",
            "Epoch: 0008 loss_train: 1.8723 acc_train: 0.3020 loss_val: 1.8492 acc_val: 0.3026\n",
            "Epoch: 0009 loss_train: 1.8581 acc_train: 0.3020 loss_val: 1.8383 acc_val: 0.3026\n",
            "Epoch: 0010 loss_train: 1.8462 acc_train: 0.3020 loss_val: 1.8308 acc_val: 0.3026\n",
            "Epoch: 0011 loss_train: 1.8380 acc_train: 0.3020 loss_val: 1.8267 acc_val: 0.3026\n",
            "Epoch: 0012 loss_train: 1.8327 acc_train: 0.3020 loss_val: 1.8252 acc_val: 0.3026\n",
            "Epoch: 0013 loss_train: 1.8288 acc_train: 0.3020 loss_val: 1.8245 acc_val: 0.3026\n",
            "Epoch: 0014 loss_train: 1.8279 acc_train: 0.3020 loss_val: 1.8235 acc_val: 0.3026\n",
            "Epoch: 0015 loss_train: 1.8265 acc_train: 0.3020 loss_val: 1.8216 acc_val: 0.3026\n",
            "Epoch: 0016 loss_train: 1.8240 acc_train: 0.3020 loss_val: 1.8188 acc_val: 0.3026\n",
            "Epoch: 0017 loss_train: 1.8229 acc_train: 0.3020 loss_val: 1.8152 acc_val: 0.3026\n",
            "Epoch: 0018 loss_train: 1.8184 acc_train: 0.3020 loss_val: 1.8115 acc_val: 0.3026\n",
            "Epoch: 0019 loss_train: 1.8186 acc_train: 0.3020 loss_val: 1.8080 acc_val: 0.3026\n",
            "Epoch: 0020 loss_train: 1.8170 acc_train: 0.3020 loss_val: 1.8046 acc_val: 0.3026\n",
            "Epoch: 0021 loss_train: 1.8133 acc_train: 0.3020 loss_val: 1.8009 acc_val: 0.3026\n",
            "Epoch: 0022 loss_train: 1.8111 acc_train: 0.3020 loss_val: 1.7962 acc_val: 0.3026\n",
            "Epoch: 0023 loss_train: 1.8093 acc_train: 0.3020 loss_val: 1.7898 acc_val: 0.3026\n",
            "Epoch: 0024 loss_train: 1.8023 acc_train: 0.3020 loss_val: 1.7818 acc_val: 0.3026\n",
            "Epoch: 0025 loss_train: 1.7975 acc_train: 0.3020 loss_val: 1.7722 acc_val: 0.3026\n",
            "Epoch: 0026 loss_train: 1.7896 acc_train: 0.3020 loss_val: 1.7596 acc_val: 0.3026\n",
            "Epoch: 0027 loss_train: 1.7816 acc_train: 0.3020 loss_val: 1.7412 acc_val: 0.3026\n",
            "Epoch: 0028 loss_train: 1.7666 acc_train: 0.3020 loss_val: 1.7137 acc_val: 0.3026\n",
            "Epoch: 0029 loss_train: 1.7465 acc_train: 0.3020 loss_val: 1.6777 acc_val: 0.3026\n",
            "Epoch: 0030 loss_train: 1.7210 acc_train: 0.3026 loss_val: 1.6355 acc_val: 0.3487\n",
            "Epoch: 0031 loss_train: 1.6918 acc_train: 0.3413 loss_val: 1.5806 acc_val: 0.4170\n",
            "Epoch: 0032 loss_train: 1.6512 acc_train: 0.4028 loss_val: 1.5123 acc_val: 0.4188\n",
            "Epoch: 0033 loss_train: 1.6034 acc_train: 0.3875 loss_val: 1.4253 acc_val: 0.4926\n",
            "Epoch: 0034 loss_train: 1.5441 acc_train: 0.4465 loss_val: 1.3285 acc_val: 0.6476\n",
            "Epoch: 0035 loss_train: 1.4844 acc_train: 0.5535 loss_val: 1.2397 acc_val: 0.6624\n",
            "Epoch: 0036 loss_train: 1.4195 acc_train: 0.5677 loss_val: 1.1604 acc_val: 0.6771\n",
            "Epoch: 0037 loss_train: 1.3668 acc_train: 0.5793 loss_val: 1.0915 acc_val: 0.7085\n",
            "Epoch: 0038 loss_train: 1.3191 acc_train: 0.6027 loss_val: 1.0266 acc_val: 0.7214\n",
            "Epoch: 0039 loss_train: 1.2737 acc_train: 0.6101 loss_val: 0.9633 acc_val: 0.7638\n",
            "Epoch: 0040 loss_train: 1.2200 acc_train: 0.6538 loss_val: 0.8931 acc_val: 0.7804\n",
            "Epoch: 0041 loss_train: 1.1647 acc_train: 0.6630 loss_val: 0.8302 acc_val: 0.7915\n",
            "Epoch: 0042 loss_train: 1.1143 acc_train: 0.6734 loss_val: 0.7783 acc_val: 0.8081\n",
            "Epoch: 0043 loss_train: 1.0733 acc_train: 0.6888 loss_val: 0.7262 acc_val: 0.8266\n",
            "Epoch: 0044 loss_train: 1.0268 acc_train: 0.7030 loss_val: 0.6773 acc_val: 0.8450\n",
            "Epoch: 0045 loss_train: 0.9852 acc_train: 0.7109 loss_val: 0.6190 acc_val: 0.8450\n",
            "Epoch: 0046 loss_train: 0.9423 acc_train: 0.7214 loss_val: 0.5949 acc_val: 0.8598\n",
            "Epoch: 0047 loss_train: 0.9076 acc_train: 0.7269 loss_val: 0.5311 acc_val: 0.8819\n",
            "Epoch: 0048 loss_train: 0.8620 acc_train: 0.7423 loss_val: 0.4770 acc_val: 0.8893\n",
            "Epoch: 0049 loss_train: 0.8142 acc_train: 0.7552 loss_val: 0.4734 acc_val: 0.8690\n",
            "Epoch: 0050 loss_train: 0.7952 acc_train: 0.7608 loss_val: 0.4127 acc_val: 0.9004\n",
            "Epoch: 0051 loss_train: 0.7465 acc_train: 0.7632 loss_val: 0.3942 acc_val: 0.9022\n",
            "Epoch: 0052 loss_train: 0.7243 acc_train: 0.7755 loss_val: 0.3640 acc_val: 0.9059\n",
            "Epoch: 0053 loss_train: 0.6830 acc_train: 0.7878 loss_val: 0.3599 acc_val: 0.9096\n",
            "Epoch: 0054 loss_train: 0.6612 acc_train: 0.7909 loss_val: 0.3290 acc_val: 0.9225\n",
            "Epoch: 0055 loss_train: 0.6274 acc_train: 0.8001 loss_val: 0.3113 acc_val: 0.9188\n",
            "Epoch: 0056 loss_train: 0.6080 acc_train: 0.8044 loss_val: 0.2993 acc_val: 0.9170\n",
            "Epoch: 0057 loss_train: 0.5811 acc_train: 0.8075 loss_val: 0.2872 acc_val: 0.9317\n",
            "Epoch: 0058 loss_train: 0.5516 acc_train: 0.8100 loss_val: 0.2851 acc_val: 0.9244\n",
            "Epoch: 0059 loss_train: 0.5341 acc_train: 0.8216 loss_val: 0.2676 acc_val: 0.9225\n",
            "Epoch: 0060 loss_train: 0.5052 acc_train: 0.8260 loss_val: 0.2572 acc_val: 0.9299\n",
            "Epoch: 0061 loss_train: 0.4866 acc_train: 0.8309 loss_val: 0.2578 acc_val: 0.9244\n",
            "Epoch: 0062 loss_train: 0.4661 acc_train: 0.8358 loss_val: 0.2483 acc_val: 0.9225\n",
            "Epoch: 0063 loss_train: 0.4453 acc_train: 0.8370 loss_val: 0.2489 acc_val: 0.9225\n",
            "Epoch: 0064 loss_train: 0.4243 acc_train: 0.8456 loss_val: 0.2578 acc_val: 0.9133\n",
            "Epoch: 0065 loss_train: 0.4032 acc_train: 0.8561 loss_val: 0.2612 acc_val: 0.9004\n",
            "Epoch: 0066 loss_train: 0.3839 acc_train: 0.8641 loss_val: 0.2588 acc_val: 0.9096\n",
            "Epoch: 0067 loss_train: 0.3664 acc_train: 0.8745 loss_val: 0.2679 acc_val: 0.9096\n",
            "Epoch: 0068 loss_train: 0.3503 acc_train: 0.8862 loss_val: 0.2828 acc_val: 0.9022\n",
            "Epoch: 0069 loss_train: 0.3303 acc_train: 0.8930 loss_val: 0.2997 acc_val: 0.8948\n",
            "Epoch: 0070 loss_train: 0.3164 acc_train: 0.8961 loss_val: 0.3156 acc_val: 0.8911\n",
            "Epoch: 0071 loss_train: 0.2984 acc_train: 0.9114 loss_val: 0.3360 acc_val: 0.8764\n",
            "Epoch: 0072 loss_train: 0.2791 acc_train: 0.9231 loss_val: 0.3516 acc_val: 0.8727\n",
            "Epoch: 0073 loss_train: 0.2633 acc_train: 0.9293 loss_val: 0.3685 acc_val: 0.8672\n",
            "Epoch: 0074 loss_train: 0.2474 acc_train: 0.9373 loss_val: 0.3810 acc_val: 0.8764\n",
            "Epoch: 0075 loss_train: 0.2312 acc_train: 0.9434 loss_val: 0.3941 acc_val: 0.8708\n",
            "Epoch: 0076 loss_train: 0.2137 acc_train: 0.9496 loss_val: 0.4024 acc_val: 0.8635\n",
            "Epoch: 0077 loss_train: 0.1995 acc_train: 0.9594 loss_val: 0.4142 acc_val: 0.8653\n",
            "Epoch: 0078 loss_train: 0.1856 acc_train: 0.9643 loss_val: 0.4286 acc_val: 0.8653\n",
            "Epoch: 0079 loss_train: 0.1680 acc_train: 0.9668 loss_val: 0.4439 acc_val: 0.8653\n",
            "Epoch: 0080 loss_train: 0.1551 acc_train: 0.9717 loss_val: 0.4542 acc_val: 0.8653\n",
            "Epoch: 0081 loss_train: 0.1427 acc_train: 0.9772 loss_val: 0.4646 acc_val: 0.8598\n",
            "Epoch: 0082 loss_train: 0.1273 acc_train: 0.9822 loss_val: 0.4805 acc_val: 0.8579\n",
            "Epoch: 0083 loss_train: 0.1168 acc_train: 0.9846 loss_val: 0.4863 acc_val: 0.8524\n",
            "Epoch: 0084 loss_train: 0.1047 acc_train: 0.9920 loss_val: 0.4974 acc_val: 0.8561\n",
            "Epoch: 0085 loss_train: 0.0922 acc_train: 0.9957 loss_val: 0.5106 acc_val: 0.8616\n",
            "Epoch: 0086 loss_train: 0.0828 acc_train: 0.9957 loss_val: 0.5232 acc_val: 0.8616\n",
            "Epoch: 0087 loss_train: 0.0726 acc_train: 0.9957 loss_val: 0.5252 acc_val: 0.8598\n",
            "Epoch: 0088 loss_train: 0.0660 acc_train: 0.9969 loss_val: 0.5248 acc_val: 0.8524\n",
            "Epoch: 0089 loss_train: 0.0553 acc_train: 0.9975 loss_val: 0.5370 acc_val: 0.8635\n",
            "Epoch: 0090 loss_train: 0.0484 acc_train: 0.9988 loss_val: 0.5468 acc_val: 0.8579\n",
            "Epoch: 0091 loss_train: 0.0401 acc_train: 0.9994 loss_val: 0.5551 acc_val: 0.8579\n",
            "Epoch: 0092 loss_train: 0.0358 acc_train: 0.9988 loss_val: 0.5572 acc_val: 0.8598\n",
            "Epoch: 0093 loss_train: 0.0322 acc_train: 0.9982 loss_val: 0.5548 acc_val: 0.8635\n",
            "Epoch: 0094 loss_train: 0.0283 acc_train: 0.9988 loss_val: 0.5517 acc_val: 0.8672\n",
            "Epoch: 0095 loss_train: 0.0232 acc_train: 0.9994 loss_val: 0.5532 acc_val: 0.8690\n",
            "Epoch: 0096 loss_train: 0.0200 acc_train: 1.0000 loss_val: 0.5552 acc_val: 0.8690\n",
            "Epoch: 0097 loss_train: 0.0170 acc_train: 1.0000 loss_val: 0.5602 acc_val: 0.8708\n",
            "Epoch: 0098 loss_train: 0.0146 acc_train: 1.0000 loss_val: 0.5674 acc_val: 0.8672\n",
            "Epoch: 0099 loss_train: 0.0135 acc_train: 1.0000 loss_val: 0.5710 acc_val: 0.8708\n",
            "Epoch: 0100 loss_train: 0.0123 acc_train: 0.9994 loss_val: 0.5711 acc_val: 0.8690\n",
            "Epoch: 0101 loss_train: 0.0102 acc_train: 1.0000 loss_val: 0.5733 acc_val: 0.8690\n",
            "Epoch: 0102 loss_train: 0.0089 acc_train: 1.0000 loss_val: 0.5764 acc_val: 0.8782\n",
            "Epoch: 0103 loss_train: 0.0077 acc_train: 1.0000 loss_val: 0.5771 acc_val: 0.8782\n",
            "Epoch: 0104 loss_train: 0.0070 acc_train: 1.0000 loss_val: 0.5759 acc_val: 0.8745\n",
            "Epoch: 0105 loss_train: 0.0068 acc_train: 0.9994 loss_val: 0.5774 acc_val: 0.8745\n",
            "Epoch: 0106 loss_train: 0.0056 acc_train: 1.0000 loss_val: 0.5815 acc_val: 0.8745\n",
            "Epoch: 0107 loss_train: 0.0061 acc_train: 1.0000 loss_val: 0.5850 acc_val: 0.8764\n",
            "Epoch: 0108 loss_train: 0.0047 acc_train: 1.0000 loss_val: 0.5901 acc_val: 0.8764\n",
            "Epoch: 0109 loss_train: 0.0043 acc_train: 1.0000 loss_val: 0.5968 acc_val: 0.8782\n",
            "Epoch: 0110 loss_train: 0.0041 acc_train: 1.0000 loss_val: 0.6035 acc_val: 0.8819\n",
            "Epoch: 0111 loss_train: 0.0044 acc_train: 1.0000 loss_val: 0.6053 acc_val: 0.8801\n",
            "Epoch: 0112 loss_train: 0.0035 acc_train: 1.0000 loss_val: 0.6051 acc_val: 0.8856\n",
            "Optimization Finished!\n",
            "Train cost: 18.4414s\n",
            "Loading 57th epoch\n",
            "Test set results: loss= 0.2645 accuracy= 0.9370\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --dataset cora --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 7  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack none --attperc 20 --TrainTest train --weight_decay=1e-05"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEsh1luyJVG-",
        "outputId": "82239bf9-840d-4661-ddce-d913dcbd7978"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=1448, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=7, bias=True)\n",
            ")\n",
            "total params: 2979849\n",
            "Epoch: 0001 loss_train: 1.9728 acc_train: 0.1181 loss_val: 1.9696 acc_val: 0.1292\n",
            "Epoch: 0002 loss_train: 1.9676 acc_train: 0.1224 loss_val: 1.9576 acc_val: 0.1328\n",
            "Epoch: 0003 loss_train: 1.9555 acc_train: 0.1335 loss_val: 1.9396 acc_val: 0.1494\n",
            "Epoch: 0004 loss_train: 1.9401 acc_train: 0.1408 loss_val: 1.9160 acc_val: 0.1900\n",
            "Epoch: 0005 loss_train: 1.9207 acc_train: 0.1691 loss_val: 1.8871 acc_val: 0.2841\n",
            "Epoch: 0006 loss_train: 1.8928 acc_train: 0.2737 loss_val: 1.8534 acc_val: 0.4391\n",
            "Epoch: 0007 loss_train: 1.8617 acc_train: 0.3868 loss_val: 1.8154 acc_val: 0.4889\n",
            "Epoch: 0008 loss_train: 1.8266 acc_train: 0.4643 loss_val: 1.7737 acc_val: 0.4926\n",
            "Epoch: 0009 loss_train: 1.7898 acc_train: 0.4803 loss_val: 1.7294 acc_val: 0.5000\n",
            "Epoch: 0010 loss_train: 1.7486 acc_train: 0.4852 loss_val: 1.6833 acc_val: 0.4982\n",
            "Epoch: 0011 loss_train: 1.7045 acc_train: 0.4951 loss_val: 1.6359 acc_val: 0.5074\n",
            "Epoch: 0012 loss_train: 1.6584 acc_train: 0.5025 loss_val: 1.5869 acc_val: 0.5258\n",
            "Epoch: 0013 loss_train: 1.6110 acc_train: 0.5123 loss_val: 1.5362 acc_val: 0.5369\n",
            "Epoch: 0014 loss_train: 1.5621 acc_train: 0.5277 loss_val: 1.4841 acc_val: 0.5443\n",
            "Epoch: 0015 loss_train: 1.5086 acc_train: 0.5400 loss_val: 1.4308 acc_val: 0.5517\n",
            "Epoch: 0016 loss_train: 1.4569 acc_train: 0.5590 loss_val: 1.3765 acc_val: 0.5664\n",
            "Epoch: 0017 loss_train: 1.4009 acc_train: 0.5720 loss_val: 1.3209 acc_val: 0.5886\n",
            "Epoch: 0018 loss_train: 1.3440 acc_train: 0.5867 loss_val: 1.2648 acc_val: 0.6162\n",
            "Epoch: 0019 loss_train: 1.2865 acc_train: 0.6187 loss_val: 1.2096 acc_val: 0.6476\n",
            "Epoch: 0020 loss_train: 1.2281 acc_train: 0.6667 loss_val: 1.1571 acc_val: 0.6716\n",
            "Epoch: 0021 loss_train: 1.1716 acc_train: 0.6980 loss_val: 1.1082 acc_val: 0.6845\n",
            "Epoch: 0022 loss_train: 1.1186 acc_train: 0.7140 loss_val: 1.0629 acc_val: 0.6863\n",
            "Epoch: 0023 loss_train: 1.0670 acc_train: 0.7294 loss_val: 1.0204 acc_val: 0.6937\n",
            "Epoch: 0024 loss_train: 1.0172 acc_train: 0.7374 loss_val: 0.9796 acc_val: 0.7011\n",
            "Epoch: 0025 loss_train: 0.9698 acc_train: 0.7448 loss_val: 0.9404 acc_val: 0.7140\n",
            "Epoch: 0026 loss_train: 0.9250 acc_train: 0.7528 loss_val: 0.9028 acc_val: 0.7140\n",
            "Epoch: 0027 loss_train: 0.8804 acc_train: 0.7571 loss_val: 0.8673 acc_val: 0.7196\n",
            "Epoch: 0028 loss_train: 0.8415 acc_train: 0.7681 loss_val: 0.8345 acc_val: 0.7306\n",
            "Epoch: 0029 loss_train: 0.8027 acc_train: 0.7860 loss_val: 0.8041 acc_val: 0.7509\n",
            "Epoch: 0030 loss_train: 0.7666 acc_train: 0.8038 loss_val: 0.7756 acc_val: 0.7509\n",
            "Epoch: 0031 loss_train: 0.7349 acc_train: 0.8081 loss_val: 0.7482 acc_val: 0.7601\n",
            "Epoch: 0032 loss_train: 0.7022 acc_train: 0.8167 loss_val: 0.7220 acc_val: 0.7712\n",
            "Epoch: 0033 loss_train: 0.6729 acc_train: 0.8223 loss_val: 0.6974 acc_val: 0.7768\n",
            "Epoch: 0034 loss_train: 0.6426 acc_train: 0.8290 loss_val: 0.6744 acc_val: 0.7804\n",
            "Epoch: 0035 loss_train: 0.6143 acc_train: 0.8339 loss_val: 0.6515 acc_val: 0.7878\n",
            "Epoch: 0036 loss_train: 0.5888 acc_train: 0.8358 loss_val: 0.6277 acc_val: 0.7934\n",
            "Epoch: 0037 loss_train: 0.5615 acc_train: 0.8444 loss_val: 0.6027 acc_val: 0.8007\n",
            "Epoch: 0038 loss_train: 0.5367 acc_train: 0.8481 loss_val: 0.5775 acc_val: 0.8063\n",
            "Epoch: 0039 loss_train: 0.5121 acc_train: 0.8567 loss_val: 0.5526 acc_val: 0.8155\n",
            "Epoch: 0040 loss_train: 0.4885 acc_train: 0.8604 loss_val: 0.5287 acc_val: 0.8155\n",
            "Epoch: 0041 loss_train: 0.4642 acc_train: 0.8641 loss_val: 0.5062 acc_val: 0.8192\n",
            "Epoch: 0042 loss_train: 0.4419 acc_train: 0.8727 loss_val: 0.4858 acc_val: 0.8284\n",
            "Epoch: 0043 loss_train: 0.4212 acc_train: 0.8807 loss_val: 0.4661 acc_val: 0.8358\n",
            "Epoch: 0044 loss_train: 0.4022 acc_train: 0.8838 loss_val: 0.4460 acc_val: 0.8598\n",
            "Epoch: 0045 loss_train: 0.3811 acc_train: 0.8856 loss_val: 0.4268 acc_val: 0.8708\n",
            "Epoch: 0046 loss_train: 0.3592 acc_train: 0.8985 loss_val: 0.4109 acc_val: 0.8727\n",
            "Epoch: 0047 loss_train: 0.3401 acc_train: 0.8985 loss_val: 0.3993 acc_val: 0.8782\n",
            "Epoch: 0048 loss_train: 0.3222 acc_train: 0.9114 loss_val: 0.3918 acc_val: 0.8782\n",
            "Epoch: 0049 loss_train: 0.3052 acc_train: 0.9170 loss_val: 0.3864 acc_val: 0.8801\n",
            "Epoch: 0050 loss_train: 0.2876 acc_train: 0.9237 loss_val: 0.3802 acc_val: 0.8838\n",
            "Epoch: 0051 loss_train: 0.2712 acc_train: 0.9293 loss_val: 0.3756 acc_val: 0.8764\n",
            "Epoch: 0052 loss_train: 0.2548 acc_train: 0.9367 loss_val: 0.3746 acc_val: 0.8801\n",
            "Epoch: 0053 loss_train: 0.2357 acc_train: 0.9397 loss_val: 0.3766 acc_val: 0.8764\n",
            "Epoch: 0054 loss_train: 0.2187 acc_train: 0.9459 loss_val: 0.3791 acc_val: 0.8819\n",
            "Epoch: 0055 loss_train: 0.2021 acc_train: 0.9496 loss_val: 0.3802 acc_val: 0.8819\n",
            "Epoch: 0056 loss_train: 0.1876 acc_train: 0.9557 loss_val: 0.3797 acc_val: 0.8801\n",
            "Epoch: 0057 loss_train: 0.1714 acc_train: 0.9588 loss_val: 0.3814 acc_val: 0.8801\n",
            "Epoch: 0058 loss_train: 0.1570 acc_train: 0.9606 loss_val: 0.3851 acc_val: 0.8764\n",
            "Epoch: 0059 loss_train: 0.1457 acc_train: 0.9625 loss_val: 0.3911 acc_val: 0.8727\n",
            "Epoch: 0060 loss_train: 0.1333 acc_train: 0.9662 loss_val: 0.3964 acc_val: 0.8764\n",
            "Epoch: 0061 loss_train: 0.1227 acc_train: 0.9717 loss_val: 0.4023 acc_val: 0.8782\n",
            "Epoch: 0062 loss_train: 0.1113 acc_train: 0.9760 loss_val: 0.4099 acc_val: 0.8764\n",
            "Epoch: 0063 loss_train: 0.1029 acc_train: 0.9785 loss_val: 0.4203 acc_val: 0.8727\n",
            "Epoch: 0064 loss_train: 0.0919 acc_train: 0.9834 loss_val: 0.4294 acc_val: 0.8745\n",
            "Epoch: 0065 loss_train: 0.0847 acc_train: 0.9852 loss_val: 0.4345 acc_val: 0.8745\n",
            "Epoch: 0066 loss_train: 0.0759 acc_train: 0.9871 loss_val: 0.4387 acc_val: 0.8764\n",
            "Epoch: 0067 loss_train: 0.0705 acc_train: 0.9865 loss_val: 0.4446 acc_val: 0.8764\n",
            "Epoch: 0068 loss_train: 0.0631 acc_train: 0.9883 loss_val: 0.4549 acc_val: 0.8764\n",
            "Epoch: 0069 loss_train: 0.0579 acc_train: 0.9895 loss_val: 0.4656 acc_val: 0.8727\n",
            "Epoch: 0070 loss_train: 0.0517 acc_train: 0.9914 loss_val: 0.4741 acc_val: 0.8727\n",
            "Epoch: 0071 loss_train: 0.0478 acc_train: 0.9926 loss_val: 0.4778 acc_val: 0.8672\n",
            "Epoch: 0072 loss_train: 0.0429 acc_train: 0.9926 loss_val: 0.4800 acc_val: 0.8635\n",
            "Epoch: 0073 loss_train: 0.0389 acc_train: 0.9938 loss_val: 0.4849 acc_val: 0.8672\n",
            "Epoch: 0074 loss_train: 0.0352 acc_train: 0.9938 loss_val: 0.4926 acc_val: 0.8653\n",
            "Epoch: 0075 loss_train: 0.0314 acc_train: 0.9938 loss_val: 0.5027 acc_val: 0.8672\n",
            "Epoch: 0076 loss_train: 0.0287 acc_train: 0.9957 loss_val: 0.5130 acc_val: 0.8672\n",
            "Epoch: 0077 loss_train: 0.0266 acc_train: 0.9969 loss_val: 0.5193 acc_val: 0.8653\n",
            "Epoch: 0078 loss_train: 0.0238 acc_train: 0.9982 loss_val: 0.5221 acc_val: 0.8672\n",
            "Epoch: 0079 loss_train: 0.0216 acc_train: 0.9988 loss_val: 0.5228 acc_val: 0.8672\n",
            "Epoch: 0080 loss_train: 0.0199 acc_train: 0.9982 loss_val: 0.5258 acc_val: 0.8690\n",
            "Epoch: 0081 loss_train: 0.0179 acc_train: 0.9975 loss_val: 0.5325 acc_val: 0.8672\n",
            "Epoch: 0082 loss_train: 0.0161 acc_train: 0.9988 loss_val: 0.5423 acc_val: 0.8672\n",
            "Epoch: 0083 loss_train: 0.0142 acc_train: 0.9994 loss_val: 0.5545 acc_val: 0.8635\n",
            "Epoch: 0084 loss_train: 0.0127 acc_train: 0.9994 loss_val: 0.5660 acc_val: 0.8635\n",
            "Epoch: 0085 loss_train: 0.0118 acc_train: 0.9994 loss_val: 0.5745 acc_val: 0.8653\n",
            "Epoch: 0086 loss_train: 0.0101 acc_train: 0.9994 loss_val: 0.5798 acc_val: 0.8672\n",
            "Epoch: 0087 loss_train: 0.0100 acc_train: 0.9994 loss_val: 0.5826 acc_val: 0.8653\n",
            "Epoch: 0088 loss_train: 0.0086 acc_train: 0.9994 loss_val: 0.5862 acc_val: 0.8653\n",
            "Epoch: 0089 loss_train: 0.0080 acc_train: 0.9994 loss_val: 0.5914 acc_val: 0.8653\n",
            "Epoch: 0090 loss_train: 0.0071 acc_train: 0.9994 loss_val: 0.5987 acc_val: 0.8672\n",
            "Epoch: 0091 loss_train: 0.0066 acc_train: 0.9994 loss_val: 0.6060 acc_val: 0.8690\n",
            "Epoch: 0092 loss_train: 0.0060 acc_train: 1.0000 loss_val: 0.6127 acc_val: 0.8708\n",
            "Epoch: 0093 loss_train: 0.0054 acc_train: 1.0000 loss_val: 0.6192 acc_val: 0.8672\n",
            "Epoch: 0094 loss_train: 0.0050 acc_train: 1.0000 loss_val: 0.6254 acc_val: 0.8635\n",
            "Epoch: 0095 loss_train: 0.0047 acc_train: 1.0000 loss_val: 0.6320 acc_val: 0.8635\n",
            "Epoch: 0096 loss_train: 0.0041 acc_train: 1.0000 loss_val: 0.6377 acc_val: 0.8635\n",
            "Epoch: 0097 loss_train: 0.0039 acc_train: 1.0000 loss_val: 0.6425 acc_val: 0.8635\n",
            "Epoch: 0098 loss_train: 0.0037 acc_train: 1.0000 loss_val: 0.6470 acc_val: 0.8635\n",
            "Epoch: 0099 loss_train: 0.0034 acc_train: 1.0000 loss_val: 0.6508 acc_val: 0.8635\n",
            "Epoch: 0100 loss_train: 0.0031 acc_train: 1.0000 loss_val: 0.6542 acc_val: 0.8653\n",
            "Epoch: 0101 loss_train: 0.0030 acc_train: 1.0000 loss_val: 0.6573 acc_val: 0.8672\n",
            "Epoch: 0102 loss_train: 0.0029 acc_train: 1.0000 loss_val: 0.6602 acc_val: 0.8672\n",
            "Optimization Finished!\n",
            "Train cost: 17.1951s\n",
            "Loading 50th epoch\n",
            "Test set results: loss= 0.3530 accuracy= 0.9000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pubmed.pt"
      ],
      "metadata": {
        "id": "FgBfscceJyx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --dataset pubmed --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 7  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63f82HrIJ8U5",
        "outputId": "0bc37704-c215-482e-83ca-0f337c3ef34a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=515, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=3, bias=True)\n",
            ")\n",
            "total params: 2501125\n",
            "Epoch: 0001 loss_train: 6.5268 acc_train: 0.3937 loss_val: 2.0050 acc_val: 0.5621\n",
            "Epoch: 0002 loss_train: 5.6107 acc_train: 0.6451 loss_val: 1.6404 acc_val: 0.6889\n",
            "Epoch: 0003 loss_train: 4.5798 acc_train: 0.7072 loss_val: 1.2995 acc_val: 0.7518\n",
            "Epoch: 0004 loss_train: 3.6574 acc_train: 0.7731 loss_val: 1.0907 acc_val: 0.7875\n",
            "Epoch: 0005 loss_train: 3.1752 acc_train: 0.7946 loss_val: 1.0121 acc_val: 0.8055\n",
            "Epoch: 0006 loss_train: 2.9188 acc_train: 0.8090 loss_val: 0.9214 acc_val: 0.8230\n",
            "Epoch: 0007 loss_train: 2.6477 acc_train: 0.8278 loss_val: 0.8484 acc_val: 0.8347\n",
            "Epoch: 0008 loss_train: 2.4033 acc_train: 0.8418 loss_val: 0.7762 acc_val: 0.8537\n",
            "Epoch: 0009 loss_train: 2.1739 acc_train: 0.8575 loss_val: 0.7444 acc_val: 0.8631\n",
            "Epoch: 0010 loss_train: 2.0221 acc_train: 0.8680 loss_val: 0.7093 acc_val: 0.8671\n",
            "Epoch: 0011 loss_train: 1.8280 acc_train: 0.8831 loss_val: 0.6645 acc_val: 0.8765\n",
            "Epoch: 0012 loss_train: 1.7502 acc_train: 0.8883 loss_val: 0.6576 acc_val: 0.8793\n",
            "Epoch: 0013 loss_train: 1.5969 acc_train: 0.8994 loss_val: 0.6499 acc_val: 0.8801\n",
            "Epoch: 0014 loss_train: 1.5122 acc_train: 0.9037 loss_val: 0.6885 acc_val: 0.8760\n",
            "Epoch: 0015 loss_train: 1.4774 acc_train: 0.9064 loss_val: 0.6343 acc_val: 0.8867\n",
            "Epoch: 0016 loss_train: 1.3783 acc_train: 0.9145 loss_val: 0.6252 acc_val: 0.8834\n",
            "Epoch: 0017 loss_train: 1.2856 acc_train: 0.9230 loss_val: 0.6251 acc_val: 0.8897\n",
            "Epoch: 0018 loss_train: 1.1536 acc_train: 0.9307 loss_val: 0.6529 acc_val: 0.8854\n",
            "Epoch: 0019 loss_train: 1.0949 acc_train: 0.9336 loss_val: 0.6890 acc_val: 0.8851\n",
            "Epoch: 0020 loss_train: 1.0392 acc_train: 0.9375 loss_val: 0.6600 acc_val: 0.8887\n",
            "Epoch: 0021 loss_train: 0.9177 acc_train: 0.9467 loss_val: 0.6957 acc_val: 0.8887\n",
            "Epoch: 0022 loss_train: 0.8809 acc_train: 0.9458 loss_val: 0.7379 acc_val: 0.8803\n",
            "Epoch: 0023 loss_train: 0.8144 acc_train: 0.9530 loss_val: 0.7723 acc_val: 0.8826\n",
            "Epoch: 0024 loss_train: 0.7890 acc_train: 0.9523 loss_val: 0.8411 acc_val: 0.8747\n",
            "Epoch: 0025 loss_train: 0.7780 acc_train: 0.9509 loss_val: 0.8303 acc_val: 0.8783\n",
            "Epoch: 0026 loss_train: 0.7116 acc_train: 0.9564 loss_val: 0.8051 acc_val: 0.8839\n",
            "Epoch: 0027 loss_train: 0.5610 acc_train: 0.9684 loss_val: 0.8101 acc_val: 0.8869\n",
            "Epoch: 0028 loss_train: 0.4670 acc_train: 0.9745 loss_val: 0.9474 acc_val: 0.8780\n",
            "Epoch: 0029 loss_train: 0.4295 acc_train: 0.9751 loss_val: 0.9985 acc_val: 0.8760\n",
            "Epoch: 0030 loss_train: 0.3867 acc_train: 0.9766 loss_val: 1.0297 acc_val: 0.8813\n",
            "Epoch: 0031 loss_train: 0.3107 acc_train: 0.9823 loss_val: 1.0278 acc_val: 0.8841\n",
            "Epoch: 0032 loss_train: 0.2900 acc_train: 0.9840 loss_val: 1.1981 acc_val: 0.8763\n",
            "Epoch: 0033 loss_train: 0.7144 acc_train: 0.9576 loss_val: 1.1541 acc_val: 0.8605\n",
            "Epoch: 0034 loss_train: 0.6196 acc_train: 0.9594 loss_val: 1.0990 acc_val: 0.8826\n",
            "Epoch: 0035 loss_train: 0.6583 acc_train: 0.9623 loss_val: 1.1192 acc_val: 0.8770\n",
            "Epoch: 0036 loss_train: 0.5769 acc_train: 0.9657 loss_val: 0.8821 acc_val: 0.8778\n",
            "Epoch: 0037 loss_train: 0.3560 acc_train: 0.9807 loss_val: 0.9838 acc_val: 0.8816\n",
            "Epoch: 0038 loss_train: 0.2502 acc_train: 0.9858 loss_val: 1.0691 acc_val: 0.8785\n",
            "Epoch: 0039 loss_train: 0.1834 acc_train: 0.9899 loss_val: 1.1508 acc_val: 0.8803\n",
            "Epoch: 0040 loss_train: 0.1176 acc_train: 0.9932 loss_val: 1.1723 acc_val: 0.8770\n",
            "Epoch: 0041 loss_train: 0.0921 acc_train: 0.9954 loss_val: 1.2731 acc_val: 0.8791\n",
            "Epoch: 0042 loss_train: 0.0806 acc_train: 0.9963 loss_val: 1.3150 acc_val: 0.8824\n",
            "Epoch: 0043 loss_train: 0.0630 acc_train: 0.9975 loss_val: 1.3461 acc_val: 0.8851\n",
            "Epoch: 0044 loss_train: 0.0423 acc_train: 0.9982 loss_val: 1.4321 acc_val: 0.8834\n",
            "Epoch: 0045 loss_train: 0.0405 acc_train: 0.9981 loss_val: 1.4822 acc_val: 0.8816\n",
            "Epoch: 0046 loss_train: 0.0403 acc_train: 0.9978 loss_val: 1.4735 acc_val: 0.8803\n",
            "Epoch: 0047 loss_train: 0.0452 acc_train: 0.9975 loss_val: 1.5169 acc_val: 0.8818\n",
            "Epoch: 0048 loss_train: 0.0573 acc_train: 0.9966 loss_val: 1.5403 acc_val: 0.8811\n",
            "Epoch: 0049 loss_train: 0.0507 acc_train: 0.9970 loss_val: 1.6027 acc_val: 0.8780\n",
            "Epoch: 0050 loss_train: 0.0522 acc_train: 0.9970 loss_val: 1.5824 acc_val: 0.8791\n",
            "Epoch: 0051 loss_train: 0.0622 acc_train: 0.9969 loss_val: 1.6464 acc_val: 0.8753\n",
            "Epoch: 0052 loss_train: 0.0543 acc_train: 0.9969 loss_val: 1.6753 acc_val: 0.8742\n",
            "Epoch: 0053 loss_train: 0.0756 acc_train: 0.9961 loss_val: 1.7133 acc_val: 0.8735\n",
            "Epoch: 0054 loss_train: 0.0723 acc_train: 0.9958 loss_val: 1.7299 acc_val: 0.8770\n",
            "Epoch: 0055 loss_train: 0.1142 acc_train: 0.9930 loss_val: 2.0479 acc_val: 0.8514\n",
            "Epoch: 0056 loss_train: 5.6391 acc_train: 0.7626 loss_val: 2.3017 acc_val: 0.5294\n",
            "Epoch: 0057 loss_train: 4.7769 acc_train: 0.6653 loss_val: 1.0550 acc_val: 0.7918\n",
            "Epoch: 0058 loss_train: 2.7487 acc_train: 0.8244 loss_val: 0.8637 acc_val: 0.8347\n",
            "Epoch: 0059 loss_train: 3.2688 acc_train: 0.7867 loss_val: 0.9494 acc_val: 0.8012\n",
            "Epoch: 0060 loss_train: 2.5691 acc_train: 0.8255 loss_val: 0.9026 acc_val: 0.8131\n",
            "Epoch: 0061 loss_train: 2.3066 acc_train: 0.8536 loss_val: 0.7375 acc_val: 0.8598\n",
            "Epoch: 0062 loss_train: 2.0242 acc_train: 0.8661 loss_val: 0.7294 acc_val: 0.8646\n",
            "Epoch: 0063 loss_train: 1.9044 acc_train: 0.8779 loss_val: 0.6774 acc_val: 0.8687\n",
            "Epoch: 0064 loss_train: 1.7796 acc_train: 0.8870 loss_val: 0.6624 acc_val: 0.8709\n",
            "Epoch: 0065 loss_train: 1.6959 acc_train: 0.8922 loss_val: 0.6309 acc_val: 0.8791\n",
            "Epoch: 0066 loss_train: 1.6078 acc_train: 0.8992 loss_val: 0.6236 acc_val: 0.8806\n",
            "Epoch: 0067 loss_train: 1.5173 acc_train: 0.9060 loss_val: 0.6224 acc_val: 0.8806\n",
            "Epoch: 0068 loss_train: 1.4677 acc_train: 0.9092 loss_val: 0.6067 acc_val: 0.8851\n",
            "Epoch: 0069 loss_train: 1.3689 acc_train: 0.9154 loss_val: 0.6076 acc_val: 0.8867\n",
            "Epoch: 0070 loss_train: 1.3036 acc_train: 0.9176 loss_val: 0.6205 acc_val: 0.8851\n",
            "Epoch: 0071 loss_train: 1.2713 acc_train: 0.9203 loss_val: 0.6351 acc_val: 0.8905\n",
            "Epoch: 0072 loss_train: 1.2381 acc_train: 0.9236 loss_val: 0.6851 acc_val: 0.8750\n",
            "Epoch: 0073 loss_train: 1.2760 acc_train: 0.9182 loss_val: 0.6240 acc_val: 0.8872\n",
            "Epoch: 0074 loss_train: 1.2092 acc_train: 0.9250 loss_val: 0.6453 acc_val: 0.8869\n",
            "Epoch: 0075 loss_train: 1.1753 acc_train: 0.9275 loss_val: 0.6733 acc_val: 0.8796\n",
            "Epoch: 0076 loss_train: 1.0985 acc_train: 0.9304 loss_val: 0.6581 acc_val: 0.8889\n",
            "Epoch: 0077 loss_train: 1.0217 acc_train: 0.9390 loss_val: 0.6515 acc_val: 0.8846\n",
            "Epoch: 0078 loss_train: 0.9535 acc_train: 0.9441 loss_val: 0.6804 acc_val: 0.8915\n",
            "Epoch: 0079 loss_train: 0.9203 acc_train: 0.9451 loss_val: 0.7042 acc_val: 0.8895\n",
            "Epoch: 0080 loss_train: 0.8812 acc_train: 0.9471 loss_val: 0.7121 acc_val: 0.8884\n",
            "Epoch: 0081 loss_train: 0.8589 acc_train: 0.9501 loss_val: 0.7313 acc_val: 0.8859\n",
            "Epoch: 0082 loss_train: 0.7780 acc_train: 0.9577 loss_val: 0.7680 acc_val: 0.8889\n",
            "Epoch: 0083 loss_train: 0.7578 acc_train: 0.9568 loss_val: 0.7519 acc_val: 0.8864\n",
            "Epoch: 0084 loss_train: 0.7270 acc_train: 0.9597 loss_val: 0.8329 acc_val: 0.8796\n",
            "Epoch: 0085 loss_train: 0.9022 acc_train: 0.9493 loss_val: 0.8528 acc_val: 0.8697\n",
            "Epoch: 0086 loss_train: 1.0906 acc_train: 0.9307 loss_val: 0.6755 acc_val: 0.8877\n",
            "Epoch: 0087 loss_train: 0.9863 acc_train: 0.9408 loss_val: 0.6651 acc_val: 0.8912\n",
            "Epoch: 0088 loss_train: 0.8798 acc_train: 0.9475 loss_val: 0.6891 acc_val: 0.8859\n",
            "Epoch: 0089 loss_train: 0.7700 acc_train: 0.9553 loss_val: 0.6914 acc_val: 0.8887\n",
            "Epoch: 0090 loss_train: 0.7212 acc_train: 0.9589 loss_val: 0.7566 acc_val: 0.8920\n",
            "Epoch: 0091 loss_train: 0.6481 acc_train: 0.9633 loss_val: 0.7884 acc_val: 0.8846\n",
            "Epoch: 0092 loss_train: 0.6038 acc_train: 0.9670 loss_val: 0.8251 acc_val: 0.8839\n",
            "Epoch: 0093 loss_train: 0.5660 acc_train: 0.9691 loss_val: 0.8529 acc_val: 0.8851\n",
            "Epoch: 0094 loss_train: 0.5659 acc_train: 0.9702 loss_val: 0.8750 acc_val: 0.8829\n",
            "Epoch: 0095 loss_train: 0.5375 acc_train: 0.9702 loss_val: 0.8799 acc_val: 0.8859\n",
            "Epoch: 0096 loss_train: 0.5349 acc_train: 0.9714 loss_val: 0.8961 acc_val: 0.8874\n",
            "Epoch: 0097 loss_train: 0.4617 acc_train: 0.9767 loss_val: 0.9232 acc_val: 0.8854\n",
            "Epoch: 0098 loss_train: 0.4605 acc_train: 0.9757 loss_val: 0.9660 acc_val: 0.8791\n",
            "Epoch: 0099 loss_train: 0.4736 acc_train: 0.9759 loss_val: 0.9793 acc_val: 0.8829\n",
            "Epoch: 0100 loss_train: 0.4557 acc_train: 0.9761 loss_val: 0.9576 acc_val: 0.8826\n",
            "Epoch: 0101 loss_train: 0.4262 acc_train: 0.9791 loss_val: 0.9954 acc_val: 0.8841\n",
            "Epoch: 0102 loss_train: 0.4439 acc_train: 0.9765 loss_val: 1.0609 acc_val: 0.8798\n",
            "Epoch: 0103 loss_train: 0.5062 acc_train: 0.9711 loss_val: 0.9898 acc_val: 0.8834\n",
            "Epoch: 0104 loss_train: 0.4448 acc_train: 0.9768 loss_val: 0.9441 acc_val: 0.8887\n",
            "Epoch: 0105 loss_train: 0.4457 acc_train: 0.9773 loss_val: 0.9701 acc_val: 0.8801\n",
            "Epoch: 0106 loss_train: 0.4389 acc_train: 0.9767 loss_val: 1.0631 acc_val: 0.8763\n",
            "Epoch: 0107 loss_train: 0.4121 acc_train: 0.9794 loss_val: 0.9949 acc_val: 0.8793\n",
            "Epoch: 0108 loss_train: 0.3538 acc_train: 0.9837 loss_val: 1.0254 acc_val: 0.8824\n",
            "Epoch: 0109 loss_train: 0.3436 acc_train: 0.9836 loss_val: 1.0270 acc_val: 0.8839\n",
            "Epoch: 0110 loss_train: 0.3398 acc_train: 0.9838 loss_val: 1.0582 acc_val: 0.8829\n",
            "Epoch: 0111 loss_train: 0.3228 acc_train: 0.9840 loss_val: 1.0969 acc_val: 0.8798\n",
            "Epoch: 0112 loss_train: 0.3675 acc_train: 0.9806 loss_val: 1.0711 acc_val: 0.8829\n",
            "Epoch: 0113 loss_train: 0.3062 acc_train: 0.9853 loss_val: 1.0910 acc_val: 0.8816\n",
            "Epoch: 0114 loss_train: 0.2863 acc_train: 0.9860 loss_val: 1.1241 acc_val: 0.8775\n",
            "Epoch: 0115 loss_train: 0.2794 acc_train: 0.9877 loss_val: 1.1985 acc_val: 0.8798\n",
            "Epoch: 0116 loss_train: 0.2628 acc_train: 0.9877 loss_val: 1.1814 acc_val: 0.8780\n",
            "Epoch: 0117 loss_train: 0.2706 acc_train: 0.9870 loss_val: 1.2116 acc_val: 0.8811\n",
            "Epoch: 0118 loss_train: 0.2647 acc_train: 0.9873 loss_val: 1.1897 acc_val: 0.8811\n",
            "Epoch: 0119 loss_train: 0.2516 acc_train: 0.9891 loss_val: 1.1783 acc_val: 0.8824\n",
            "Epoch: 0120 loss_train: 0.2204 acc_train: 0.9901 loss_val: 1.2201 acc_val: 0.8798\n",
            "Epoch: 0121 loss_train: 0.2276 acc_train: 0.9901 loss_val: 1.2666 acc_val: 0.8793\n",
            "Epoch: 0122 loss_train: 0.2196 acc_train: 0.9899 loss_val: 1.2593 acc_val: 0.8801\n",
            "Epoch: 0123 loss_train: 0.1977 acc_train: 0.9910 loss_val: 1.2743 acc_val: 0.8818\n",
            "Epoch: 0124 loss_train: 0.1905 acc_train: 0.9919 loss_val: 1.3226 acc_val: 0.8773\n",
            "Epoch: 0125 loss_train: 0.1880 acc_train: 0.9926 loss_val: 1.3028 acc_val: 0.8801\n",
            "Epoch: 0126 loss_train: 0.1949 acc_train: 0.9920 loss_val: 1.3192 acc_val: 0.8780\n",
            "Epoch: 0127 loss_train: 0.2209 acc_train: 0.9902 loss_val: 1.3070 acc_val: 0.8758\n",
            "Epoch: 0128 loss_train: 0.2622 acc_train: 0.9866 loss_val: 1.2770 acc_val: 0.8829\n",
            "Epoch: 0129 loss_train: 0.2576 acc_train: 0.9884 loss_val: 1.2361 acc_val: 0.8785\n",
            "Epoch: 0130 loss_train: 0.2407 acc_train: 0.9892 loss_val: 1.3049 acc_val: 0.8765\n",
            "Epoch: 0131 loss_train: 0.2296 acc_train: 0.9901 loss_val: 1.2518 acc_val: 0.8791\n",
            "Epoch: 0132 loss_train: 0.2209 acc_train: 0.9899 loss_val: 1.2675 acc_val: 0.8803\n",
            "Epoch: 0133 loss_train: 0.2049 acc_train: 0.9909 loss_val: 1.2856 acc_val: 0.8796\n",
            "Epoch: 0134 loss_train: 0.1963 acc_train: 0.9913 loss_val: 1.2263 acc_val: 0.8803\n",
            "Epoch: 0135 loss_train: 0.2195 acc_train: 0.9897 loss_val: 1.2534 acc_val: 0.8826\n",
            "Epoch: 0136 loss_train: 0.1919 acc_train: 0.9917 loss_val: 1.3379 acc_val: 0.8778\n",
            "Epoch: 0137 loss_train: 0.1735 acc_train: 0.9931 loss_val: 1.2871 acc_val: 0.8829\n",
            "Epoch: 0138 loss_train: 0.1707 acc_train: 0.9933 loss_val: 1.3179 acc_val: 0.8818\n",
            "Epoch: 0139 loss_train: 0.1558 acc_train: 0.9936 loss_val: 1.3507 acc_val: 0.8831\n",
            "Epoch: 0140 loss_train: 0.1588 acc_train: 0.9929 loss_val: 1.3458 acc_val: 0.8806\n",
            "Optimization Finished!\n",
            "Train cost: 105.7145s\n",
            "Loading 90th epoch\n",
            "Test set results: loss= 0.7947 accuracy= 0.8813\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --dataset pubmed --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 7  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack l2_strong --attperc 20 --TrainTest train --weight_decay=1e-05"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qYvK6LiKy2Q",
        "outputId": "9ed1ce55-13cd-46e9-8151-80b4e58b453a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:354: DGLWarning: DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\n",
            "  dgl_warning(\"DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\")\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=515, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=3, bias=True)\n",
            ")\n",
            "total params: 2501125\n",
            "Epoch: 0001 loss_train: 6.4231 acc_train: 0.4138 loss_val: 1.9984 acc_val: 0.6032\n",
            "Epoch: 0002 loss_train: 5.5705 acc_train: 0.6688 loss_val: 1.6777 acc_val: 0.6562\n",
            "Epoch: 0003 loss_train: 4.5290 acc_train: 0.7005 loss_val: 1.3451 acc_val: 0.7551\n",
            "Epoch: 0004 loss_train: 3.5464 acc_train: 0.7857 loss_val: 1.1296 acc_val: 0.7984\n",
            "Epoch: 0005 loss_train: 2.9477 acc_train: 0.8142 loss_val: 1.0064 acc_val: 0.8134\n",
            "Epoch: 0006 loss_train: 2.6665 acc_train: 0.8336 loss_val: 0.9335 acc_val: 0.8215\n",
            "Epoch: 0007 loss_train: 2.4186 acc_train: 0.8458 loss_val: 0.8532 acc_val: 0.8339\n",
            "Epoch: 0008 loss_train: 2.1657 acc_train: 0.8582 loss_val: 0.7927 acc_val: 0.8471\n",
            "Epoch: 0009 loss_train: 1.9743 acc_train: 0.8735 loss_val: 0.7560 acc_val: 0.8608\n",
            "Epoch: 0010 loss_train: 1.8308 acc_train: 0.8812 loss_val: 0.7232 acc_val: 0.8671\n",
            "Epoch: 0011 loss_train: 1.6912 acc_train: 0.8897 loss_val: 0.6928 acc_val: 0.8694\n",
            "Epoch: 0012 loss_train: 1.5390 acc_train: 0.9022 loss_val: 0.6760 acc_val: 0.8737\n",
            "Epoch: 0013 loss_train: 1.4368 acc_train: 0.9095 loss_val: 0.6773 acc_val: 0.8783\n",
            "Epoch: 0014 loss_train: 1.3392 acc_train: 0.9155 loss_val: 0.6917 acc_val: 0.8785\n",
            "Epoch: 0015 loss_train: 1.2569 acc_train: 0.9221 loss_val: 0.7062 acc_val: 0.8768\n",
            "Epoch: 0016 loss_train: 1.1998 acc_train: 0.9244 loss_val: 0.6779 acc_val: 0.8763\n",
            "Epoch: 0017 loss_train: 1.1570 acc_train: 0.9266 loss_val: 0.7111 acc_val: 0.8763\n",
            "Epoch: 0018 loss_train: 1.1495 acc_train: 0.9237 loss_val: 0.6846 acc_val: 0.8821\n",
            "Epoch: 0019 loss_train: 1.0540 acc_train: 0.9330 loss_val: 0.6777 acc_val: 0.8834\n",
            "Epoch: 0020 loss_train: 0.9090 acc_train: 0.9425 loss_val: 0.7236 acc_val: 0.8813\n",
            "Epoch: 0021 loss_train: 0.8405 acc_train: 0.9486 loss_val: 0.7337 acc_val: 0.8834\n",
            "Epoch: 0022 loss_train: 0.7790 acc_train: 0.9514 loss_val: 0.7898 acc_val: 0.8770\n",
            "Epoch: 0023 loss_train: 0.8742 acc_train: 0.9437 loss_val: 0.8407 acc_val: 0.8727\n",
            "Epoch: 0024 loss_train: 0.7385 acc_train: 0.9538 loss_val: 0.7662 acc_val: 0.8829\n",
            "Epoch: 0025 loss_train: 0.5946 acc_train: 0.9626 loss_val: 0.7979 acc_val: 0.8872\n",
            "Epoch: 0026 loss_train: 0.4760 acc_train: 0.9735 loss_val: 0.8355 acc_val: 0.8841\n",
            "Epoch: 0027 loss_train: 0.4281 acc_train: 0.9759 loss_val: 0.9632 acc_val: 0.8793\n",
            "Epoch: 0028 loss_train: 0.3779 acc_train: 0.9771 loss_val: 0.9869 acc_val: 0.8806\n",
            "Epoch: 0029 loss_train: 0.3395 acc_train: 0.9798 loss_val: 1.0269 acc_val: 0.8687\n",
            "Epoch: 0030 loss_train: 0.4532 acc_train: 0.9698 loss_val: 1.0218 acc_val: 0.8768\n",
            "Epoch: 0031 loss_train: 0.3775 acc_train: 0.9773 loss_val: 1.0309 acc_val: 0.8836\n",
            "Epoch: 0032 loss_train: 0.2779 acc_train: 0.9842 loss_val: 1.1242 acc_val: 0.8803\n",
            "Epoch: 0033 loss_train: 0.2233 acc_train: 0.9869 loss_val: 1.1919 acc_val: 0.8755\n",
            "Epoch: 0034 loss_train: 0.1833 acc_train: 0.9887 loss_val: 1.3566 acc_val: 0.8773\n",
            "Epoch: 0035 loss_train: 0.2033 acc_train: 0.9871 loss_val: 1.2961 acc_val: 0.8788\n",
            "Epoch: 0036 loss_train: 0.1919 acc_train: 0.9890 loss_val: 1.2947 acc_val: 0.8750\n",
            "Epoch: 0037 loss_train: 0.1800 acc_train: 0.9888 loss_val: 1.4649 acc_val: 0.8722\n",
            "Epoch: 0038 loss_train: 0.2126 acc_train: 0.9871 loss_val: 1.5165 acc_val: 0.8529\n",
            "Epoch: 0039 loss_train: 0.3306 acc_train: 0.9790 loss_val: 1.3386 acc_val: 0.8580\n",
            "Epoch: 0040 loss_train: 0.2874 acc_train: 0.9821 loss_val: 1.3450 acc_val: 0.8671\n",
            "Epoch: 0041 loss_train: 0.1585 acc_train: 0.9899 loss_val: 1.2802 acc_val: 0.8735\n",
            "Epoch: 0042 loss_train: 0.1068 acc_train: 0.9948 loss_val: 1.3848 acc_val: 0.8765\n",
            "Epoch: 0043 loss_train: 0.0945 acc_train: 0.9943 loss_val: 1.4241 acc_val: 0.8747\n",
            "Epoch: 0044 loss_train: 0.0718 acc_train: 0.9956 loss_val: 1.4819 acc_val: 0.8694\n",
            "Epoch: 0045 loss_train: 0.0830 acc_train: 0.9948 loss_val: 1.5326 acc_val: 0.8801\n",
            "Epoch: 0046 loss_train: 0.0546 acc_train: 0.9969 loss_val: 1.5654 acc_val: 0.8806\n",
            "Epoch: 0047 loss_train: 0.0630 acc_train: 0.9971 loss_val: 1.6040 acc_val: 0.8725\n",
            "Epoch: 0048 loss_train: 0.0516 acc_train: 0.9972 loss_val: 1.6261 acc_val: 0.8745\n",
            "Epoch: 0049 loss_train: 0.0563 acc_train: 0.9971 loss_val: 1.6794 acc_val: 0.8780\n",
            "Epoch: 0050 loss_train: 0.0759 acc_train: 0.9957 loss_val: 1.6476 acc_val: 0.8811\n",
            "Epoch: 0051 loss_train: 0.1239 acc_train: 0.9926 loss_val: 1.6102 acc_val: 0.8740\n",
            "Epoch: 0052 loss_train: 0.1574 acc_train: 0.9905 loss_val: 1.6718 acc_val: 0.8694\n",
            "Epoch: 0053 loss_train: 2.9127 acc_train: 0.9036 loss_val: 2.8021 acc_val: 0.7219\n",
            "Epoch: 0054 loss_train: 4.1801 acc_train: 0.7631 loss_val: 1.0129 acc_val: 0.8301\n",
            "Epoch: 0055 loss_train: 2.0499 acc_train: 0.8799 loss_val: 0.7401 acc_val: 0.8583\n",
            "Epoch: 0056 loss_train: 1.7932 acc_train: 0.8891 loss_val: 0.7311 acc_val: 0.8717\n",
            "Epoch: 0057 loss_train: 1.6559 acc_train: 0.8953 loss_val: 0.7214 acc_val: 0.8651\n",
            "Epoch: 0058 loss_train: 1.5446 acc_train: 0.9019 loss_val: 0.6527 acc_val: 0.8796\n",
            "Epoch: 0059 loss_train: 1.5063 acc_train: 0.9044 loss_val: 0.6561 acc_val: 0.8821\n",
            "Epoch: 0060 loss_train: 1.3642 acc_train: 0.9163 loss_val: 0.6888 acc_val: 0.8803\n",
            "Epoch: 0061 loss_train: 1.2729 acc_train: 0.9213 loss_val: 0.6169 acc_val: 0.8889\n",
            "Epoch: 0062 loss_train: 1.1160 acc_train: 0.9343 loss_val: 0.6304 acc_val: 0.8892\n",
            "Epoch: 0063 loss_train: 1.0503 acc_train: 0.9359 loss_val: 0.6922 acc_val: 0.8877\n",
            "Epoch: 0064 loss_train: 1.0090 acc_train: 0.9410 loss_val: 0.6867 acc_val: 0.8803\n",
            "Epoch: 0065 loss_train: 0.9417 acc_train: 0.9444 loss_val: 0.7159 acc_val: 0.8856\n",
            "Epoch: 0066 loss_train: 0.8606 acc_train: 0.9511 loss_val: 0.7058 acc_val: 0.8884\n",
            "Epoch: 0067 loss_train: 0.7449 acc_train: 0.9568 loss_val: 0.7388 acc_val: 0.8826\n",
            "Epoch: 0068 loss_train: 0.6783 acc_train: 0.9620 loss_val: 0.7673 acc_val: 0.8831\n",
            "Epoch: 0069 loss_train: 0.6133 acc_train: 0.9676 loss_val: 0.8382 acc_val: 0.8851\n",
            "Epoch: 0070 loss_train: 0.6893 acc_train: 0.9604 loss_val: 0.8494 acc_val: 0.8841\n",
            "Epoch: 0071 loss_train: 0.8889 acc_train: 0.9461 loss_val: 0.7047 acc_val: 0.8884\n",
            "Epoch: 0072 loss_train: 0.6833 acc_train: 0.9598 loss_val: 0.7466 acc_val: 0.8841\n",
            "Epoch: 0073 loss_train: 0.5807 acc_train: 0.9686 loss_val: 0.8134 acc_val: 0.8879\n",
            "Epoch: 0074 loss_train: 0.4999 acc_train: 0.9730 loss_val: 0.8529 acc_val: 0.8821\n",
            "Epoch: 0075 loss_train: 0.4091 acc_train: 0.9791 loss_val: 0.9655 acc_val: 0.8806\n",
            "Epoch: 0076 loss_train: 0.3397 acc_train: 0.9837 loss_val: 1.0103 acc_val: 0.8780\n",
            "Epoch: 0077 loss_train: 0.3119 acc_train: 0.9839 loss_val: 1.0732 acc_val: 0.8834\n",
            "Epoch: 0078 loss_train: 0.3265 acc_train: 0.9840 loss_val: 1.0466 acc_val: 0.8791\n",
            "Epoch: 0079 loss_train: 0.3195 acc_train: 0.9831 loss_val: 1.0796 acc_val: 0.8813\n",
            "Epoch: 0080 loss_train: 0.3145 acc_train: 0.9841 loss_val: 1.0561 acc_val: 0.8826\n",
            "Epoch: 0081 loss_train: 0.2555 acc_train: 0.9869 loss_val: 0.9926 acc_val: 0.8806\n",
            "Epoch: 0082 loss_train: 0.2137 acc_train: 0.9898 loss_val: 1.0818 acc_val: 0.8856\n",
            "Epoch: 0083 loss_train: 0.1849 acc_train: 0.9921 loss_val: 1.0707 acc_val: 0.8775\n",
            "Epoch: 0084 loss_train: 0.1776 acc_train: 0.9921 loss_val: 1.1869 acc_val: 0.8793\n",
            "Epoch: 0085 loss_train: 0.1692 acc_train: 0.9920 loss_val: 1.1882 acc_val: 0.8768\n",
            "Epoch: 0086 loss_train: 0.1652 acc_train: 0.9926 loss_val: 1.1929 acc_val: 0.8785\n",
            "Epoch: 0087 loss_train: 0.1809 acc_train: 0.9911 loss_val: 1.2375 acc_val: 0.8778\n",
            "Epoch: 0088 loss_train: 0.1732 acc_train: 0.9915 loss_val: 1.1825 acc_val: 0.8818\n",
            "Epoch: 0089 loss_train: 0.1625 acc_train: 0.9921 loss_val: 1.2681 acc_val: 0.8765\n",
            "Epoch: 0090 loss_train: 0.1362 acc_train: 0.9931 loss_val: 1.2896 acc_val: 0.8778\n",
            "Epoch: 0091 loss_train: 0.1146 acc_train: 0.9948 loss_val: 1.2660 acc_val: 0.8755\n",
            "Epoch: 0092 loss_train: 0.1136 acc_train: 0.9953 loss_val: 1.3038 acc_val: 0.8806\n",
            "Epoch: 0093 loss_train: 0.1125 acc_train: 0.9946 loss_val: 1.3244 acc_val: 0.8753\n",
            "Epoch: 0094 loss_train: 0.1080 acc_train: 0.9954 loss_val: 1.3673 acc_val: 0.8770\n",
            "Epoch: 0095 loss_train: 0.1193 acc_train: 0.9943 loss_val: 1.3778 acc_val: 0.8785\n",
            "Epoch: 0096 loss_train: 0.1148 acc_train: 0.9934 loss_val: 1.4161 acc_val: 0.8753\n",
            "Epoch: 0097 loss_train: 0.1322 acc_train: 0.9929 loss_val: 1.3954 acc_val: 0.8727\n",
            "Epoch: 0098 loss_train: 0.1098 acc_train: 0.9941 loss_val: 1.3649 acc_val: 0.8747\n",
            "Epoch: 0099 loss_train: 0.0995 acc_train: 0.9953 loss_val: 1.3532 acc_val: 0.8778\n",
            "Epoch: 0100 loss_train: 0.0975 acc_train: 0.9952 loss_val: 1.4008 acc_val: 0.8773\n",
            "Epoch: 0101 loss_train: 0.0693 acc_train: 0.9965 loss_val: 1.4611 acc_val: 0.8735\n",
            "Epoch: 0102 loss_train: 0.0870 acc_train: 0.9956 loss_val: 1.4214 acc_val: 0.8808\n",
            "Epoch: 0103 loss_train: 0.0722 acc_train: 0.9967 loss_val: 1.4524 acc_val: 0.8831\n",
            "Epoch: 0104 loss_train: 0.0691 acc_train: 0.9967 loss_val: 1.4513 acc_val: 0.8798\n",
            "Epoch: 0105 loss_train: 0.0540 acc_train: 0.9978 loss_val: 1.4694 acc_val: 0.8753\n",
            "Epoch: 0106 loss_train: 0.0578 acc_train: 0.9972 loss_val: 1.4737 acc_val: 0.8796\n",
            "Epoch: 0107 loss_train: 0.0452 acc_train: 0.9981 loss_val: 1.4622 acc_val: 0.8801\n",
            "Epoch: 0108 loss_train: 0.0632 acc_train: 0.9975 loss_val: 1.5107 acc_val: 0.8763\n",
            "Epoch: 0109 loss_train: 0.0547 acc_train: 0.9974 loss_val: 1.5351 acc_val: 0.8765\n",
            "Epoch: 0110 loss_train: 0.0414 acc_train: 0.9978 loss_val: 1.4905 acc_val: 0.8793\n",
            "Epoch: 0111 loss_train: 0.0381 acc_train: 0.9981 loss_val: 1.4946 acc_val: 0.8831\n",
            "Epoch: 0112 loss_train: 0.0316 acc_train: 0.9986 loss_val: 1.5176 acc_val: 0.8785\n",
            "Optimization Finished!\n",
            "Train cost: 85.5272s\n",
            "Loading 62th epoch\n",
            "Test set results: loss= 0.6658 accuracy= 0.8871\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --dataset pubmed --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 7  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack l2_weak --attperc 20 --TrainTest train --weight_decay=1e-05"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5d_Fr3tLbhg",
        "outputId": "86971f0a-d70d-4ad8-d346-1f42b2cad33d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:354: DGLWarning: DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\n",
            "  dgl_warning(\"DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\")\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=515, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=3, bias=True)\n",
            ")\n",
            "total params: 2501125\n",
            "Epoch: 0001 loss_train: 6.4449 acc_train: 0.4355 loss_val: 1.9859 acc_val: 0.6080\n",
            "Epoch: 0002 loss_train: 5.5969 acc_train: 0.6399 loss_val: 1.6338 acc_val: 0.6833\n",
            "Epoch: 0003 loss_train: 4.6004 acc_train: 0.6953 loss_val: 1.3017 acc_val: 0.7482\n",
            "Epoch: 0004 loss_train: 3.7289 acc_train: 0.7609 loss_val: 1.1154 acc_val: 0.7817\n",
            "Epoch: 0005 loss_train: 3.2896 acc_train: 0.7819 loss_val: 1.0359 acc_val: 0.7951\n",
            "Epoch: 0006 loss_train: 3.0217 acc_train: 0.7964 loss_val: 0.9423 acc_val: 0.8149\n",
            "Epoch: 0007 loss_train: 2.7396 acc_train: 0.8189 loss_val: 0.8633 acc_val: 0.8316\n",
            "Epoch: 0008 loss_train: 2.4711 acc_train: 0.8358 loss_val: 0.7931 acc_val: 0.8479\n",
            "Epoch: 0009 loss_train: 2.2445 acc_train: 0.8497 loss_val: 0.7514 acc_val: 0.8595\n",
            "Epoch: 0010 loss_train: 2.0818 acc_train: 0.8609 loss_val: 0.7110 acc_val: 0.8638\n",
            "Epoch: 0011 loss_train: 1.9268 acc_train: 0.8710 loss_val: 0.6755 acc_val: 0.8704\n",
            "Epoch: 0012 loss_train: 1.7628 acc_train: 0.8844 loss_val: 0.6649 acc_val: 0.8742\n",
            "Epoch: 0013 loss_train: 1.6351 acc_train: 0.8966 loss_val: 0.6416 acc_val: 0.8806\n",
            "Epoch: 0014 loss_train: 1.5392 acc_train: 0.9026 loss_val: 0.6552 acc_val: 0.8808\n",
            "Epoch: 0015 loss_train: 1.5299 acc_train: 0.9002 loss_val: 0.6877 acc_val: 0.8742\n",
            "Epoch: 0016 loss_train: 1.4306 acc_train: 0.9072 loss_val: 0.6643 acc_val: 0.8775\n",
            "Epoch: 0017 loss_train: 1.3298 acc_train: 0.9154 loss_val: 0.6654 acc_val: 0.8816\n",
            "Epoch: 0018 loss_train: 1.2356 acc_train: 0.9238 loss_val: 0.6407 acc_val: 0.8882\n",
            "Epoch: 0019 loss_train: 1.1534 acc_train: 0.9298 loss_val: 0.6400 acc_val: 0.8874\n",
            "Epoch: 0020 loss_train: 1.0421 acc_train: 0.9377 loss_val: 0.6682 acc_val: 0.8851\n",
            "Epoch: 0021 loss_train: 0.9494 acc_train: 0.9445 loss_val: 0.7054 acc_val: 0.8882\n",
            "Epoch: 0022 loss_train: 0.9179 acc_train: 0.9431 loss_val: 0.7014 acc_val: 0.8874\n",
            "Epoch: 0023 loss_train: 0.8684 acc_train: 0.9484 loss_val: 0.8024 acc_val: 0.8737\n",
            "Epoch: 0024 loss_train: 0.8476 acc_train: 0.9450 loss_val: 0.7094 acc_val: 0.8824\n",
            "Epoch: 0025 loss_train: 0.6705 acc_train: 0.9627 loss_val: 0.7878 acc_val: 0.8821\n",
            "Epoch: 0026 loss_train: 0.6099 acc_train: 0.9653 loss_val: 0.8196 acc_val: 0.8839\n",
            "Epoch: 0027 loss_train: 0.5652 acc_train: 0.9679 loss_val: 0.9217 acc_val: 0.8844\n",
            "Epoch: 0028 loss_train: 0.9511 acc_train: 0.9460 loss_val: 1.3243 acc_val: 0.8344\n",
            "Epoch: 0029 loss_train: 1.3763 acc_train: 0.9137 loss_val: 0.7977 acc_val: 0.8811\n",
            "Epoch: 0030 loss_train: 0.8212 acc_train: 0.9497 loss_val: 0.8006 acc_val: 0.8864\n",
            "Epoch: 0031 loss_train: 0.6735 acc_train: 0.9613 loss_val: 0.7811 acc_val: 0.8887\n",
            "Epoch: 0032 loss_train: 0.5137 acc_train: 0.9711 loss_val: 0.8705 acc_val: 0.8889\n",
            "Epoch: 0033 loss_train: 0.4024 acc_train: 0.9792 loss_val: 0.9597 acc_val: 0.8836\n",
            "Epoch: 0034 loss_train: 0.3382 acc_train: 0.9833 loss_val: 1.0117 acc_val: 0.8811\n",
            "Epoch: 0035 loss_train: 0.2774 acc_train: 0.9851 loss_val: 1.0484 acc_val: 0.8826\n",
            "Epoch: 0036 loss_train: 0.2079 acc_train: 0.9898 loss_val: 1.1533 acc_val: 0.8808\n",
            "Epoch: 0037 loss_train: 0.1740 acc_train: 0.9915 loss_val: 1.1836 acc_val: 0.8788\n",
            "Epoch: 0038 loss_train: 0.1381 acc_train: 0.9942 loss_val: 1.2451 acc_val: 0.8788\n",
            "Epoch: 0039 loss_train: 0.1192 acc_train: 0.9941 loss_val: 1.4171 acc_val: 0.8737\n",
            "Epoch: 0040 loss_train: 0.1078 acc_train: 0.9954 loss_val: 1.3991 acc_val: 0.8796\n",
            "Epoch: 0041 loss_train: 0.1036 acc_train: 0.9948 loss_val: 1.4328 acc_val: 0.8742\n",
            "Epoch: 0042 loss_train: 0.1251 acc_train: 0.9931 loss_val: 1.4868 acc_val: 0.8727\n",
            "Epoch: 0043 loss_train: 0.1099 acc_train: 0.9943 loss_val: 1.4328 acc_val: 0.8770\n",
            "Epoch: 0044 loss_train: 0.1523 acc_train: 0.9910 loss_val: 1.5689 acc_val: 0.8702\n",
            "Epoch: 0045 loss_train: 0.2970 acc_train: 0.9823 loss_val: 1.5666 acc_val: 0.8717\n",
            "Epoch: 0046 loss_train: 0.4308 acc_train: 0.9765 loss_val: 1.3755 acc_val: 0.8687\n",
            "Epoch: 0047 loss_train: 0.4558 acc_train: 0.9746 loss_val: 1.1415 acc_val: 0.8808\n",
            "Epoch: 0048 loss_train: 0.3225 acc_train: 0.9817 loss_val: 1.1148 acc_val: 0.8783\n",
            "Epoch: 0049 loss_train: 0.2325 acc_train: 0.9850 loss_val: 1.3600 acc_val: 0.8651\n",
            "Epoch: 0050 loss_train: 0.2005 acc_train: 0.9896 loss_val: 1.2901 acc_val: 0.8768\n",
            "Epoch: 0051 loss_train: 0.1409 acc_train: 0.9918 loss_val: 1.3081 acc_val: 0.8801\n",
            "Epoch: 0052 loss_train: 0.1875 acc_train: 0.9892 loss_val: 1.3137 acc_val: 0.8785\n",
            "Epoch: 0053 loss_train: 0.3059 acc_train: 0.9811 loss_val: 1.4231 acc_val: 0.8679\n",
            "Epoch: 0054 loss_train: 0.2972 acc_train: 0.9829 loss_val: 1.3667 acc_val: 0.8697\n",
            "Epoch: 0055 loss_train: 0.2887 acc_train: 0.9812 loss_val: 1.2251 acc_val: 0.8765\n",
            "Epoch: 0056 loss_train: 0.1601 acc_train: 0.9912 loss_val: 1.2057 acc_val: 0.8808\n",
            "Epoch: 0057 loss_train: 0.1190 acc_train: 0.9938 loss_val: 1.3397 acc_val: 0.8768\n",
            "Epoch: 0058 loss_train: 0.0816 acc_train: 0.9959 loss_val: 1.3989 acc_val: 0.8783\n",
            "Epoch: 0059 loss_train: 0.0633 acc_train: 0.9971 loss_val: 1.4827 acc_val: 0.8796\n",
            "Epoch: 0060 loss_train: 0.0519 acc_train: 0.9973 loss_val: 1.4857 acc_val: 0.8811\n",
            "Epoch: 0061 loss_train: 0.0380 acc_train: 0.9981 loss_val: 1.5565 acc_val: 0.8791\n",
            "Epoch: 0062 loss_train: 0.0331 acc_train: 0.9979 loss_val: 1.6205 acc_val: 0.8755\n",
            "Epoch: 0063 loss_train: 0.0304 acc_train: 0.9985 loss_val: 1.6183 acc_val: 0.8813\n",
            "Epoch: 0064 loss_train: 0.0183 acc_train: 0.9992 loss_val: 1.6354 acc_val: 0.8796\n",
            "Epoch: 0065 loss_train: 0.0257 acc_train: 0.9990 loss_val: 1.6446 acc_val: 0.8834\n",
            "Epoch: 0066 loss_train: 0.0260 acc_train: 0.9987 loss_val: 1.7022 acc_val: 0.8773\n",
            "Epoch: 0067 loss_train: 0.0508 acc_train: 0.9973 loss_val: 1.7379 acc_val: 0.8755\n",
            "Epoch: 0068 loss_train: 0.0753 acc_train: 0.9964 loss_val: 1.7129 acc_val: 0.8747\n",
            "Epoch: 0069 loss_train: 0.0802 acc_train: 0.9961 loss_val: 1.7771 acc_val: 0.8796\n",
            "Epoch: 0070 loss_train: 0.1003 acc_train: 0.9948 loss_val: 1.5902 acc_val: 0.8722\n",
            "Epoch: 0071 loss_train: 0.0709 acc_train: 0.9955 loss_val: 1.6227 acc_val: 0.8778\n",
            "Epoch: 0072 loss_train: 0.0550 acc_train: 0.9971 loss_val: 1.5786 acc_val: 0.8801\n",
            "Epoch: 0073 loss_train: 0.0687 acc_train: 0.9962 loss_val: 1.6514 acc_val: 0.8753\n",
            "Epoch: 0074 loss_train: 0.0884 acc_train: 0.9959 loss_val: 1.6551 acc_val: 0.8687\n",
            "Epoch: 0075 loss_train: 0.0754 acc_train: 0.9954 loss_val: 1.6820 acc_val: 0.8783\n",
            "Epoch: 0076 loss_train: 0.0632 acc_train: 0.9963 loss_val: 1.6438 acc_val: 0.8796\n",
            "Epoch: 0077 loss_train: 0.0700 acc_train: 0.9968 loss_val: 1.6715 acc_val: 0.8813\n",
            "Epoch: 0078 loss_train: 0.0671 acc_train: 0.9971 loss_val: 1.6853 acc_val: 0.8783\n",
            "Epoch: 0079 loss_train: 0.0483 acc_train: 0.9975 loss_val: 1.7142 acc_val: 0.8753\n",
            "Epoch: 0080 loss_train: 0.0326 acc_train: 0.9984 loss_val: 1.6612 acc_val: 0.8826\n",
            "Epoch: 0081 loss_train: 0.0380 acc_train: 0.9975 loss_val: 1.6170 acc_val: 0.8785\n",
            "Epoch: 0082 loss_train: 0.0322 acc_train: 0.9984 loss_val: 1.7242 acc_val: 0.8768\n",
            "Optimization Finished!\n",
            "Train cost: 61.4882s\n",
            "Loading 32th epoch\n",
            "Test set results: loss= 0.9408 accuracy= 0.8805\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --dataset pubmed --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 7  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack dice --attperc 20 --TrainTest train --weight_decay=1e-05"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4F-MBExLuBY",
        "outputId": "d79de82e-caf0-4a34-b71d-5f72f7772864"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=515, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=3, bias=True)\n",
            ")\n",
            "total params: 2501125\n",
            "Epoch: 0001 loss_train: 6.5548 acc_train: 0.3992 loss_val: 2.1075 acc_val: 0.5763\n",
            "Epoch: 0002 loss_train: 6.3283 acc_train: 0.4828 loss_val: 2.0950 acc_val: 0.7893\n",
            "Epoch: 0003 loss_train: 6.2429 acc_train: 0.5888 loss_val: 1.9839 acc_val: 0.7918\n",
            "Epoch: 0004 loss_train: 5.7921 acc_train: 0.6947 loss_val: 1.5714 acc_val: 0.7918\n",
            "Epoch: 0005 loss_train: 4.4954 acc_train: 0.7112 loss_val: 0.8723 acc_val: 0.7921\n",
            "Epoch: 0006 loss_train: 2.9489 acc_train: 0.8147 loss_val: 0.3485 acc_val: 0.9954\n",
            "Epoch: 0007 loss_train: 1.9673 acc_train: 0.8754 loss_val: 0.1416 acc_val: 1.0000\n",
            "Epoch: 0008 loss_train: 1.2262 acc_train: 0.9407 loss_val: 0.0494 acc_val: 1.0000\n",
            "Epoch: 0009 loss_train: 0.4547 acc_train: 0.9895 loss_val: 0.0203 acc_val: 1.0000\n",
            "Epoch: 0010 loss_train: 0.1126 acc_train: 0.9981 loss_val: 0.0047 acc_val: 1.0000\n",
            "Epoch: 0011 loss_train: 0.0281 acc_train: 0.9999 loss_val: 0.0017 acc_val: 1.0000\n",
            "Epoch: 0012 loss_train: 0.0123 acc_train: 1.0000 loss_val: 0.0010 acc_val: 1.0000\n",
            "Epoch: 0013 loss_train: 0.0074 acc_train: 0.9999 loss_val: 0.0006 acc_val: 1.0000\n",
            "Epoch: 0014 loss_train: 0.0041 acc_train: 1.0000 loss_val: 0.0004 acc_val: 1.0000\n",
            "Epoch: 0015 loss_train: 0.0030 acc_train: 1.0000 loss_val: 0.0005 acc_val: 1.0000\n",
            "Epoch: 0016 loss_train: 0.0023 acc_train: 1.0000 loss_val: 0.0004 acc_val: 1.0000\n",
            "Epoch: 0017 loss_train: 0.0017 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 0018 loss_train: 0.0015 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 0019 loss_train: 0.0013 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 0020 loss_train: 0.0011 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 0021 loss_train: 0.0010 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 0022 loss_train: 0.0009 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 0023 loss_train: 0.0008 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 0024 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0025 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0026 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0027 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0028 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0029 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0030 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0031 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0032 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0033 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0034 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0035 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0036 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0037 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0038 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0039 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0040 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0041 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0042 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0043 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0044 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0045 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0046 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0047 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0048 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0049 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0050 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0051 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0052 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0053 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0054 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0055 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0056 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0057 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0058 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0059 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0060 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0061 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0062 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0063 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0064 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0065 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0066 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0067 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0068 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0069 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0070 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0071 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0072 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0073 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0074 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0075 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0076 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0077 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0078 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0079 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0080 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0081 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0082 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0083 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0084 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0085 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0086 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0087 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0088 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0089 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0090 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0091 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0092 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0093 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0094 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0095 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0096 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0097 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0098 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0099 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0100 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0101 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0102 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0103 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0104 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0105 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0106 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0107 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0108 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0109 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0110 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0111 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0112 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0113 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0114 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0115 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0116 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0117 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0118 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0119 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0120 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0121 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0122 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0123 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0124 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0125 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0126 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0127 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0128 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0129 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0130 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0131 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0132 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0133 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0134 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0135 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0136 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0137 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0138 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0139 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0140 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0141 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0142 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0143 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0144 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0145 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0146 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0147 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0148 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0149 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0150 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0151 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0152 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0153 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0154 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0155 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0156 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0157 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0158 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0159 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0160 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0161 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0162 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0163 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0164 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0165 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0166 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0167 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0168 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0169 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0170 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0171 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0172 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0173 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0174 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0175 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0176 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0177 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0178 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0179 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0180 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0181 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0182 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0183 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0184 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0185 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0186 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0187 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0188 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0189 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0190 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0191 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0192 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0193 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0194 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0195 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0196 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0197 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0198 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0199 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0200 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0201 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0202 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0203 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0204 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0205 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0206 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0207 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0208 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0209 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0210 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0211 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0212 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0213 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0214 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0215 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0216 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0217 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0218 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0219 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0220 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0221 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0222 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0223 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0224 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0225 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0226 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0227 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0228 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0229 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0230 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0231 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0232 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0233 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0234 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0235 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0236 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0237 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0238 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0239 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0240 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0241 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0242 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0243 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0244 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0245 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0246 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0247 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0248 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0249 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0250 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0251 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0252 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0253 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0254 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0255 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0256 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0257 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0258 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0259 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0260 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0261 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0262 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0263 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0264 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0265 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0266 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0267 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0268 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0269 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0270 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0271 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0272 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0273 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0274 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0275 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0276 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0277 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0278 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0279 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0280 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0281 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0282 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0283 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0284 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0285 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0286 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0287 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0288 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0289 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0290 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0291 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0292 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0293 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0294 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0295 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0296 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0297 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0298 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0299 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0300 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0301 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0302 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0303 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0304 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0305 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0306 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0307 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0308 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0309 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0310 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0311 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0312 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0313 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0314 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0315 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0316 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0317 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0318 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0319 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0320 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0321 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0322 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0323 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0324 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0325 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0326 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0327 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0328 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0329 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0330 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0331 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0332 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0333 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0334 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0335 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0336 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0337 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0338 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0339 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0340 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0341 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0342 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0343 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0344 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0345 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0346 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0347 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0348 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0349 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0350 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0351 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0352 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0353 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0354 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0355 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0356 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0357 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0358 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0359 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0360 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0361 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0362 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0363 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0364 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0365 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0366 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0367 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0368 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0369 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0370 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0371 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0372 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0373 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0374 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0375 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0376 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0377 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0378 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0379 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0380 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0381 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0382 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0383 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0384 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0385 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0386 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0387 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0388 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0389 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0390 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0391 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0392 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0393 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0394 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0395 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0396 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0397 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0398 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0399 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0400 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0401 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0402 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0403 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0404 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0405 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0406 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0407 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0408 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0409 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0410 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0411 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0412 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0413 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0414 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0415 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0416 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0417 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0418 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0419 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0420 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0421 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0422 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0423 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0424 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0425 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0426 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0427 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0428 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0429 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0430 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0431 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0432 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0433 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0434 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0435 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0436 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0437 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0438 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0439 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0440 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0441 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0442 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0443 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0444 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0445 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0446 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0447 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0448 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0449 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0450 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0451 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0452 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0453 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0454 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0455 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0456 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0457 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0458 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0459 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0460 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0461 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0462 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0463 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0464 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0465 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0466 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0467 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0468 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0469 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0470 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0471 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0472 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0473 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0474 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0475 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0476 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0477 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0478 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0479 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0480 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0481 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0482 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0483 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0484 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0485 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0486 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0487 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0488 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0489 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0490 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0491 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0492 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0493 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0494 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0495 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0496 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0497 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0498 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0499 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0500 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0501 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0502 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0503 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0504 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0505 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0506 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0507 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0508 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0509 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0510 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0511 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0512 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0513 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0514 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0515 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0516 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0517 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0518 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0519 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0520 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0521 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0522 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0523 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0524 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0525 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0526 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0527 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0528 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0529 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0530 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0531 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0532 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0533 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0534 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0535 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0536 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0537 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0538 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0539 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0540 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0541 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0542 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0543 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0544 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0545 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0546 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0547 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0548 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0549 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0550 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0551 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0552 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0553 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0554 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0555 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0556 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0557 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0558 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0559 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0560 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0561 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0562 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0563 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0564 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0565 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0566 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0567 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0568 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0569 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0570 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0571 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0572 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0573 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0574 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0575 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0576 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0577 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0578 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0579 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0580 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0581 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0582 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0583 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0584 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0585 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0586 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0587 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0588 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0589 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0590 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0591 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0592 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0593 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0594 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0595 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0596 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0597 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0598 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0599 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0600 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0601 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0602 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0603 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0604 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0605 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0606 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0607 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0608 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0609 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0610 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0611 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0612 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0613 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0614 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0615 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0616 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0617 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0618 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0619 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0620 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0621 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0622 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0623 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0624 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0625 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0626 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0627 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0628 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0629 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0630 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0631 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0632 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0633 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0634 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0635 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0636 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0637 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0638 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0639 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0640 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0641 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0642 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0643 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0644 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0645 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0646 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0647 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0648 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0649 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0650 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0651 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0652 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0653 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0654 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0655 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0656 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0657 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0658 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0659 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0660 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0661 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0662 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0663 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0664 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0665 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0666 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0667 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0668 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0669 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0670 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0671 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0672 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0673 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0674 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0675 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0676 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0677 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0678 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0679 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0680 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0681 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0682 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0683 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0684 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0685 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0686 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0687 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0688 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0689 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0690 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0691 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0692 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0693 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0694 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0695 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0696 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0697 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0698 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0699 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0700 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0701 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0702 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0703 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0704 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0705 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0706 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0707 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0708 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0709 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0710 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0711 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0712 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0713 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0714 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0715 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0716 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0717 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0718 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0719 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0720 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0721 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0722 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0723 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0724 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0725 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0726 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0727 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0728 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0729 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0730 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0731 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0732 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0733 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0734 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0735 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0736 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0737 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0738 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0739 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0740 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0741 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0742 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0743 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0744 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0745 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0746 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0747 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0748 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0749 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0750 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0751 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0752 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0753 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0754 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0755 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0756 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0757 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0758 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0759 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0760 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0761 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0762 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0763 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0764 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0765 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0766 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0767 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0768 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0769 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0770 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0771 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0772 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0773 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0774 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0775 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0776 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0777 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0778 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0779 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0780 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0781 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0782 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0783 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0784 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0785 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0786 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0787 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0788 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0789 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0790 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0791 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0792 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0793 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0794 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0795 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0796 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0797 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0798 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0799 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0800 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0801 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0802 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0803 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0804 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0805 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0806 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0807 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0808 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0809 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0810 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0811 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0812 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0813 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0814 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0815 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0816 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0817 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0818 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0819 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0820 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0821 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0822 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0823 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0824 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0825 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0826 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0827 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0828 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0829 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0830 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0831 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0832 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0833 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0834 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0835 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0836 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0837 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0838 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0839 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0840 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0841 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0842 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0843 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0844 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0845 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0846 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0847 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0848 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0849 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0850 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0851 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0852 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0853 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0854 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0855 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0856 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0857 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0858 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0859 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0860 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0861 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0862 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0863 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0864 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0865 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0866 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0867 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0868 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0869 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0870 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0871 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0872 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0873 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0874 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0875 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0876 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0877 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0878 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0879 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0880 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0881 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0882 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0883 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0884 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0885 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0886 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0887 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0888 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0889 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0890 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0891 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0892 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0893 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0894 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0895 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0896 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0897 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0898 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0899 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0900 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0901 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0902 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0903 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0904 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0905 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0906 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0907 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0908 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0909 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0910 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0911 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0912 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0913 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0914 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0915 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0916 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0917 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0918 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0919 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0920 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0921 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0922 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0923 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0924 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0925 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0926 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0927 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0928 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0929 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0930 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0931 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0932 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0933 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0934 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0935 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0936 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0937 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0938 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0939 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0940 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0941 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0942 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0943 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0944 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0945 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0946 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0947 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0948 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0949 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0950 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0951 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0952 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0953 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0954 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0955 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0956 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0957 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0958 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0959 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0960 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0961 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0962 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0963 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0964 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0965 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0966 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0967 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0968 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0969 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0970 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0971 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0972 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0973 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0974 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0975 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0976 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0977 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0978 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0979 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0980 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0981 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0982 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0983 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0984 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0985 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0986 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0987 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0988 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0989 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0990 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0991 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0992 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0993 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0994 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0995 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0996 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0997 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0998 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 0999 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1000 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1001 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1002 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1003 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1004 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1005 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1006 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1007 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1008 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1009 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1010 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1011 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1012 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1013 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1014 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1015 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1016 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1017 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1018 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1019 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1020 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1021 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1022 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1023 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1024 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1025 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1026 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1027 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1028 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1029 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1030 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1031 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1032 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1033 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1034 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1035 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1036 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1037 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1038 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1039 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1040 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1041 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1042 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1043 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1044 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1045 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1046 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1047 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1048 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1049 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1050 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1051 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1052 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1053 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1054 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1055 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1056 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1057 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1058 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1059 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1060 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1061 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1062 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1063 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1064 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1065 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1066 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1067 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1068 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1069 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1070 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1071 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1072 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1073 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1074 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1075 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1076 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1077 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1078 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1079 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1080 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1081 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1082 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1083 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1084 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1085 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1086 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1087 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0043 acc_val: 0.9997\n",
            "Epoch: 1088 loss_train: 19.9871 acc_train: 0.7340 loss_val: 4.5367 acc_val: 0.3925\n",
            "Epoch: 1089 loss_train: 7.8762 acc_train: 0.3995 loss_val: 2.1692 acc_val: 0.3993\n",
            "Epoch: 1090 loss_train: 6.3016 acc_train: 0.4481 loss_val: 1.9885 acc_val: 0.7809\n",
            "Epoch: 1091 loss_train: 5.6888 acc_train: 0.5692 loss_val: 1.3948 acc_val: 0.7726\n",
            "Epoch: 1092 loss_train: 2.9637 acc_train: 0.8779 loss_val: 0.2050 acc_val: 0.9858\n",
            "Epoch: 1093 loss_train: 0.8237 acc_train: 0.9672 loss_val: 0.0783 acc_val: 0.9878\n",
            "Epoch: 1094 loss_train: 0.3127 acc_train: 0.9844 loss_val: 0.0115 acc_val: 0.9997\n",
            "Epoch: 1095 loss_train: 0.1260 acc_train: 0.9971 loss_val: 0.0041 acc_val: 1.0000\n",
            "Epoch: 1096 loss_train: 0.0674 acc_train: 0.9975 loss_val: 0.0021 acc_val: 1.0000\n",
            "Epoch: 1097 loss_train: 0.0440 acc_train: 0.9982 loss_val: 0.0011 acc_val: 1.0000\n",
            "Epoch: 1098 loss_train: 0.0272 acc_train: 0.9992 loss_val: 0.0008 acc_val: 1.0000\n",
            "Epoch: 1099 loss_train: 0.0140 acc_train: 0.9998 loss_val: 0.0009 acc_val: 1.0000\n",
            "Epoch: 1100 loss_train: 0.0132 acc_train: 0.9997 loss_val: 0.0006 acc_val: 1.0000\n",
            "Epoch: 1101 loss_train: 0.0084 acc_train: 0.9998 loss_val: 0.0006 acc_val: 1.0000\n",
            "Epoch: 1102 loss_train: 0.0118 acc_train: 0.9997 loss_val: 0.0006 acc_val: 1.0000\n",
            "Epoch: 1103 loss_train: 0.0088 acc_train: 0.9999 loss_val: 0.0006 acc_val: 1.0000\n",
            "Epoch: 1104 loss_train: 0.0063 acc_train: 0.9999 loss_val: 0.0005 acc_val: 1.0000\n",
            "Epoch: 1105 loss_train: 0.0049 acc_train: 1.0000 loss_val: 0.0005 acc_val: 1.0000\n",
            "Epoch: 1106 loss_train: 0.0083 acc_train: 0.9999 loss_val: 0.0004 acc_val: 1.0000\n",
            "Epoch: 1107 loss_train: 0.0043 acc_train: 1.0000 loss_val: 0.0004 acc_val: 1.0000\n",
            "Epoch: 1108 loss_train: 0.0037 acc_train: 1.0000 loss_val: 0.0004 acc_val: 1.0000\n",
            "Epoch: 1109 loss_train: 0.0072 acc_train: 0.9999 loss_val: 0.0004 acc_val: 1.0000\n",
            "Epoch: 1110 loss_train: 0.0038 acc_train: 1.0000 loss_val: 0.0004 acc_val: 1.0000\n",
            "Epoch: 1111 loss_train: 0.0099 acc_train: 0.9998 loss_val: 0.0004 acc_val: 1.0000\n",
            "Epoch: 1112 loss_train: 0.0051 acc_train: 0.9998 loss_val: 0.0004 acc_val: 1.0000\n",
            "Epoch: 1113 loss_train: 0.0072 acc_train: 0.9997 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 1114 loss_train: 0.0041 acc_train: 1.0000 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 1115 loss_train: 0.0083 acc_train: 0.9998 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 1116 loss_train: 0.0023 acc_train: 1.0000 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 1117 loss_train: 0.0032 acc_train: 0.9999 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 1118 loss_train: 0.0021 acc_train: 1.0000 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 1119 loss_train: 0.0070 acc_train: 0.9998 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 1120 loss_train: 0.0040 acc_train: 0.9999 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 1121 loss_train: 0.0035 acc_train: 1.0000 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 1122 loss_train: 0.0032 acc_train: 0.9999 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 1123 loss_train: 0.0030 acc_train: 1.0000 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 1124 loss_train: 0.0027 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1125 loss_train: 0.0022 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1126 loss_train: 0.0020 acc_train: 0.9999 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1127 loss_train: 0.0023 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1128 loss_train: 0.0022 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1129 loss_train: 0.0017 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1130 loss_train: 0.0020 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1131 loss_train: 0.0017 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1132 loss_train: 0.0018 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1133 loss_train: 0.0016 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1134 loss_train: 0.0014 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1135 loss_train: 0.0013 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1136 loss_train: 0.0013 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1137 loss_train: 0.0014 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1138 loss_train: 0.0016 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1139 loss_train: 0.0013 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1140 loss_train: 0.0015 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1141 loss_train: 0.0012 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1142 loss_train: 0.0015 acc_train: 0.9999 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1143 loss_train: 0.0012 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1144 loss_train: 0.0012 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1145 loss_train: 0.0012 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1146 loss_train: 0.0012 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1147 loss_train: 0.0014 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1148 loss_train: 0.0013 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1149 loss_train: 0.0011 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1150 loss_train: 0.0011 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1151 loss_train: 0.0010 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1152 loss_train: 0.0011 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1153 loss_train: 0.0010 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1154 loss_train: 0.0010 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1155 loss_train: 0.0010 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1156 loss_train: 0.0013 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1157 loss_train: 0.0008 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1158 loss_train: 0.0011 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1159 loss_train: 0.0009 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1160 loss_train: 0.0010 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1161 loss_train: 0.0009 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1162 loss_train: 0.0009 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1163 loss_train: 0.0008 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1164 loss_train: 0.0008 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1165 loss_train: 0.0008 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1166 loss_train: 0.0008 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1167 loss_train: 0.0021 acc_train: 0.9999 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1168 loss_train: 0.0039 acc_train: 0.9998 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1169 loss_train: 0.0051 acc_train: 0.9998 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1170 loss_train: 0.0017 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1171 loss_train: 0.0011 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1172 loss_train: 0.0010 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1173 loss_train: 0.0008 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1174 loss_train: 0.0009 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1175 loss_train: 0.0008 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1176 loss_train: 0.0008 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1177 loss_train: 0.0009 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1178 loss_train: 0.0010 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1179 loss_train: 0.0014 acc_train: 0.9999 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1180 loss_train: 0.0008 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1181 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1182 loss_train: 0.0014 acc_train: 0.9999 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1183 loss_train: 0.0010 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1184 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1185 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1186 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1187 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1188 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1189 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1190 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1191 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1192 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1193 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1194 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1195 loss_train: 0.0009 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1196 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1197 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1198 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1199 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1200 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1201 loss_train: 0.0012 acc_train: 0.9999 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1202 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1203 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1204 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1205 loss_train: 0.0020 acc_train: 0.9999 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1206 loss_train: 0.0008 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1207 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1208 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1209 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1210 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1211 loss_train: 0.0077 acc_train: 0.9999 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1212 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1213 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1214 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1215 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1216 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1217 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1218 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1219 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1220 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1221 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1222 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1223 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1224 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1225 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1226 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1227 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1228 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1229 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1230 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1231 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1232 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1233 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1234 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1235 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1236 loss_train: 0.0069 acc_train: 0.9999 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1237 loss_train: 0.0042 acc_train: 0.9999 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1238 loss_train: 0.0044 acc_train: 0.9998 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1239 loss_train: 0.0097 acc_train: 0.9996 loss_val: 0.0023 acc_val: 0.9995\n",
            "Epoch: 1240 loss_train: 3.3166 acc_train: 0.9225 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 1241 loss_train: 4.4590 acc_train: 0.8736 loss_val: 0.1254 acc_val: 0.9873\n",
            "Epoch: 1242 loss_train: 0.9785 acc_train: 0.9480 loss_val: 0.1008 acc_val: 0.9911\n",
            "Epoch: 1243 loss_train: 0.4765 acc_train: 0.9801 loss_val: 0.0897 acc_val: 0.9924\n",
            "Epoch: 1244 loss_train: 0.3591 acc_train: 0.9871 loss_val: 0.0540 acc_val: 0.9921\n",
            "Epoch: 1245 loss_train: 0.1949 acc_train: 0.9920 loss_val: 0.0202 acc_val: 0.9977\n",
            "Epoch: 1246 loss_train: 0.1439 acc_train: 0.9950 loss_val: 0.0127 acc_val: 0.9980\n",
            "Epoch: 1247 loss_train: 0.0899 acc_train: 0.9974 loss_val: 0.0073 acc_val: 1.0000\n",
            "Epoch: 1248 loss_train: 0.0600 acc_train: 0.9986 loss_val: 0.0055 acc_val: 1.0000\n",
            "Epoch: 1249 loss_train: 0.0481 acc_train: 0.9990 loss_val: 0.0039 acc_val: 1.0000\n",
            "Epoch: 1250 loss_train: 0.0442 acc_train: 0.9990 loss_val: 0.0036 acc_val: 1.0000\n",
            "Epoch: 1251 loss_train: 0.0436 acc_train: 0.9988 loss_val: 0.0041 acc_val: 1.0000\n",
            "Epoch: 1252 loss_train: 0.0352 acc_train: 0.9992 loss_val: 0.0033 acc_val: 1.0000\n",
            "Epoch: 1253 loss_train: 0.0213 acc_train: 0.9997 loss_val: 0.0028 acc_val: 1.0000\n",
            "Epoch: 1254 loss_train: 0.0243 acc_train: 0.9994 loss_val: 0.0026 acc_val: 1.0000\n",
            "Epoch: 1255 loss_train: 0.0163 acc_train: 0.9996 loss_val: 0.0024 acc_val: 1.0000\n",
            "Epoch: 1256 loss_train: 0.0195 acc_train: 0.9993 loss_val: 0.0023 acc_val: 1.0000\n",
            "Epoch: 1257 loss_train: 0.0145 acc_train: 0.9998 loss_val: 0.0021 acc_val: 1.0000\n",
            "Epoch: 1258 loss_train: 0.0143 acc_train: 0.9997 loss_val: 0.0019 acc_val: 1.0000\n",
            "Epoch: 1259 loss_train: 0.0111 acc_train: 0.9999 loss_val: 0.0018 acc_val: 1.0000\n",
            "Epoch: 1260 loss_train: 0.0126 acc_train: 0.9998 loss_val: 0.0016 acc_val: 1.0000\n",
            "Epoch: 1261 loss_train: 0.0104 acc_train: 0.9998 loss_val: 0.0016 acc_val: 1.0000\n",
            "Epoch: 1262 loss_train: 0.0090 acc_train: 0.9999 loss_val: 0.0015 acc_val: 1.0000\n",
            "Epoch: 1263 loss_train: 0.0116 acc_train: 0.9998 loss_val: 0.0014 acc_val: 1.0000\n",
            "Epoch: 1264 loss_train: 0.0080 acc_train: 0.9999 loss_val: 0.0013 acc_val: 1.0000\n",
            "Epoch: 1265 loss_train: 0.0096 acc_train: 0.9998 loss_val: 0.0012 acc_val: 1.0000\n",
            "Epoch: 1266 loss_train: 0.0092 acc_train: 0.9998 loss_val: 0.0012 acc_val: 1.0000\n",
            "Epoch: 1267 loss_train: 0.0086 acc_train: 0.9998 loss_val: 0.0011 acc_val: 1.0000\n",
            "Epoch: 1268 loss_train: 0.0060 acc_train: 1.0000 loss_val: 0.0010 acc_val: 1.0000\n",
            "Epoch: 1269 loss_train: 0.0091 acc_train: 0.9997 loss_val: 0.0010 acc_val: 1.0000\n",
            "Epoch: 1270 loss_train: 0.0074 acc_train: 0.9999 loss_val: 0.0009 acc_val: 1.0000\n",
            "Epoch: 1271 loss_train: 0.0087 acc_train: 0.9998 loss_val: 0.0009 acc_val: 1.0000\n",
            "Epoch: 1272 loss_train: 0.0061 acc_train: 0.9999 loss_val: 0.0009 acc_val: 1.0000\n",
            "Epoch: 1273 loss_train: 0.0059 acc_train: 0.9999 loss_val: 0.0009 acc_val: 1.0000\n",
            "Epoch: 1274 loss_train: 0.0058 acc_train: 0.9999 loss_val: 0.0008 acc_val: 1.0000\n",
            "Epoch: 1275 loss_train: 0.0049 acc_train: 1.0000 loss_val: 0.0008 acc_val: 1.0000\n",
            "Epoch: 1276 loss_train: 0.0048 acc_train: 0.9999 loss_val: 0.0008 acc_val: 1.0000\n",
            "Epoch: 1277 loss_train: 0.0042 acc_train: 1.0000 loss_val: 0.0007 acc_val: 1.0000\n",
            "Epoch: 1278 loss_train: 0.0046 acc_train: 1.0000 loss_val: 0.0007 acc_val: 1.0000\n",
            "Epoch: 1279 loss_train: 0.0045 acc_train: 1.0000 loss_val: 0.0007 acc_val: 1.0000\n",
            "Epoch: 1280 loss_train: 0.0043 acc_train: 0.9999 loss_val: 0.0007 acc_val: 1.0000\n",
            "Epoch: 1281 loss_train: 0.0037 acc_train: 1.0000 loss_val: 0.0006 acc_val: 1.0000\n",
            "Epoch: 1282 loss_train: 0.0045 acc_train: 0.9999 loss_val: 0.0006 acc_val: 1.0000\n",
            "Epoch: 1283 loss_train: 0.0040 acc_train: 0.9999 loss_val: 0.0006 acc_val: 1.0000\n",
            "Epoch: 1284 loss_train: 0.0058 acc_train: 0.9999 loss_val: 0.0006 acc_val: 1.0000\n",
            "Epoch: 1285 loss_train: 0.0038 acc_train: 1.0000 loss_val: 0.0006 acc_val: 1.0000\n",
            "Epoch: 1286 loss_train: 0.0034 acc_train: 1.0000 loss_val: 0.0006 acc_val: 1.0000\n",
            "Epoch: 1287 loss_train: 0.0039 acc_train: 0.9999 loss_val: 0.0005 acc_val: 1.0000\n",
            "Epoch: 1288 loss_train: 0.0036 acc_train: 0.9999 loss_val: 0.0005 acc_val: 1.0000\n",
            "Epoch: 1289 loss_train: 0.0052 acc_train: 0.9999 loss_val: 0.0005 acc_val: 1.0000\n",
            "Epoch: 1290 loss_train: 0.0046 acc_train: 0.9999 loss_val: 0.0005 acc_val: 1.0000\n",
            "Epoch: 1291 loss_train: 0.0032 acc_train: 1.0000 loss_val: 0.0005 acc_val: 1.0000\n",
            "Epoch: 1292 loss_train: 0.0041 acc_train: 0.9999 loss_val: 0.0005 acc_val: 1.0000\n",
            "Epoch: 1293 loss_train: 0.0030 acc_train: 1.0000 loss_val: 0.0005 acc_val: 1.0000\n",
            "Epoch: 1294 loss_train: 0.0026 acc_train: 1.0000 loss_val: 0.0005 acc_val: 1.0000\n",
            "Epoch: 1295 loss_train: 0.0060 acc_train: 0.9998 loss_val: 0.0004 acc_val: 1.0000\n",
            "Epoch: 1296 loss_train: 0.0030 acc_train: 1.0000 loss_val: 0.0004 acc_val: 1.0000\n",
            "Epoch: 1297 loss_train: 0.0027 acc_train: 1.0000 loss_val: 0.0004 acc_val: 1.0000\n",
            "Epoch: 1298 loss_train: 0.0036 acc_train: 0.9999 loss_val: 0.0004 acc_val: 1.0000\n",
            "Epoch: 1299 loss_train: 0.0033 acc_train: 1.0000 loss_val: 0.0004 acc_val: 1.0000\n",
            "Epoch: 1300 loss_train: 0.0023 acc_train: 1.0000 loss_val: 0.0004 acc_val: 1.0000\n",
            "Epoch: 1301 loss_train: 0.0033 acc_train: 0.9999 loss_val: 0.0004 acc_val: 1.0000\n",
            "Epoch: 1302 loss_train: 0.0057 acc_train: 0.9998 loss_val: 0.0004 acc_val: 1.0000\n",
            "Epoch: 1303 loss_train: 0.0022 acc_train: 1.0000 loss_val: 0.0004 acc_val: 1.0000\n",
            "Epoch: 1304 loss_train: 0.0024 acc_train: 1.0000 loss_val: 0.0004 acc_val: 1.0000\n",
            "Epoch: 1305 loss_train: 0.0038 acc_train: 0.9999 loss_val: 0.0004 acc_val: 1.0000\n",
            "Epoch: 1306 loss_train: 0.0024 acc_train: 1.0000 loss_val: 0.0004 acc_val: 1.0000\n",
            "Epoch: 1307 loss_train: 0.0030 acc_train: 0.9999 loss_val: 0.0004 acc_val: 1.0000\n",
            "Epoch: 1308 loss_train: 0.0035 acc_train: 0.9999 loss_val: 0.0004 acc_val: 1.0000\n",
            "Epoch: 1309 loss_train: 0.0022 acc_train: 1.0000 loss_val: 0.0004 acc_val: 1.0000\n",
            "Epoch: 1310 loss_train: 0.0021 acc_train: 1.0000 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 1311 loss_train: 0.0020 acc_train: 1.0000 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 1312 loss_train: 0.0022 acc_train: 1.0000 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 1313 loss_train: 0.0020 acc_train: 1.0000 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 1314 loss_train: 0.0018 acc_train: 1.0000 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 1315 loss_train: 0.0018 acc_train: 1.0000 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 1316 loss_train: 0.0020 acc_train: 1.0000 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 1317 loss_train: 0.0027 acc_train: 0.9999 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 1318 loss_train: 0.0018 acc_train: 1.0000 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 1319 loss_train: 0.0018 acc_train: 1.0000 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 1320 loss_train: 0.0018 acc_train: 1.0000 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 1321 loss_train: 0.0024 acc_train: 0.9999 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 1322 loss_train: 0.0018 acc_train: 1.0000 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 1323 loss_train: 0.0016 acc_train: 1.0000 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 1324 loss_train: 0.0026 acc_train: 0.9999 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 1325 loss_train: 0.0020 acc_train: 0.9999 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 1326 loss_train: 0.0018 acc_train: 1.0000 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 1327 loss_train: 0.0019 acc_train: 1.0000 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 1328 loss_train: 0.0015 acc_train: 1.0000 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 1329 loss_train: 0.0015 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1330 loss_train: 0.0015 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1331 loss_train: 0.0024 acc_train: 0.9999 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1332 loss_train: 0.0014 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1333 loss_train: 0.0015 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1334 loss_train: 0.0014 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1335 loss_train: 0.0015 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1336 loss_train: 0.0015 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1337 loss_train: 0.0020 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1338 loss_train: 0.0014 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1339 loss_train: 0.0013 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1340 loss_train: 0.0012 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1341 loss_train: 0.0012 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1342 loss_train: 0.0013 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1343 loss_train: 0.0011 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1344 loss_train: 0.0012 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1345 loss_train: 0.0012 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1346 loss_train: 0.0011 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1347 loss_train: 0.0022 acc_train: 0.9998 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1348 loss_train: 0.0013 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1349 loss_train: 0.0016 acc_train: 0.9999 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1350 loss_train: 0.0012 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1351 loss_train: 0.0014 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1352 loss_train: 0.0011 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1353 loss_train: 0.0014 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1354 loss_train: 0.0011 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1355 loss_train: 0.0012 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1356 loss_train: 0.0010 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1357 loss_train: 0.0011 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1358 loss_train: 0.0012 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1359 loss_train: 0.0011 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1360 loss_train: 0.0011 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1361 loss_train: 0.0018 acc_train: 0.9999 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1362 loss_train: 0.0015 acc_train: 0.9999 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1363 loss_train: 0.0009 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1364 loss_train: 0.0010 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1365 loss_train: 0.0010 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1366 loss_train: 0.0010 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1367 loss_train: 0.0012 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 1368 loss_train: 0.0009 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1369 loss_train: 0.0010 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1370 loss_train: 0.0009 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1371 loss_train: 0.0020 acc_train: 0.9999 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1372 loss_train: 0.0013 acc_train: 0.9999 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1373 loss_train: 0.0009 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1374 loss_train: 0.0009 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1375 loss_train: 0.0020 acc_train: 0.9999 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1376 loss_train: 0.0011 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1377 loss_train: 0.0010 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1378 loss_train: 0.0009 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1379 loss_train: 0.0012 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1380 loss_train: 0.0009 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1381 loss_train: 0.0009 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1382 loss_train: 0.0009 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1383 loss_train: 0.0008 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1384 loss_train: 0.0009 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1385 loss_train: 0.0010 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1386 loss_train: 0.0009 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1387 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1388 loss_train: 0.0008 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1389 loss_train: 0.0009 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1390 loss_train: 0.0009 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1391 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1392 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1393 loss_train: 0.0008 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1394 loss_train: 0.0008 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1395 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1396 loss_train: 0.0010 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1397 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1398 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1399 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1400 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1401 loss_train: 0.0008 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1402 loss_train: 0.0008 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1403 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1404 loss_train: 0.0027 acc_train: 0.9999 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1405 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1406 loss_train: 0.0008 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1407 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1408 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1409 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1410 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1411 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1412 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1413 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1414 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1415 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1416 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1417 loss_train: 0.0008 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1418 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1419 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1420 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1421 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1422 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1423 loss_train: 0.0014 acc_train: 0.9999 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1424 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1425 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1426 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1427 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1428 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1429 loss_train: 0.0008 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1430 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1431 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1432 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1433 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1434 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1435 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1436 loss_train: 0.0009 acc_train: 0.9999 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1437 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1438 loss_train: 0.0011 acc_train: 0.9999 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1439 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1440 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1441 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1442 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1443 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1444 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1445 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1446 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1447 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1448 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1449 loss_train: 0.0026 acc_train: 0.9999 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1450 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1451 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1452 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1453 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1454 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1455 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1456 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1457 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1458 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1459 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1460 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1461 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1462 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1463 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1464 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1465 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1466 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1467 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1468 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1469 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1470 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1471 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1472 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1473 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1474 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1475 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1476 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1477 loss_train: 0.0041 acc_train: 0.9999 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1478 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1479 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1480 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1481 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1482 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1483 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1484 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1485 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1486 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1487 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1488 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1489 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1490 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1491 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1492 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1493 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1494 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1495 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1496 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1497 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1498 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1499 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1500 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1501 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1502 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1503 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1504 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1505 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1506 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1507 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1508 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1509 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1510 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 1511 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1512 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1513 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1514 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1515 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1516 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1517 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1518 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1519 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1520 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1521 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1522 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1523 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1524 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1525 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1526 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1527 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1528 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1529 loss_train: 0.0035 acc_train: 0.9999 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1530 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1531 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1532 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1533 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1534 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1535 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1536 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1537 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1538 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1539 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1540 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1541 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1542 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1543 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1544 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1545 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1546 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1547 loss_train: 0.0025 acc_train: 0.9999 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1548 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1549 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1550 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1551 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1552 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1553 loss_train: 0.0008 acc_train: 0.9999 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1554 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1555 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1556 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1557 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1558 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1559 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1560 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1561 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1562 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1563 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1564 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1565 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1566 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1567 loss_train: 0.0020 acc_train: 0.9999 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1568 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1569 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1570 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1571 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1572 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1573 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1574 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1575 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1576 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1577 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1578 loss_train: 0.0007 acc_train: 0.9999 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1579 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1580 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1581 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1582 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1583 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1584 loss_train: 0.0021 acc_train: 0.9999 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1585 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1586 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1587 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1588 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1589 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1590 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1591 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1592 loss_train: 0.0009 acc_train: 0.9999 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1593 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1594 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1595 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1596 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1597 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1598 loss_train: 0.0007 acc_train: 0.9999 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1599 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1600 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1601 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1602 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1603 loss_train: 0.0021 acc_train: 0.9999 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1604 loss_train: 0.0029 acc_train: 0.9999 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1605 loss_train: 0.0008 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1606 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1607 loss_train: 0.0008 acc_train: 0.9999 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1608 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1609 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1610 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1611 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1612 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1613 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1614 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1615 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1616 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1617 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1618 loss_train: 0.0007 acc_train: 0.9999 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1619 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1620 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1621 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1622 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1623 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1624 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1625 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1626 loss_train: 0.0017 acc_train: 0.9999 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1627 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1628 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1629 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1630 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1631 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1632 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1633 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1634 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1635 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1636 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1637 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1638 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1639 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1640 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1641 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1642 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1643 loss_train: 0.0040 acc_train: 0.9999 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1644 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1645 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1646 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1647 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1648 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1649 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1650 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1651 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1652 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1653 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1654 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1655 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1656 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1657 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1658 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1659 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1660 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1661 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1662 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1663 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1664 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1665 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1666 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1667 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1668 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1669 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1670 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1671 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1672 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1673 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1674 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1675 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1676 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1677 loss_train: 0.0037 acc_train: 0.9999 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1678 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1679 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1680 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1681 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1682 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1683 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1684 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1685 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1686 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1687 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1688 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1689 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1690 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1691 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1692 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1693 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1694 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1695 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1696 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1697 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1698 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1699 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1700 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1701 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1702 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1703 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1704 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1705 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1706 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1707 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1708 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1709 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1710 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1711 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1712 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1713 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1714 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1715 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1716 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1717 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1718 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1719 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1720 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1721 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1722 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1723 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1724 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1725 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1726 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1727 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1728 loss_train: 0.0021 acc_train: 0.9999 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1729 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1730 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1731 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1732 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1733 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1734 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1735 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1736 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1737 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1738 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1739 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1740 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1741 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1742 loss_train: 0.0027 acc_train: 0.9999 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1743 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1744 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1745 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1746 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1747 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1748 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1749 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1750 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1751 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1752 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1753 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1754 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1755 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1756 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1757 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1758 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1759 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1760 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1761 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1762 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1763 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1764 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1765 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1766 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1767 loss_train: 0.0020 acc_train: 0.9999 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1768 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1769 loss_train: 0.0006 acc_train: 0.9999 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1770 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1771 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1772 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1773 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1774 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1775 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1776 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1777 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1778 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1779 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1780 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1781 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1782 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1783 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1784 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1785 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1786 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1787 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1788 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1789 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1790 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1791 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1792 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1793 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1794 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1795 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1796 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1797 loss_train: 0.0020 acc_train: 0.9999 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1798 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1799 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1800 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1801 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1802 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1803 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1804 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1805 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1806 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1807 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1808 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1809 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1810 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1811 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1812 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1813 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1814 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1815 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1816 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1817 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1818 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1819 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1820 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1821 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1822 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1823 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1824 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1825 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1826 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1827 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1828 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1829 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1830 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1831 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1832 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1833 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1834 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1835 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1836 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1837 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1838 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1839 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1840 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1841 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1842 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1843 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1844 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1845 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1846 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1847 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1848 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1849 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1850 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1851 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1852 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1853 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1854 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1855 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1856 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1857 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1858 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1859 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1860 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1861 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1862 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1863 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1864 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1865 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1866 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1867 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1868 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1869 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1870 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1871 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1872 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1873 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1874 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1875 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1876 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1877 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1878 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1879 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1880 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1881 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1882 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1883 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1884 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1885 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1886 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1887 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1888 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1889 loss_train: 0.0014 acc_train: 0.9999 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1890 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1891 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1892 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1893 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1894 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1895 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1896 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1897 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1898 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1899 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1900 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1901 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1902 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1903 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1904 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1905 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1906 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1907 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1908 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1909 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1910 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1911 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1912 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1913 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1914 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1915 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1916 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1917 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1918 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1919 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1920 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1921 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1922 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1923 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1924 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1925 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1926 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1927 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1928 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1929 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1930 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1931 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1932 loss_train: 0.0029 acc_train: 0.9999 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1933 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1934 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1935 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1936 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1937 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1938 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1939 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1940 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1941 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1942 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1943 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1944 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1945 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1946 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1947 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1948 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1949 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1950 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1951 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1952 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1953 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1954 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1955 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1956 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1957 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1958 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1959 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1960 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1961 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1962 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1963 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1964 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1965 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1966 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1967 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1968 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1969 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1970 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1971 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1972 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1973 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1974 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1975 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1976 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1977 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1978 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1979 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1980 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1981 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1982 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1983 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1984 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1985 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1986 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1987 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1988 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1989 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1990 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1991 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1992 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1993 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1994 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1995 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1996 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1997 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1998 loss_train: 0.0001 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 1999 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Epoch: 2000 loss_train: 0.0000 acc_train: 1.0000 loss_val: 0.0000 acc_val: 1.0000\n",
            "Optimization Finished!\n",
            "Train cost: 1511.1987s\n",
            "Loading 697th epoch\n",
            "Test set results: loss= 0.0000 accuracy= 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BI9IYFZdNCjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyper parameter tuning\n"
      ],
      "metadata": {
        "id": "OVvwJ32kQ3jH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --dataset pubmed --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 2  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack none --attperc 20 --TrainTest train --weight_decay=1e-05"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8U1VdihVQ5Xr",
        "outputId": "30678cbb-2c36-4abf-e742-987f88fd7854"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n",
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n",
            "Downloading /root/.dgl/pubmed.zip from https://data.dgl.ai/dataset/pubmed.zip...\n",
            "Extracting file to /root/.dgl/pubmed\n",
            "Finished data loading and preprocessing.\n",
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done saving data into cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=515, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=3, bias=True)\n",
            ")\n",
            "total params: 2501125\n",
            "Epoch: 0001 loss_train: 6.5969 acc_train: 0.3928 loss_val: 2.0207 acc_val: 0.5332\n",
            "Epoch: 0002 loss_train: 5.6164 acc_train: 0.6343 loss_val: 1.5826 acc_val: 0.7388\n",
            "Epoch: 0003 loss_train: 4.3610 acc_train: 0.7599 loss_val: 1.1727 acc_val: 0.7974\n",
            "Epoch: 0004 loss_train: 3.2896 acc_train: 0.8035 loss_val: 0.9422 acc_val: 0.8210\n",
            "Epoch: 0005 loss_train: 2.6989 acc_train: 0.8251 loss_val: 0.8402 acc_val: 0.8428\n",
            "Epoch: 0006 loss_train: 2.4323 acc_train: 0.8414 loss_val: 0.7912 acc_val: 0.8527\n",
            "Epoch: 0007 loss_train: 2.2510 acc_train: 0.8573 loss_val: 0.7509 acc_val: 0.8644\n",
            "Epoch: 0008 loss_train: 2.1014 acc_train: 0.8656 loss_val: 0.7048 acc_val: 0.8702\n",
            "Epoch: 0009 loss_train: 1.9589 acc_train: 0.8740 loss_val: 0.6775 acc_val: 0.8740\n",
            "Epoch: 0010 loss_train: 1.8224 acc_train: 0.8850 loss_val: 0.6592 acc_val: 0.8780\n",
            "Epoch: 0011 loss_train: 1.6971 acc_train: 0.8928 loss_val: 0.6533 acc_val: 0.8796\n",
            "Epoch: 0012 loss_train: 1.6147 acc_train: 0.8973 loss_val: 0.6441 acc_val: 0.8836\n",
            "Epoch: 0013 loss_train: 1.5109 acc_train: 0.9048 loss_val: 0.6322 acc_val: 0.8859\n",
            "Epoch: 0014 loss_train: 1.4334 acc_train: 0.9102 loss_val: 0.6254 acc_val: 0.8851\n",
            "Epoch: 0015 loss_train: 1.4250 acc_train: 0.9068 loss_val: 0.6744 acc_val: 0.8803\n",
            "Epoch: 0016 loss_train: 1.4048 acc_train: 0.9094 loss_val: 0.6232 acc_val: 0.8849\n",
            "Epoch: 0017 loss_train: 1.2668 acc_train: 0.9198 loss_val: 0.6205 acc_val: 0.8851\n",
            "Epoch: 0018 loss_train: 1.1614 acc_train: 0.9289 loss_val: 0.6337 acc_val: 0.8884\n",
            "Epoch: 0019 loss_train: 1.0483 acc_train: 0.9373 loss_val: 0.6462 acc_val: 0.8879\n",
            "Epoch: 0020 loss_train: 0.9449 acc_train: 0.9438 loss_val: 0.6717 acc_val: 0.8869\n",
            "Epoch: 0021 loss_train: 0.8590 acc_train: 0.9527 loss_val: 0.6936 acc_val: 0.8859\n",
            "Epoch: 0022 loss_train: 0.8247 acc_train: 0.9527 loss_val: 0.7012 acc_val: 0.8897\n",
            "Epoch: 0023 loss_train: 0.7125 acc_train: 0.9595 loss_val: 0.7427 acc_val: 0.8811\n",
            "Epoch: 0024 loss_train: 0.7531 acc_train: 0.9556 loss_val: 0.8192 acc_val: 0.8740\n",
            "Epoch: 0025 loss_train: 0.6212 acc_train: 0.9616 loss_val: 0.7804 acc_val: 0.8831\n",
            "Epoch: 0026 loss_train: 0.5388 acc_train: 0.9704 loss_val: 0.8617 acc_val: 0.8783\n",
            "Epoch: 0027 loss_train: 0.5209 acc_train: 0.9687 loss_val: 0.9280 acc_val: 0.8859\n",
            "Epoch: 0028 loss_train: 0.4170 acc_train: 0.9768 loss_val: 0.8911 acc_val: 0.8856\n",
            "Epoch: 0029 loss_train: 0.3583 acc_train: 0.9786 loss_val: 1.0019 acc_val: 0.8796\n",
            "Epoch: 0030 loss_train: 0.3382 acc_train: 0.9785 loss_val: 1.0658 acc_val: 0.8808\n",
            "Epoch: 0031 loss_train: 0.3227 acc_train: 0.9797 loss_val: 1.1634 acc_val: 0.8735\n",
            "Epoch: 0032 loss_train: 0.3536 acc_train: 0.9773 loss_val: 1.1325 acc_val: 0.8785\n",
            "Epoch: 0033 loss_train: 0.3292 acc_train: 0.9787 loss_val: 1.1132 acc_val: 0.8765\n",
            "Epoch: 0034 loss_train: 0.3006 acc_train: 0.9820 loss_val: 1.1663 acc_val: 0.8856\n",
            "Epoch: 0035 loss_train: 0.2504 acc_train: 0.9847 loss_val: 1.1429 acc_val: 0.8793\n",
            "Epoch: 0036 loss_train: 0.2071 acc_train: 0.9873 loss_val: 1.2184 acc_val: 0.8742\n",
            "Epoch: 0037 loss_train: 0.1551 acc_train: 0.9916 loss_val: 1.2261 acc_val: 0.8808\n",
            "Epoch: 0038 loss_train: 0.1134 acc_train: 0.9938 loss_val: 1.3148 acc_val: 0.8778\n",
            "Epoch: 0039 loss_train: 0.0787 acc_train: 0.9958 loss_val: 1.3779 acc_val: 0.8829\n",
            "Epoch: 0040 loss_train: 0.0759 acc_train: 0.9956 loss_val: 1.3833 acc_val: 0.8821\n",
            "Epoch: 0041 loss_train: 0.0625 acc_train: 0.9963 loss_val: 1.5139 acc_val: 0.8824\n",
            "Epoch: 0042 loss_train: 0.1082 acc_train: 0.9942 loss_val: 1.5611 acc_val: 0.8765\n",
            "Epoch: 0043 loss_train: 0.0899 acc_train: 0.9945 loss_val: 1.5951 acc_val: 0.8785\n",
            "Epoch: 0044 loss_train: 0.0912 acc_train: 0.9946 loss_val: 1.5776 acc_val: 0.8745\n",
            "Epoch: 0045 loss_train: 0.1209 acc_train: 0.9939 loss_val: 1.5894 acc_val: 0.8770\n",
            "Epoch: 0046 loss_train: 0.1048 acc_train: 0.9939 loss_val: 1.5811 acc_val: 0.8788\n",
            "Epoch: 0047 loss_train: 0.1266 acc_train: 0.9929 loss_val: 1.5643 acc_val: 0.8753\n",
            "Epoch: 0048 loss_train: 0.1042 acc_train: 0.9937 loss_val: 1.5555 acc_val: 0.8722\n",
            "Epoch: 0049 loss_train: 0.1156 acc_train: 0.9933 loss_val: 1.6163 acc_val: 0.8775\n",
            "Epoch: 0050 loss_train: 0.1462 acc_train: 0.9917 loss_val: 1.6284 acc_val: 0.8682\n",
            "Epoch: 0051 loss_train: 0.1151 acc_train: 0.9934 loss_val: 1.6442 acc_val: 0.8753\n",
            "Epoch: 0052 loss_train: 0.1362 acc_train: 0.9917 loss_val: 1.5523 acc_val: 0.8760\n",
            "Epoch: 0053 loss_train: 0.1218 acc_train: 0.9924 loss_val: 1.5025 acc_val: 0.8755\n",
            "Epoch: 0054 loss_train: 0.1149 acc_train: 0.9937 loss_val: 1.6337 acc_val: 0.8727\n",
            "Epoch: 0055 loss_train: 0.0932 acc_train: 0.9946 loss_val: 1.6877 acc_val: 0.8661\n",
            "Epoch: 0056 loss_train: 0.1200 acc_train: 0.9922 loss_val: 1.6021 acc_val: 0.8763\n",
            "Epoch: 0057 loss_train: 0.1457 acc_train: 0.9920 loss_val: 1.6155 acc_val: 0.8694\n",
            "Epoch: 0058 loss_train: 0.1656 acc_train: 0.9907 loss_val: 1.5132 acc_val: 0.8735\n",
            "Epoch: 0059 loss_train: 0.1139 acc_train: 0.9941 loss_val: 1.5051 acc_val: 0.8808\n",
            "Epoch: 0060 loss_train: 0.0756 acc_train: 0.9954 loss_val: 1.5011 acc_val: 0.8742\n",
            "Epoch: 0061 loss_train: 0.0500 acc_train: 0.9975 loss_val: 1.5742 acc_val: 0.8778\n",
            "Epoch: 0062 loss_train: 0.0587 acc_train: 0.9964 loss_val: 1.6390 acc_val: 0.8775\n",
            "Epoch: 0063 loss_train: 0.0582 acc_train: 0.9963 loss_val: 1.6683 acc_val: 0.8730\n",
            "Epoch: 0064 loss_train: 0.0733 acc_train: 0.9961 loss_val: 1.6826 acc_val: 0.8745\n",
            "Epoch: 0065 loss_train: 0.0645 acc_train: 0.9956 loss_val: 1.7342 acc_val: 0.8742\n",
            "Epoch: 0066 loss_train: 0.0846 acc_train: 0.9959 loss_val: 1.7562 acc_val: 0.8737\n",
            "Epoch: 0067 loss_train: 0.0718 acc_train: 0.9964 loss_val: 1.6121 acc_val: 0.8755\n",
            "Epoch: 0068 loss_train: 0.0949 acc_train: 0.9949 loss_val: 1.5390 acc_val: 0.8732\n",
            "Epoch: 0069 loss_train: 0.0736 acc_train: 0.9959 loss_val: 1.5125 acc_val: 0.8803\n",
            "Epoch: 0070 loss_train: 0.0546 acc_train: 0.9968 loss_val: 1.5663 acc_val: 0.8793\n",
            "Epoch: 0071 loss_train: 0.0574 acc_train: 0.9963 loss_val: 1.5616 acc_val: 0.8816\n",
            "Epoch: 0072 loss_train: 0.0467 acc_train: 0.9977 loss_val: 1.6496 acc_val: 0.8788\n",
            "Optimization Finished!\n",
            "Train cost: 31.3521s\n",
            "Loading 22th epoch\n",
            "Test set results: loss= 0.7277 accuracy= 0.8894\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --dataset pubmed --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 5  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack none --attperc 20 --TrainTest train --weight_decay=1e-05"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qa9RSfRCRB6Y",
        "outputId": "5aa27844-25ec-4ae3-937f-471fa54fb93e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=515, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=3, bias=True)\n",
            ")\n",
            "total params: 2501125\n",
            "Epoch: 0001 loss_train: 6.5501 acc_train: 0.3981 loss_val: 2.0119 acc_val: 0.5649\n",
            "Epoch: 0002 loss_train: 5.6065 acc_train: 0.6518 loss_val: 1.6162 acc_val: 0.7130\n",
            "Epoch: 0003 loss_train: 4.4721 acc_train: 0.7281 loss_val: 1.2459 acc_val: 0.7677\n",
            "Epoch: 0004 loss_train: 3.5075 acc_train: 0.7861 loss_val: 1.0436 acc_val: 0.7994\n",
            "Epoch: 0005 loss_train: 3.0290 acc_train: 0.8048 loss_val: 0.9649 acc_val: 0.8119\n",
            "Epoch: 0006 loss_train: 2.8020 acc_train: 0.8158 loss_val: 0.8909 acc_val: 0.8281\n",
            "Epoch: 0007 loss_train: 2.5651 acc_train: 0.8313 loss_val: 0.8295 acc_val: 0.8408\n",
            "Epoch: 0008 loss_train: 2.3503 acc_train: 0.8471 loss_val: 0.7609 acc_val: 0.8550\n",
            "Epoch: 0009 loss_train: 2.1307 acc_train: 0.8596 loss_val: 0.7297 acc_val: 0.8659\n",
            "Epoch: 0010 loss_train: 1.9902 acc_train: 0.8715 loss_val: 0.6993 acc_val: 0.8692\n",
            "Epoch: 0011 loss_train: 1.8094 acc_train: 0.8827 loss_val: 0.6603 acc_val: 0.8796\n",
            "Epoch: 0012 loss_train: 1.7259 acc_train: 0.8885 loss_val: 0.6469 acc_val: 0.8788\n",
            "Epoch: 0013 loss_train: 1.5795 acc_train: 0.8995 loss_val: 0.6279 acc_val: 0.8826\n",
            "Epoch: 0014 loss_train: 1.4672 acc_train: 0.9074 loss_val: 0.6556 acc_val: 0.8801\n",
            "Epoch: 0015 loss_train: 1.4192 acc_train: 0.9096 loss_val: 0.6758 acc_val: 0.8818\n",
            "Epoch: 0016 loss_train: 1.3827 acc_train: 0.9139 loss_val: 0.6868 acc_val: 0.8730\n",
            "Epoch: 0017 loss_train: 1.3191 acc_train: 0.9179 loss_val: 0.6547 acc_val: 0.8856\n",
            "Epoch: 0018 loss_train: 1.1921 acc_train: 0.9260 loss_val: 0.6465 acc_val: 0.8841\n",
            "Epoch: 0019 loss_train: 1.0909 acc_train: 0.9345 loss_val: 0.6463 acc_val: 0.8897\n",
            "Epoch: 0020 loss_train: 0.9645 acc_train: 0.9439 loss_val: 0.6901 acc_val: 0.8844\n",
            "Epoch: 0021 loss_train: 0.9005 acc_train: 0.9473 loss_val: 0.7124 acc_val: 0.8869\n",
            "Epoch: 0022 loss_train: 0.8555 acc_train: 0.9510 loss_val: 0.6967 acc_val: 0.8925\n",
            "Epoch: 0023 loss_train: 0.8406 acc_train: 0.9485 loss_val: 0.8138 acc_val: 0.8785\n",
            "Epoch: 0024 loss_train: 0.7509 acc_train: 0.9572 loss_val: 0.7262 acc_val: 0.8841\n",
            "Epoch: 0025 loss_train: 0.6399 acc_train: 0.9624 loss_val: 0.8403 acc_val: 0.8793\n",
            "Epoch: 0026 loss_train: 0.5904 acc_train: 0.9652 loss_val: 0.8542 acc_val: 0.8783\n",
            "Epoch: 0027 loss_train: 0.6518 acc_train: 0.9594 loss_val: 0.8782 acc_val: 0.8846\n",
            "Epoch: 0028 loss_train: 0.5043 acc_train: 0.9713 loss_val: 0.8964 acc_val: 0.8821\n",
            "Epoch: 0029 loss_train: 0.4189 acc_train: 0.9770 loss_val: 0.9551 acc_val: 0.8859\n",
            "Epoch: 0030 loss_train: 0.3413 acc_train: 0.9801 loss_val: 0.9907 acc_val: 0.8849\n",
            "Epoch: 0031 loss_train: 0.2914 acc_train: 0.9848 loss_val: 1.0831 acc_val: 0.8796\n",
            "Epoch: 0032 loss_train: 0.2430 acc_train: 0.9871 loss_val: 1.1238 acc_val: 0.8753\n",
            "Epoch: 0033 loss_train: 0.9079 acc_train: 0.9593 loss_val: 1.3988 acc_val: 0.8418\n",
            "Epoch: 0034 loss_train: 1.2033 acc_train: 0.9333 loss_val: 0.8806 acc_val: 0.8780\n",
            "Epoch: 0035 loss_train: 0.9049 acc_train: 0.9462 loss_val: 0.8230 acc_val: 0.8841\n",
            "Epoch: 0036 loss_train: 0.6416 acc_train: 0.9631 loss_val: 0.8846 acc_val: 0.8758\n",
            "Epoch: 0037 loss_train: 0.4660 acc_train: 0.9724 loss_val: 0.9672 acc_val: 0.8758\n",
            "Epoch: 0038 loss_train: 0.3359 acc_train: 0.9814 loss_val: 1.0595 acc_val: 0.8735\n",
            "Epoch: 0039 loss_train: 0.2306 acc_train: 0.9863 loss_val: 1.0960 acc_val: 0.8796\n",
            "Epoch: 0040 loss_train: 0.1673 acc_train: 0.9912 loss_val: 1.1927 acc_val: 0.8737\n",
            "Epoch: 0041 loss_train: 0.1448 acc_train: 0.9924 loss_val: 1.2910 acc_val: 0.8758\n",
            "Epoch: 0042 loss_train: 0.1070 acc_train: 0.9951 loss_val: 1.3392 acc_val: 0.8768\n",
            "Epoch: 0043 loss_train: 0.0794 acc_train: 0.9963 loss_val: 1.4030 acc_val: 0.8720\n",
            "Epoch: 0044 loss_train: 0.1010 acc_train: 0.9944 loss_val: 1.3984 acc_val: 0.8798\n",
            "Epoch: 0045 loss_train: 0.2647 acc_train: 0.9848 loss_val: 1.5598 acc_val: 0.8697\n",
            "Epoch: 0046 loss_train: 0.7891 acc_train: 0.9584 loss_val: 1.3054 acc_val: 0.8644\n",
            "Epoch: 0047 loss_train: 0.8853 acc_train: 0.9497 loss_val: 0.9761 acc_val: 0.8867\n",
            "Epoch: 0048 loss_train: 0.4858 acc_train: 0.9712 loss_val: 1.0435 acc_val: 0.8674\n",
            "Epoch: 0049 loss_train: 0.3412 acc_train: 0.9793 loss_val: 1.0470 acc_val: 0.8783\n",
            "Epoch: 0050 loss_train: 0.2004 acc_train: 0.9900 loss_val: 1.1576 acc_val: 0.8747\n",
            "Epoch: 0051 loss_train: 0.1549 acc_train: 0.9921 loss_val: 1.1801 acc_val: 0.8770\n",
            "Epoch: 0052 loss_train: 0.0915 acc_train: 0.9959 loss_val: 1.2973 acc_val: 0.8796\n",
            "Epoch: 0053 loss_train: 0.0652 acc_train: 0.9970 loss_val: 1.3719 acc_val: 0.8806\n",
            "Epoch: 0054 loss_train: 0.0450 acc_train: 0.9981 loss_val: 1.4272 acc_val: 0.8793\n",
            "Epoch: 0055 loss_train: 0.0417 acc_train: 0.9980 loss_val: 1.4693 acc_val: 0.8788\n",
            "Epoch: 0056 loss_train: 0.0374 acc_train: 0.9986 loss_val: 1.5087 acc_val: 0.8763\n",
            "Epoch: 0057 loss_train: 0.0356 acc_train: 0.9982 loss_val: 1.5357 acc_val: 0.8818\n",
            "Epoch: 0058 loss_train: 0.0346 acc_train: 0.9981 loss_val: 1.5415 acc_val: 0.8796\n",
            "Epoch: 0059 loss_train: 0.0227 acc_train: 0.9992 loss_val: 1.5624 acc_val: 0.8808\n",
            "Epoch: 0060 loss_train: 0.0378 acc_train: 0.9981 loss_val: 1.6500 acc_val: 0.8813\n",
            "Epoch: 0061 loss_train: 0.0418 acc_train: 0.9984 loss_val: 1.6331 acc_val: 0.8811\n",
            "Epoch: 0062 loss_train: 0.0380 acc_train: 0.9981 loss_val: 1.6321 acc_val: 0.8818\n",
            "Epoch: 0063 loss_train: 0.0487 acc_train: 0.9975 loss_val: 1.6724 acc_val: 0.8747\n",
            "Epoch: 0064 loss_train: 0.0577 acc_train: 0.9970 loss_val: 1.6476 acc_val: 0.8811\n",
            "Epoch: 0065 loss_train: 0.0627 acc_train: 0.9969 loss_val: 1.6544 acc_val: 0.8753\n",
            "Epoch: 0066 loss_train: 0.0658 acc_train: 0.9959 loss_val: 1.7261 acc_val: 0.8712\n",
            "Epoch: 0067 loss_train: 0.0810 acc_train: 0.9960 loss_val: 1.6016 acc_val: 0.8720\n",
            "Epoch: 0068 loss_train: 0.0848 acc_train: 0.9956 loss_val: 1.5358 acc_val: 0.8765\n",
            "Epoch: 0069 loss_train: 0.0741 acc_train: 0.9965 loss_val: 1.5594 acc_val: 0.8770\n",
            "Epoch: 0070 loss_train: 0.0644 acc_train: 0.9959 loss_val: 1.5393 acc_val: 0.8783\n",
            "Epoch: 0071 loss_train: 0.0665 acc_train: 0.9964 loss_val: 1.5793 acc_val: 0.8796\n",
            "Epoch: 0072 loss_train: 0.0726 acc_train: 0.9964 loss_val: 1.5245 acc_val: 0.8763\n",
            "Optimization Finished!\n",
            "Train cost: 47.2305s\n",
            "Loading 22th epoch\n",
            "Test set results: loss= 0.7160 accuracy= 0.8922\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --dataset pubmed --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 7  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack none --attperc 20 --TrainTest train --weight_decay=1e-05"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2XWz4UuREgw",
        "outputId": "90d3a6b4-ca4c-408b-92a2-99ec303ca383"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=515, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=3, bias=True)\n",
            ")\n",
            "total params: 2501125\n",
            "Epoch: 0001 loss_train: 6.5268 acc_train: 0.3937 loss_val: 2.0050 acc_val: 0.5621\n",
            "Epoch: 0002 loss_train: 5.6107 acc_train: 0.6451 loss_val: 1.6404 acc_val: 0.6889\n",
            "Epoch: 0003 loss_train: 4.5798 acc_train: 0.7072 loss_val: 1.2995 acc_val: 0.7518\n",
            "Epoch: 0004 loss_train: 3.6574 acc_train: 0.7731 loss_val: 1.0907 acc_val: 0.7875\n",
            "Epoch: 0005 loss_train: 3.1752 acc_train: 0.7946 loss_val: 1.0121 acc_val: 0.8055\n",
            "Epoch: 0006 loss_train: 2.9188 acc_train: 0.8090 loss_val: 0.9214 acc_val: 0.8230\n",
            "Epoch: 0007 loss_train: 2.6477 acc_train: 0.8278 loss_val: 0.8484 acc_val: 0.8347\n",
            "Epoch: 0008 loss_train: 2.4033 acc_train: 0.8418 loss_val: 0.7762 acc_val: 0.8537\n",
            "Epoch: 0009 loss_train: 2.1739 acc_train: 0.8575 loss_val: 0.7444 acc_val: 0.8631\n",
            "Epoch: 0010 loss_train: 2.0221 acc_train: 0.8680 loss_val: 0.7093 acc_val: 0.8671\n",
            "Epoch: 0011 loss_train: 1.8280 acc_train: 0.8831 loss_val: 0.6645 acc_val: 0.8765\n",
            "Epoch: 0012 loss_train: 1.7502 acc_train: 0.8883 loss_val: 0.6576 acc_val: 0.8793\n",
            "Epoch: 0013 loss_train: 1.5969 acc_train: 0.8994 loss_val: 0.6499 acc_val: 0.8801\n",
            "Epoch: 0014 loss_train: 1.5122 acc_train: 0.9037 loss_val: 0.6885 acc_val: 0.8760\n",
            "Epoch: 0015 loss_train: 1.4774 acc_train: 0.9064 loss_val: 0.6343 acc_val: 0.8867\n",
            "Epoch: 0016 loss_train: 1.3783 acc_train: 0.9145 loss_val: 0.6252 acc_val: 0.8834\n",
            "Epoch: 0017 loss_train: 1.2856 acc_train: 0.9230 loss_val: 0.6251 acc_val: 0.8897\n",
            "Epoch: 0018 loss_train: 1.1536 acc_train: 0.9307 loss_val: 0.6529 acc_val: 0.8854\n",
            "Epoch: 0019 loss_train: 1.0949 acc_train: 0.9336 loss_val: 0.6890 acc_val: 0.8851\n",
            "Epoch: 0020 loss_train: 1.0392 acc_train: 0.9375 loss_val: 0.6600 acc_val: 0.8887\n",
            "Epoch: 0021 loss_train: 0.9177 acc_train: 0.9467 loss_val: 0.6957 acc_val: 0.8887\n",
            "Epoch: 0022 loss_train: 0.8809 acc_train: 0.9458 loss_val: 0.7379 acc_val: 0.8803\n",
            "Epoch: 0023 loss_train: 0.8144 acc_train: 0.9530 loss_val: 0.7723 acc_val: 0.8826\n",
            "Epoch: 0024 loss_train: 0.7890 acc_train: 0.9523 loss_val: 0.8411 acc_val: 0.8747\n",
            "Epoch: 0025 loss_train: 0.7780 acc_train: 0.9509 loss_val: 0.8303 acc_val: 0.8783\n",
            "Epoch: 0026 loss_train: 0.7116 acc_train: 0.9564 loss_val: 0.8051 acc_val: 0.8839\n",
            "Epoch: 0027 loss_train: 0.5610 acc_train: 0.9684 loss_val: 0.8101 acc_val: 0.8869\n",
            "Epoch: 0028 loss_train: 0.4670 acc_train: 0.9745 loss_val: 0.9474 acc_val: 0.8780\n",
            "Epoch: 0029 loss_train: 0.4295 acc_train: 0.9751 loss_val: 0.9985 acc_val: 0.8760\n",
            "Epoch: 0030 loss_train: 0.3867 acc_train: 0.9766 loss_val: 1.0297 acc_val: 0.8813\n",
            "Epoch: 0031 loss_train: 0.3107 acc_train: 0.9823 loss_val: 1.0278 acc_val: 0.8841\n",
            "Epoch: 0032 loss_train: 0.2900 acc_train: 0.9840 loss_val: 1.1981 acc_val: 0.8763\n",
            "Epoch: 0033 loss_train: 0.7144 acc_train: 0.9576 loss_val: 1.1541 acc_val: 0.8605\n",
            "Epoch: 0034 loss_train: 0.6196 acc_train: 0.9594 loss_val: 1.0990 acc_val: 0.8826\n",
            "Epoch: 0035 loss_train: 0.6583 acc_train: 0.9623 loss_val: 1.1192 acc_val: 0.8770\n",
            "Epoch: 0036 loss_train: 0.5769 acc_train: 0.9657 loss_val: 0.8821 acc_val: 0.8778\n",
            "Epoch: 0037 loss_train: 0.3560 acc_train: 0.9807 loss_val: 0.9838 acc_val: 0.8816\n",
            "Epoch: 0038 loss_train: 0.2502 acc_train: 0.9858 loss_val: 1.0691 acc_val: 0.8785\n",
            "Epoch: 0039 loss_train: 0.1834 acc_train: 0.9899 loss_val: 1.1508 acc_val: 0.8803\n",
            "Epoch: 0040 loss_train: 0.1176 acc_train: 0.9932 loss_val: 1.1723 acc_val: 0.8770\n",
            "Epoch: 0041 loss_train: 0.0921 acc_train: 0.9954 loss_val: 1.2731 acc_val: 0.8791\n",
            "Epoch: 0042 loss_train: 0.0806 acc_train: 0.9963 loss_val: 1.3150 acc_val: 0.8824\n",
            "Epoch: 0043 loss_train: 0.0630 acc_train: 0.9975 loss_val: 1.3461 acc_val: 0.8851\n",
            "Epoch: 0044 loss_train: 0.0423 acc_train: 0.9982 loss_val: 1.4321 acc_val: 0.8834\n",
            "Epoch: 0045 loss_train: 0.0405 acc_train: 0.9981 loss_val: 1.4822 acc_val: 0.8816\n",
            "Epoch: 0046 loss_train: 0.0403 acc_train: 0.9978 loss_val: 1.4735 acc_val: 0.8803\n",
            "Epoch: 0047 loss_train: 0.0452 acc_train: 0.9975 loss_val: 1.5169 acc_val: 0.8818\n",
            "Epoch: 0048 loss_train: 0.0573 acc_train: 0.9966 loss_val: 1.5403 acc_val: 0.8811\n",
            "Epoch: 0049 loss_train: 0.0507 acc_train: 0.9970 loss_val: 1.6027 acc_val: 0.8780\n",
            "Epoch: 0050 loss_train: 0.0522 acc_train: 0.9970 loss_val: 1.5824 acc_val: 0.8791\n",
            "Epoch: 0051 loss_train: 0.0622 acc_train: 0.9969 loss_val: 1.6464 acc_val: 0.8753\n",
            "Epoch: 0052 loss_train: 0.0543 acc_train: 0.9969 loss_val: 1.6753 acc_val: 0.8742\n",
            "Epoch: 0053 loss_train: 0.0756 acc_train: 0.9961 loss_val: 1.7133 acc_val: 0.8735\n",
            "Epoch: 0054 loss_train: 0.0723 acc_train: 0.9958 loss_val: 1.7299 acc_val: 0.8770\n",
            "Epoch: 0055 loss_train: 0.1142 acc_train: 0.9930 loss_val: 2.0479 acc_val: 0.8514\n",
            "Epoch: 0056 loss_train: 5.6391 acc_train: 0.7626 loss_val: 2.3017 acc_val: 0.5294\n",
            "Epoch: 0057 loss_train: 4.7769 acc_train: 0.6653 loss_val: 1.0550 acc_val: 0.7918\n",
            "Epoch: 0058 loss_train: 2.7487 acc_train: 0.8244 loss_val: 0.8637 acc_val: 0.8347\n",
            "Epoch: 0059 loss_train: 3.2688 acc_train: 0.7867 loss_val: 0.9494 acc_val: 0.8012\n",
            "Epoch: 0060 loss_train: 2.5691 acc_train: 0.8255 loss_val: 0.9026 acc_val: 0.8131\n",
            "Epoch: 0061 loss_train: 2.3066 acc_train: 0.8536 loss_val: 0.7375 acc_val: 0.8598\n",
            "Epoch: 0062 loss_train: 2.0242 acc_train: 0.8661 loss_val: 0.7294 acc_val: 0.8646\n",
            "Epoch: 0063 loss_train: 1.9044 acc_train: 0.8779 loss_val: 0.6774 acc_val: 0.8687\n",
            "Epoch: 0064 loss_train: 1.7796 acc_train: 0.8870 loss_val: 0.6624 acc_val: 0.8709\n",
            "Epoch: 0065 loss_train: 1.6959 acc_train: 0.8922 loss_val: 0.6309 acc_val: 0.8791\n",
            "Epoch: 0066 loss_train: 1.6078 acc_train: 0.8992 loss_val: 0.6236 acc_val: 0.8806\n",
            "Epoch: 0067 loss_train: 1.5173 acc_train: 0.9060 loss_val: 0.6224 acc_val: 0.8806\n",
            "Epoch: 0068 loss_train: 1.4677 acc_train: 0.9092 loss_val: 0.6067 acc_val: 0.8851\n",
            "Epoch: 0069 loss_train: 1.3689 acc_train: 0.9154 loss_val: 0.6076 acc_val: 0.8867\n",
            "Epoch: 0070 loss_train: 1.3036 acc_train: 0.9176 loss_val: 0.6205 acc_val: 0.8851\n",
            "Epoch: 0071 loss_train: 1.2713 acc_train: 0.9203 loss_val: 0.6351 acc_val: 0.8905\n",
            "Epoch: 0072 loss_train: 1.2381 acc_train: 0.9236 loss_val: 0.6851 acc_val: 0.8750\n",
            "Epoch: 0073 loss_train: 1.2760 acc_train: 0.9182 loss_val: 0.6240 acc_val: 0.8872\n",
            "Epoch: 0074 loss_train: 1.2092 acc_train: 0.9250 loss_val: 0.6453 acc_val: 0.8869\n",
            "Epoch: 0075 loss_train: 1.1753 acc_train: 0.9275 loss_val: 0.6733 acc_val: 0.8796\n",
            "Epoch: 0076 loss_train: 1.0985 acc_train: 0.9304 loss_val: 0.6581 acc_val: 0.8889\n",
            "Epoch: 0077 loss_train: 1.0217 acc_train: 0.9390 loss_val: 0.6515 acc_val: 0.8846\n",
            "Epoch: 0078 loss_train: 0.9535 acc_train: 0.9441 loss_val: 0.6804 acc_val: 0.8915\n",
            "Epoch: 0079 loss_train: 0.9203 acc_train: 0.9451 loss_val: 0.7042 acc_val: 0.8895\n",
            "Epoch: 0080 loss_train: 0.8812 acc_train: 0.9471 loss_val: 0.7121 acc_val: 0.8884\n",
            "Epoch: 0081 loss_train: 0.8589 acc_train: 0.9501 loss_val: 0.7313 acc_val: 0.8859\n",
            "Epoch: 0082 loss_train: 0.7780 acc_train: 0.9577 loss_val: 0.7680 acc_val: 0.8889\n",
            "Epoch: 0083 loss_train: 0.7578 acc_train: 0.9568 loss_val: 0.7519 acc_val: 0.8864\n",
            "Epoch: 0084 loss_train: 0.7270 acc_train: 0.9597 loss_val: 0.8329 acc_val: 0.8796\n",
            "Epoch: 0085 loss_train: 0.9022 acc_train: 0.9493 loss_val: 0.8528 acc_val: 0.8697\n",
            "Epoch: 0086 loss_train: 1.0906 acc_train: 0.9307 loss_val: 0.6755 acc_val: 0.8877\n",
            "Epoch: 0087 loss_train: 0.9863 acc_train: 0.9408 loss_val: 0.6651 acc_val: 0.8912\n",
            "Epoch: 0088 loss_train: 0.8798 acc_train: 0.9475 loss_val: 0.6891 acc_val: 0.8859\n",
            "Epoch: 0089 loss_train: 0.7700 acc_train: 0.9553 loss_val: 0.6914 acc_val: 0.8887\n",
            "Epoch: 0090 loss_train: 0.7212 acc_train: 0.9589 loss_val: 0.7566 acc_val: 0.8920\n",
            "Epoch: 0091 loss_train: 0.6481 acc_train: 0.9633 loss_val: 0.7884 acc_val: 0.8846\n",
            "Epoch: 0092 loss_train: 0.6038 acc_train: 0.9670 loss_val: 0.8251 acc_val: 0.8839\n",
            "Epoch: 0093 loss_train: 0.5660 acc_train: 0.9691 loss_val: 0.8529 acc_val: 0.8851\n",
            "Epoch: 0094 loss_train: 0.5659 acc_train: 0.9702 loss_val: 0.8750 acc_val: 0.8829\n",
            "Epoch: 0095 loss_train: 0.5375 acc_train: 0.9702 loss_val: 0.8799 acc_val: 0.8859\n",
            "Epoch: 0096 loss_train: 0.5349 acc_train: 0.9714 loss_val: 0.8961 acc_val: 0.8874\n",
            "Epoch: 0097 loss_train: 0.4617 acc_train: 0.9767 loss_val: 0.9232 acc_val: 0.8854\n",
            "Epoch: 0098 loss_train: 0.4605 acc_train: 0.9757 loss_val: 0.9660 acc_val: 0.8791\n",
            "Epoch: 0099 loss_train: 0.4736 acc_train: 0.9759 loss_val: 0.9793 acc_val: 0.8829\n",
            "Epoch: 0100 loss_train: 0.4557 acc_train: 0.9761 loss_val: 0.9576 acc_val: 0.8826\n",
            "Epoch: 0101 loss_train: 0.4262 acc_train: 0.9791 loss_val: 0.9954 acc_val: 0.8841\n",
            "Epoch: 0102 loss_train: 0.4439 acc_train: 0.9765 loss_val: 1.0609 acc_val: 0.8798\n",
            "Epoch: 0103 loss_train: 0.5062 acc_train: 0.9711 loss_val: 0.9898 acc_val: 0.8834\n",
            "Epoch: 0104 loss_train: 0.4448 acc_train: 0.9768 loss_val: 0.9441 acc_val: 0.8887\n",
            "Epoch: 0105 loss_train: 0.4457 acc_train: 0.9773 loss_val: 0.9701 acc_val: 0.8801\n",
            "Epoch: 0106 loss_train: 0.4389 acc_train: 0.9767 loss_val: 1.0631 acc_val: 0.8763\n",
            "Epoch: 0107 loss_train: 0.4121 acc_train: 0.9794 loss_val: 0.9949 acc_val: 0.8793\n",
            "Epoch: 0108 loss_train: 0.3538 acc_train: 0.9837 loss_val: 1.0254 acc_val: 0.8824\n",
            "Epoch: 0109 loss_train: 0.3436 acc_train: 0.9836 loss_val: 1.0270 acc_val: 0.8839\n",
            "Epoch: 0110 loss_train: 0.3398 acc_train: 0.9838 loss_val: 1.0582 acc_val: 0.8829\n",
            "Epoch: 0111 loss_train: 0.3228 acc_train: 0.9840 loss_val: 1.0969 acc_val: 0.8798\n",
            "Epoch: 0112 loss_train: 0.3675 acc_train: 0.9806 loss_val: 1.0711 acc_val: 0.8829\n",
            "Epoch: 0113 loss_train: 0.3062 acc_train: 0.9853 loss_val: 1.0910 acc_val: 0.8816\n",
            "Epoch: 0114 loss_train: 0.2863 acc_train: 0.9860 loss_val: 1.1241 acc_val: 0.8775\n",
            "Epoch: 0115 loss_train: 0.2794 acc_train: 0.9877 loss_val: 1.1985 acc_val: 0.8798\n",
            "Epoch: 0116 loss_train: 0.2628 acc_train: 0.9877 loss_val: 1.1814 acc_val: 0.8780\n",
            "Epoch: 0117 loss_train: 0.2706 acc_train: 0.9870 loss_val: 1.2116 acc_val: 0.8811\n",
            "Epoch: 0118 loss_train: 0.2647 acc_train: 0.9873 loss_val: 1.1897 acc_val: 0.8811\n",
            "Epoch: 0119 loss_train: 0.2516 acc_train: 0.9891 loss_val: 1.1783 acc_val: 0.8824\n",
            "Epoch: 0120 loss_train: 0.2204 acc_train: 0.9901 loss_val: 1.2201 acc_val: 0.8798\n",
            "Epoch: 0121 loss_train: 0.2276 acc_train: 0.9901 loss_val: 1.2666 acc_val: 0.8793\n",
            "Epoch: 0122 loss_train: 0.2196 acc_train: 0.9899 loss_val: 1.2593 acc_val: 0.8801\n",
            "Epoch: 0123 loss_train: 0.1977 acc_train: 0.9910 loss_val: 1.2743 acc_val: 0.8818\n",
            "Epoch: 0124 loss_train: 0.1905 acc_train: 0.9919 loss_val: 1.3226 acc_val: 0.8773\n",
            "Epoch: 0125 loss_train: 0.1880 acc_train: 0.9926 loss_val: 1.3028 acc_val: 0.8801\n",
            "Epoch: 0126 loss_train: 0.1949 acc_train: 0.9920 loss_val: 1.3192 acc_val: 0.8780\n",
            "Epoch: 0127 loss_train: 0.2209 acc_train: 0.9902 loss_val: 1.3070 acc_val: 0.8758\n",
            "Epoch: 0128 loss_train: 0.2622 acc_train: 0.9866 loss_val: 1.2770 acc_val: 0.8829\n",
            "Epoch: 0129 loss_train: 0.2576 acc_train: 0.9884 loss_val: 1.2361 acc_val: 0.8785\n",
            "Epoch: 0130 loss_train: 0.2407 acc_train: 0.9892 loss_val: 1.3049 acc_val: 0.8765\n",
            "Epoch: 0131 loss_train: 0.2296 acc_train: 0.9901 loss_val: 1.2518 acc_val: 0.8791\n",
            "Epoch: 0132 loss_train: 0.2209 acc_train: 0.9899 loss_val: 1.2675 acc_val: 0.8803\n",
            "Epoch: 0133 loss_train: 0.2049 acc_train: 0.9909 loss_val: 1.2856 acc_val: 0.8796\n",
            "Epoch: 0134 loss_train: 0.1963 acc_train: 0.9913 loss_val: 1.2263 acc_val: 0.8803\n",
            "Epoch: 0135 loss_train: 0.2195 acc_train: 0.9897 loss_val: 1.2534 acc_val: 0.8826\n",
            "Epoch: 0136 loss_train: 0.1919 acc_train: 0.9917 loss_val: 1.3379 acc_val: 0.8778\n",
            "Epoch: 0137 loss_train: 0.1735 acc_train: 0.9931 loss_val: 1.2871 acc_val: 0.8829\n",
            "Epoch: 0138 loss_train: 0.1707 acc_train: 0.9933 loss_val: 1.3179 acc_val: 0.8818\n",
            "Epoch: 0139 loss_train: 0.1558 acc_train: 0.9936 loss_val: 1.3507 acc_val: 0.8831\n",
            "Epoch: 0140 loss_train: 0.1588 acc_train: 0.9929 loss_val: 1.3458 acc_val: 0.8806\n",
            "Optimization Finished!\n",
            "Train cost: 115.6131s\n",
            "Loading 90th epoch\n",
            "Test set results: loss= 0.7947 accuracy= 0.8813\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --dataset pubmed --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 10  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack none --attperc 20 --TrainTest train --weight_decay=1e-05"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEqcu9w8RGgz",
        "outputId": "2412d5a9-e9d5-47a9-c853-8488413ca91f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=515, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=3, bias=True)\n",
            ")\n",
            "total params: 2501125\n",
            "Epoch: 0001 loss_train: 6.5042 acc_train: 0.4004 loss_val: 2.0011 acc_val: 0.5636\n",
            "Epoch: 0002 loss_train: 5.6308 acc_train: 0.6199 loss_val: 1.6749 acc_val: 0.6516\n",
            "Epoch: 0003 loss_train: 4.7308 acc_train: 0.6751 loss_val: 1.3786 acc_val: 0.7244\n",
            "Epoch: 0004 loss_train: 3.8708 acc_train: 0.7570 loss_val: 1.1440 acc_val: 0.7827\n",
            "Epoch: 0005 loss_train: 3.3376 acc_train: 0.7861 loss_val: 1.0582 acc_val: 0.7959\n",
            "Epoch: 0006 loss_train: 3.0418 acc_train: 0.7999 loss_val: 0.9503 acc_val: 0.8134\n",
            "Epoch: 0007 loss_train: 2.7301 acc_train: 0.8211 loss_val: 0.8717 acc_val: 0.8311\n",
            "Epoch: 0008 loss_train: 2.4690 acc_train: 0.8394 loss_val: 0.7975 acc_val: 0.8453\n",
            "Epoch: 0009 loss_train: 2.2280 acc_train: 0.8532 loss_val: 0.7525 acc_val: 0.8626\n",
            "Epoch: 0010 loss_train: 2.0569 acc_train: 0.8658 loss_val: 0.7155 acc_val: 0.8644\n",
            "Epoch: 0011 loss_train: 1.8737 acc_train: 0.8784 loss_val: 0.6710 acc_val: 0.8737\n",
            "Epoch: 0012 loss_train: 1.7773 acc_train: 0.8880 loss_val: 0.6746 acc_val: 0.8760\n",
            "Epoch: 0013 loss_train: 1.6400 acc_train: 0.8964 loss_val: 0.6623 acc_val: 0.8755\n",
            "Epoch: 0014 loss_train: 1.5562 acc_train: 0.9024 loss_val: 0.6692 acc_val: 0.8742\n",
            "Epoch: 0015 loss_train: 1.5095 acc_train: 0.9028 loss_val: 0.6524 acc_val: 0.8824\n",
            "Epoch: 0016 loss_train: 1.4393 acc_train: 0.9092 loss_val: 0.6127 acc_val: 0.8884\n",
            "Epoch: 0017 loss_train: 1.3511 acc_train: 0.9172 loss_val: 0.6348 acc_val: 0.8882\n",
            "Epoch: 0018 loss_train: 1.2446 acc_train: 0.9219 loss_val: 0.6384 acc_val: 0.8887\n",
            "Epoch: 0019 loss_train: 1.1311 acc_train: 0.9302 loss_val: 0.6543 acc_val: 0.8846\n",
            "Epoch: 0020 loss_train: 1.0499 acc_train: 0.9383 loss_val: 0.6672 acc_val: 0.8862\n",
            "Epoch: 0021 loss_train: 0.9692 acc_train: 0.9437 loss_val: 0.6939 acc_val: 0.8884\n",
            "Epoch: 0022 loss_train: 0.9750 acc_train: 0.9402 loss_val: 0.6694 acc_val: 0.8930\n",
            "Epoch: 0023 loss_train: 0.8704 acc_train: 0.9467 loss_val: 0.6943 acc_val: 0.8933\n",
            "Epoch: 0024 loss_train: 0.8761 acc_train: 0.9454 loss_val: 0.7139 acc_val: 0.8889\n",
            "Epoch: 0025 loss_train: 0.7613 acc_train: 0.9546 loss_val: 0.7279 acc_val: 0.8877\n",
            "Epoch: 0026 loss_train: 0.6740 acc_train: 0.9623 loss_val: 0.7663 acc_val: 0.8877\n",
            "Epoch: 0027 loss_train: 0.5619 acc_train: 0.9691 loss_val: 0.8314 acc_val: 0.8867\n",
            "Epoch: 0028 loss_train: 0.4968 acc_train: 0.9724 loss_val: 0.9458 acc_val: 0.8753\n",
            "Epoch: 0029 loss_train: 0.7621 acc_train: 0.9563 loss_val: 1.1755 acc_val: 0.8514\n",
            "Epoch: 0030 loss_train: 0.8497 acc_train: 0.9462 loss_val: 0.8563 acc_val: 0.8740\n",
            "Epoch: 0031 loss_train: 0.6144 acc_train: 0.9609 loss_val: 0.8825 acc_val: 0.8920\n",
            "Epoch: 0032 loss_train: 0.4643 acc_train: 0.9724 loss_val: 0.9170 acc_val: 0.8806\n",
            "Epoch: 0033 loss_train: 0.3341 acc_train: 0.9822 loss_val: 0.9645 acc_val: 0.8793\n",
            "Epoch: 0034 loss_train: 0.2771 acc_train: 0.9845 loss_val: 1.1216 acc_val: 0.8867\n",
            "Epoch: 0035 loss_train: 0.2852 acc_train: 0.9850 loss_val: 1.0747 acc_val: 0.8803\n",
            "Epoch: 0036 loss_train: 0.2362 acc_train: 0.9868 loss_val: 1.1505 acc_val: 0.8712\n",
            "Epoch: 0037 loss_train: 0.1997 acc_train: 0.9884 loss_val: 1.2212 acc_val: 0.8730\n",
            "Epoch: 0038 loss_train: 0.1641 acc_train: 0.9906 loss_val: 1.3128 acc_val: 0.8689\n",
            "Epoch: 0039 loss_train: 0.1663 acc_train: 0.9916 loss_val: 1.2706 acc_val: 0.8730\n",
            "Epoch: 0040 loss_train: 0.1551 acc_train: 0.9911 loss_val: 1.3375 acc_val: 0.8770\n",
            "Epoch: 0041 loss_train: 0.1753 acc_train: 0.9899 loss_val: 1.3925 acc_val: 0.8732\n",
            "Epoch: 0042 loss_train: 0.1709 acc_train: 0.9904 loss_val: 1.4197 acc_val: 0.8717\n",
            "Epoch: 0043 loss_train: 0.2437 acc_train: 0.9850 loss_val: 1.4927 acc_val: 0.8666\n",
            "Epoch: 0044 loss_train: 0.3130 acc_train: 0.9814 loss_val: 1.3722 acc_val: 0.8801\n",
            "Epoch: 0045 loss_train: 0.3488 acc_train: 0.9790 loss_val: 1.3090 acc_val: 0.8765\n",
            "Epoch: 0046 loss_train: 0.3541 acc_train: 0.9795 loss_val: 1.1579 acc_val: 0.8692\n",
            "Epoch: 0047 loss_train: 0.2207 acc_train: 0.9870 loss_val: 1.2305 acc_val: 0.8773\n",
            "Epoch: 0048 loss_train: 0.1480 acc_train: 0.9915 loss_val: 1.2510 acc_val: 0.8836\n",
            "Epoch: 0049 loss_train: 0.1119 acc_train: 0.9946 loss_val: 1.3411 acc_val: 0.8727\n",
            "Epoch: 0050 loss_train: 0.0773 acc_train: 0.9958 loss_val: 1.4023 acc_val: 0.8765\n",
            "Epoch: 0051 loss_train: 0.0609 acc_train: 0.9964 loss_val: 1.5399 acc_val: 0.8664\n",
            "Epoch: 0052 loss_train: 0.0533 acc_train: 0.9971 loss_val: 1.5841 acc_val: 0.8780\n",
            "Epoch: 0053 loss_train: 0.0463 acc_train: 0.9977 loss_val: 1.5754 acc_val: 0.8773\n",
            "Epoch: 0054 loss_train: 0.0391 acc_train: 0.9981 loss_val: 1.5584 acc_val: 0.8813\n",
            "Epoch: 0055 loss_train: 0.0326 acc_train: 0.9979 loss_val: 1.6606 acc_val: 0.8747\n",
            "Epoch: 0056 loss_train: 0.0367 acc_train: 0.9980 loss_val: 1.6939 acc_val: 0.8796\n",
            "Epoch: 0057 loss_train: 0.0423 acc_train: 0.9979 loss_val: 1.7222 acc_val: 0.8753\n",
            "Epoch: 0058 loss_train: 0.0682 acc_train: 0.9964 loss_val: 1.7249 acc_val: 0.8768\n",
            "Epoch: 0059 loss_train: 0.2168 acc_train: 0.9886 loss_val: 2.1752 acc_val: 0.8590\n",
            "Epoch: 0060 loss_train: 4.7717 acc_train: 0.8369 loss_val: 4.1930 acc_val: 0.5160\n",
            "Epoch: 0061 loss_train: 5.8772 acc_train: 0.6713 loss_val: 1.3359 acc_val: 0.7683\n",
            "Epoch: 0062 loss_train: 3.2401 acc_train: 0.8097 loss_val: 0.8878 acc_val: 0.8390\n",
            "Epoch: 0063 loss_train: 2.3083 acc_train: 0.8578 loss_val: 0.7220 acc_val: 0.8654\n",
            "Epoch: 0064 loss_train: 2.0435 acc_train: 0.8770 loss_val: 0.7281 acc_val: 0.8737\n",
            "Epoch: 0065 loss_train: 1.8632 acc_train: 0.8874 loss_val: 0.7080 acc_val: 0.8730\n",
            "Epoch: 0066 loss_train: 1.7159 acc_train: 0.8946 loss_val: 0.6579 acc_val: 0.8813\n",
            "Epoch: 0067 loss_train: 1.5744 acc_train: 0.9025 loss_val: 0.6288 acc_val: 0.8834\n",
            "Epoch: 0068 loss_train: 1.4423 acc_train: 0.9139 loss_val: 0.6676 acc_val: 0.8791\n",
            "Epoch: 0069 loss_train: 1.3955 acc_train: 0.9153 loss_val: 0.6464 acc_val: 0.8856\n",
            "Epoch: 0070 loss_train: 1.2712 acc_train: 0.9247 loss_val: 0.6862 acc_val: 0.8796\n",
            "Epoch: 0071 loss_train: 1.2358 acc_train: 0.9258 loss_val: 0.6581 acc_val: 0.8841\n",
            "Epoch: 0072 loss_train: 1.1651 acc_train: 0.9310 loss_val: 0.6646 acc_val: 0.8851\n",
            "Epoch: 0073 loss_train: 1.0569 acc_train: 0.9385 loss_val: 0.6908 acc_val: 0.8884\n",
            "Optimization Finished!\n",
            "Train cost: 85.3402s\n",
            "Loading 23th epoch\n",
            "Test set results: loss= 0.6893 accuracy= 0.8930\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --dataset citeseer --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 2  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack none --attperc 20 --TrainTest train --weight_decay=1e-05\n",
        "\n",
        "!python train.py --dataset citeseer --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 5  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack none --attperc 20 --TrainTest train --weight_decay=1e-05\n",
        "\n",
        "!python train.py --dataset citeseer --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 7  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack none --attperc 20 --TrainTest train --weight_decay=1e-05\n",
        "\n",
        "!python train.py --dataset citeseer --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 10  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack none --attperc 20 --TrainTest train --weight_decay=1e-05"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-940C-WRISf",
        "outputId": "a535d9e7-4c9d-486e-b662-5e332b7abf21"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading /root/.dgl/citeseer.zip from https://data.dgl.ai/dataset/citeseer.zip...\n",
            "Extracting file to /root/.dgl/citeseer\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/data/citation_graph.py:287: RuntimeWarning: divide by zero encountered in power\n",
            "  r_inv = np.power(rowsum, -1).flatten()\n",
            "Finished data loading and preprocessing.\n",
            "  NumNodes: 3327\n",
            "  NumEdges: 9228\n",
            "  NumFeats: 3703\n",
            "  NumClasses: 6\n",
            "  NumTrainingSamples: 120\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done saving data into cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=3718, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=6, bias=True)\n",
            ")\n",
            "total params: 4141832\n",
            "Epoch: 0001 loss_train: 1.8249 acc_train: 0.1437 loss_val: 1.8214 acc_val: 0.1426\n",
            "Epoch: 0002 loss_train: 1.8192 acc_train: 0.1422 loss_val: 1.8101 acc_val: 0.1577\n",
            "Epoch: 0003 loss_train: 1.8084 acc_train: 0.1652 loss_val: 1.7933 acc_val: 0.1757\n",
            "Epoch: 0004 loss_train: 1.7929 acc_train: 0.1813 loss_val: 1.7714 acc_val: 0.2057\n",
            "Epoch: 0005 loss_train: 1.7713 acc_train: 0.2018 loss_val: 1.7447 acc_val: 0.2568\n",
            "Epoch: 0006 loss_train: 1.7456 acc_train: 0.2464 loss_val: 1.7139 acc_val: 0.3258\n",
            "Epoch: 0007 loss_train: 1.7169 acc_train: 0.3055 loss_val: 1.6795 acc_val: 0.3964\n",
            "Epoch: 0008 loss_train: 1.6874 acc_train: 0.3916 loss_val: 1.6425 acc_val: 0.4505\n",
            "Epoch: 0009 loss_train: 1.6503 acc_train: 0.4482 loss_val: 1.6036 acc_val: 0.5135\n",
            "Epoch: 0010 loss_train: 1.6114 acc_train: 0.4947 loss_val: 1.5637 acc_val: 0.5315\n",
            "Epoch: 0011 loss_train: 1.5741 acc_train: 0.5403 loss_val: 1.5231 acc_val: 0.5586\n",
            "Epoch: 0012 loss_train: 1.5323 acc_train: 0.5648 loss_val: 1.4821 acc_val: 0.5736\n",
            "Epoch: 0013 loss_train: 1.4910 acc_train: 0.5929 loss_val: 1.4410 acc_val: 0.5916\n",
            "Epoch: 0014 loss_train: 1.4478 acc_train: 0.6044 loss_val: 1.3997 acc_val: 0.6051\n",
            "Epoch: 0015 loss_train: 1.4073 acc_train: 0.6194 loss_val: 1.3582 acc_val: 0.6246\n",
            "Epoch: 0016 loss_train: 1.3604 acc_train: 0.6460 loss_val: 1.3166 acc_val: 0.6336\n",
            "Epoch: 0017 loss_train: 1.3186 acc_train: 0.6455 loss_val: 1.2749 acc_val: 0.6381\n",
            "Epoch: 0018 loss_train: 1.2728 acc_train: 0.6580 loss_val: 1.2333 acc_val: 0.6456\n",
            "Epoch: 0019 loss_train: 1.2262 acc_train: 0.6685 loss_val: 1.1917 acc_val: 0.6607\n",
            "Epoch: 0020 loss_train: 1.1804 acc_train: 0.6875 loss_val: 1.1506 acc_val: 0.6787\n",
            "Epoch: 0021 loss_train: 1.1338 acc_train: 0.7021 loss_val: 1.1103 acc_val: 0.6967\n",
            "Epoch: 0022 loss_train: 1.0881 acc_train: 0.7151 loss_val: 1.0713 acc_val: 0.7042\n",
            "Epoch: 0023 loss_train: 1.0444 acc_train: 0.7211 loss_val: 1.0343 acc_val: 0.7102\n",
            "Epoch: 0024 loss_train: 0.9984 acc_train: 0.7391 loss_val: 0.9997 acc_val: 0.7192\n",
            "Epoch: 0025 loss_train: 0.9539 acc_train: 0.7426 loss_val: 0.9677 acc_val: 0.7252\n",
            "Epoch: 0026 loss_train: 0.9122 acc_train: 0.7476 loss_val: 0.9384 acc_val: 0.7267\n",
            "Epoch: 0027 loss_train: 0.8719 acc_train: 0.7476 loss_val: 0.9118 acc_val: 0.7312\n",
            "Epoch: 0028 loss_train: 0.8311 acc_train: 0.7566 loss_val: 0.8882 acc_val: 0.7312\n",
            "Epoch: 0029 loss_train: 0.7925 acc_train: 0.7656 loss_val: 0.8676 acc_val: 0.7297\n",
            "Epoch: 0030 loss_train: 0.7555 acc_train: 0.7742 loss_val: 0.8502 acc_val: 0.7312\n",
            "Epoch: 0031 loss_train: 0.7250 acc_train: 0.7817 loss_val: 0.8355 acc_val: 0.7312\n",
            "Epoch: 0032 loss_train: 0.6920 acc_train: 0.7887 loss_val: 0.8232 acc_val: 0.7297\n",
            "Epoch: 0033 loss_train: 0.6580 acc_train: 0.7972 loss_val: 0.8128 acc_val: 0.7267\n",
            "Epoch: 0034 loss_train: 0.6293 acc_train: 0.8097 loss_val: 0.8045 acc_val: 0.7237\n",
            "Epoch: 0035 loss_train: 0.6003 acc_train: 0.8192 loss_val: 0.7982 acc_val: 0.7237\n",
            "Epoch: 0036 loss_train: 0.5702 acc_train: 0.8237 loss_val: 0.7939 acc_val: 0.7237\n",
            "Epoch: 0037 loss_train: 0.5464 acc_train: 0.8282 loss_val: 0.7915 acc_val: 0.7237\n",
            "Epoch: 0038 loss_train: 0.5219 acc_train: 0.8322 loss_val: 0.7909 acc_val: 0.7312\n",
            "Epoch: 0039 loss_train: 0.4997 acc_train: 0.8388 loss_val: 0.7917 acc_val: 0.7387\n",
            "Epoch: 0040 loss_train: 0.4740 acc_train: 0.8488 loss_val: 0.7936 acc_val: 0.7387\n",
            "Epoch: 0041 loss_train: 0.4502 acc_train: 0.8543 loss_val: 0.7964 acc_val: 0.7417\n",
            "Epoch: 0042 loss_train: 0.4251 acc_train: 0.8658 loss_val: 0.8000 acc_val: 0.7462\n",
            "Epoch: 0043 loss_train: 0.4018 acc_train: 0.8728 loss_val: 0.8041 acc_val: 0.7462\n",
            "Epoch: 0044 loss_train: 0.3795 acc_train: 0.8758 loss_val: 0.8086 acc_val: 0.7508\n",
            "Epoch: 0045 loss_train: 0.3623 acc_train: 0.8873 loss_val: 0.8138 acc_val: 0.7523\n",
            "Epoch: 0046 loss_train: 0.3414 acc_train: 0.8953 loss_val: 0.8204 acc_val: 0.7538\n",
            "Epoch: 0047 loss_train: 0.3216 acc_train: 0.9039 loss_val: 0.8291 acc_val: 0.7598\n",
            "Epoch: 0048 loss_train: 0.3023 acc_train: 0.9079 loss_val: 0.8399 acc_val: 0.7538\n",
            "Epoch: 0049 loss_train: 0.2843 acc_train: 0.9194 loss_val: 0.8529 acc_val: 0.7508\n",
            "Epoch: 0050 loss_train: 0.2684 acc_train: 0.9249 loss_val: 0.8679 acc_val: 0.7462\n",
            "Epoch: 0051 loss_train: 0.2515 acc_train: 0.9279 loss_val: 0.8854 acc_val: 0.7432\n",
            "Epoch: 0052 loss_train: 0.2360 acc_train: 0.9254 loss_val: 0.9054 acc_val: 0.7417\n",
            "Epoch: 0053 loss_train: 0.2198 acc_train: 0.9354 loss_val: 0.9268 acc_val: 0.7402\n",
            "Epoch: 0054 loss_train: 0.2073 acc_train: 0.9419 loss_val: 0.9489 acc_val: 0.7372\n",
            "Epoch: 0055 loss_train: 0.1923 acc_train: 0.9469 loss_val: 0.9715 acc_val: 0.7357\n",
            "Epoch: 0056 loss_train: 0.1774 acc_train: 0.9544 loss_val: 0.9948 acc_val: 0.7312\n",
            "Epoch: 0057 loss_train: 0.1633 acc_train: 0.9564 loss_val: 1.0184 acc_val: 0.7372\n",
            "Epoch: 0058 loss_train: 0.1501 acc_train: 0.9619 loss_val: 1.0418 acc_val: 0.7402\n",
            "Epoch: 0059 loss_train: 0.1372 acc_train: 0.9685 loss_val: 1.0642 acc_val: 0.7387\n",
            "Epoch: 0060 loss_train: 0.1248 acc_train: 0.9725 loss_val: 1.0873 acc_val: 0.7372\n",
            "Epoch: 0061 loss_train: 0.1120 acc_train: 0.9765 loss_val: 1.1135 acc_val: 0.7387\n",
            "Epoch: 0062 loss_train: 0.1001 acc_train: 0.9805 loss_val: 1.1408 acc_val: 0.7372\n",
            "Epoch: 0063 loss_train: 0.0903 acc_train: 0.9835 loss_val: 1.1685 acc_val: 0.7357\n",
            "Epoch: 0064 loss_train: 0.0810 acc_train: 0.9870 loss_val: 1.1961 acc_val: 0.7372\n",
            "Epoch: 0065 loss_train: 0.0724 acc_train: 0.9890 loss_val: 1.2248 acc_val: 0.7387\n",
            "Epoch: 0066 loss_train: 0.0644 acc_train: 0.9890 loss_val: 1.2539 acc_val: 0.7372\n",
            "Epoch: 0067 loss_train: 0.0584 acc_train: 0.9895 loss_val: 1.2823 acc_val: 0.7342\n",
            "Epoch: 0068 loss_train: 0.0510 acc_train: 0.9910 loss_val: 1.3099 acc_val: 0.7372\n",
            "Epoch: 0069 loss_train: 0.0446 acc_train: 0.9920 loss_val: 1.3361 acc_val: 0.7357\n",
            "Epoch: 0070 loss_train: 0.0408 acc_train: 0.9930 loss_val: 1.3624 acc_val: 0.7357\n",
            "Epoch: 0071 loss_train: 0.0365 acc_train: 0.9940 loss_val: 1.3907 acc_val: 0.7342\n",
            "Epoch: 0072 loss_train: 0.0329 acc_train: 0.9945 loss_val: 1.4169 acc_val: 0.7327\n",
            "Epoch: 0073 loss_train: 0.0288 acc_train: 0.9950 loss_val: 1.4423 acc_val: 0.7342\n",
            "Epoch: 0074 loss_train: 0.0262 acc_train: 0.9965 loss_val: 1.4667 acc_val: 0.7342\n",
            "Epoch: 0075 loss_train: 0.0235 acc_train: 0.9970 loss_val: 1.4900 acc_val: 0.7327\n",
            "Epoch: 0076 loss_train: 0.0206 acc_train: 0.9970 loss_val: 1.5126 acc_val: 0.7327\n",
            "Epoch: 0077 loss_train: 0.0199 acc_train: 0.9975 loss_val: 1.5346 acc_val: 0.7312\n",
            "Epoch: 0078 loss_train: 0.0177 acc_train: 0.9980 loss_val: 1.5557 acc_val: 0.7327\n",
            "Epoch: 0079 loss_train: 0.0159 acc_train: 0.9975 loss_val: 1.5748 acc_val: 0.7312\n",
            "Epoch: 0080 loss_train: 0.0152 acc_train: 0.9975 loss_val: 1.5932 acc_val: 0.7312\n",
            "Epoch: 0081 loss_train: 0.0138 acc_train: 0.9975 loss_val: 1.6117 acc_val: 0.7297\n",
            "Epoch: 0082 loss_train: 0.0123 acc_train: 0.9980 loss_val: 1.6303 acc_val: 0.7312\n",
            "Epoch: 0083 loss_train: 0.0109 acc_train: 0.9980 loss_val: 1.6484 acc_val: 0.7282\n",
            "Epoch: 0084 loss_train: 0.0107 acc_train: 0.9975 loss_val: 1.6660 acc_val: 0.7297\n",
            "Epoch: 0085 loss_train: 0.0094 acc_train: 0.9985 loss_val: 1.6825 acc_val: 0.7282\n",
            "Epoch: 0086 loss_train: 0.0087 acc_train: 0.9980 loss_val: 1.6974 acc_val: 0.7282\n",
            "Epoch: 0087 loss_train: 0.0073 acc_train: 0.9985 loss_val: 1.7115 acc_val: 0.7297\n",
            "Epoch: 0088 loss_train: 0.0066 acc_train: 0.9990 loss_val: 1.7249 acc_val: 0.7282\n",
            "Epoch: 0089 loss_train: 0.0058 acc_train: 1.0000 loss_val: 1.7387 acc_val: 0.7297\n",
            "Epoch: 0090 loss_train: 0.0053 acc_train: 1.0000 loss_val: 1.7527 acc_val: 0.7297\n",
            "Epoch: 0091 loss_train: 0.0048 acc_train: 1.0000 loss_val: 1.7669 acc_val: 0.7297\n",
            "Epoch: 0092 loss_train: 0.0044 acc_train: 0.9995 loss_val: 1.7805 acc_val: 0.7297\n",
            "Epoch: 0093 loss_train: 0.0039 acc_train: 1.0000 loss_val: 1.7933 acc_val: 0.7297\n",
            "Epoch: 0094 loss_train: 0.0037 acc_train: 1.0000 loss_val: 1.8054 acc_val: 0.7297\n",
            "Epoch: 0095 loss_train: 0.0032 acc_train: 1.0000 loss_val: 1.8169 acc_val: 0.7297\n",
            "Epoch: 0096 loss_train: 0.0031 acc_train: 1.0000 loss_val: 1.8279 acc_val: 0.7297\n",
            "Epoch: 0097 loss_train: 0.0029 acc_train: 1.0000 loss_val: 1.8382 acc_val: 0.7297\n",
            "Optimization Finished!\n",
            "Train cost: 16.8937s\n",
            "Loading 47th epoch\n",
            "Test set results: loss= 0.6830 accuracy= 0.7877\n",
            "  NumNodes: 3327\n",
            "  NumEdges: 9228\n",
            "  NumFeats: 3703\n",
            "  NumClasses: 6\n",
            "  NumTrainingSamples: 120\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=3718, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=6, bias=True)\n",
            ")\n",
            "total params: 4141832\n",
            "Epoch: 0001 loss_train: 1.8160 acc_train: 0.1337 loss_val: 1.8150 acc_val: 0.1231\n",
            "Epoch: 0002 loss_train: 1.8117 acc_train: 0.1322 loss_val: 1.8035 acc_val: 0.1336\n",
            "Epoch: 0003 loss_train: 1.8011 acc_train: 0.1402 loss_val: 1.7865 acc_val: 0.1426\n",
            "Epoch: 0004 loss_train: 1.7845 acc_train: 0.1617 loss_val: 1.7644 acc_val: 0.1832\n",
            "Epoch: 0005 loss_train: 1.7631 acc_train: 0.2013 loss_val: 1.7376 acc_val: 0.2628\n",
            "Epoch: 0006 loss_train: 1.7378 acc_train: 0.2644 loss_val: 1.7066 acc_val: 0.3664\n",
            "Epoch: 0007 loss_train: 1.7095 acc_train: 0.3535 loss_val: 1.6723 acc_val: 0.4505\n",
            "Epoch: 0008 loss_train: 1.6756 acc_train: 0.4402 loss_val: 1.6353 acc_val: 0.4985\n",
            "Epoch: 0009 loss_train: 1.6403 acc_train: 0.4992 loss_val: 1.5966 acc_val: 0.5526\n",
            "Epoch: 0010 loss_train: 1.6017 acc_train: 0.5458 loss_val: 1.5571 acc_val: 0.5676\n",
            "Epoch: 0011 loss_train: 1.5628 acc_train: 0.5714 loss_val: 1.5172 acc_val: 0.5676\n",
            "Epoch: 0012 loss_train: 1.5217 acc_train: 0.5859 loss_val: 1.4771 acc_val: 0.5841\n",
            "Epoch: 0013 loss_train: 1.4837 acc_train: 0.6024 loss_val: 1.4368 acc_val: 0.5916\n",
            "Epoch: 0014 loss_train: 1.4411 acc_train: 0.6119 loss_val: 1.3962 acc_val: 0.6021\n",
            "Epoch: 0015 loss_train: 1.3992 acc_train: 0.6284 loss_val: 1.3558 acc_val: 0.6096\n",
            "Epoch: 0016 loss_train: 1.3564 acc_train: 0.6340 loss_val: 1.3157 acc_val: 0.6276\n",
            "Epoch: 0017 loss_train: 1.3127 acc_train: 0.6465 loss_val: 1.2760 acc_val: 0.6486\n",
            "Epoch: 0018 loss_train: 1.2719 acc_train: 0.6610 loss_val: 1.2370 acc_val: 0.6577\n",
            "Epoch: 0019 loss_train: 1.2290 acc_train: 0.6780 loss_val: 1.1989 acc_val: 0.6592\n",
            "Epoch: 0020 loss_train: 1.1875 acc_train: 0.6835 loss_val: 1.1618 acc_val: 0.6682\n",
            "Epoch: 0021 loss_train: 1.1446 acc_train: 0.6975 loss_val: 1.1261 acc_val: 0.6697\n",
            "Epoch: 0022 loss_train: 1.1065 acc_train: 0.6990 loss_val: 1.0921 acc_val: 0.6727\n",
            "Epoch: 0023 loss_train: 1.0657 acc_train: 0.7121 loss_val: 1.0601 acc_val: 0.6847\n",
            "Epoch: 0024 loss_train: 1.0285 acc_train: 0.7141 loss_val: 1.0302 acc_val: 0.6847\n",
            "Epoch: 0025 loss_train: 0.9924 acc_train: 0.7181 loss_val: 1.0026 acc_val: 0.6907\n",
            "Epoch: 0026 loss_train: 0.9561 acc_train: 0.7221 loss_val: 0.9773 acc_val: 0.6997\n",
            "Epoch: 0027 loss_train: 0.9214 acc_train: 0.7326 loss_val: 0.9539 acc_val: 0.7012\n",
            "Epoch: 0028 loss_train: 0.8885 acc_train: 0.7361 loss_val: 0.9325 acc_val: 0.7057\n",
            "Epoch: 0029 loss_train: 0.8584 acc_train: 0.7431 loss_val: 0.9131 acc_val: 0.7132\n",
            "Epoch: 0030 loss_train: 0.8271 acc_train: 0.7536 loss_val: 0.8957 acc_val: 0.7132\n",
            "Epoch: 0031 loss_train: 0.7984 acc_train: 0.7551 loss_val: 0.8800 acc_val: 0.7147\n",
            "Epoch: 0032 loss_train: 0.7682 acc_train: 0.7651 loss_val: 0.8661 acc_val: 0.7192\n",
            "Epoch: 0033 loss_train: 0.7403 acc_train: 0.7742 loss_val: 0.8538 acc_val: 0.7177\n",
            "Epoch: 0034 loss_train: 0.7152 acc_train: 0.7807 loss_val: 0.8432 acc_val: 0.7177\n",
            "Epoch: 0035 loss_train: 0.6884 acc_train: 0.7847 loss_val: 0.8342 acc_val: 0.7192\n",
            "Epoch: 0036 loss_train: 0.6631 acc_train: 0.7932 loss_val: 0.8271 acc_val: 0.7222\n",
            "Epoch: 0037 loss_train: 0.6387 acc_train: 0.7982 loss_val: 0.8217 acc_val: 0.7267\n",
            "Epoch: 0038 loss_train: 0.6150 acc_train: 0.8042 loss_val: 0.8180 acc_val: 0.7252\n",
            "Epoch: 0039 loss_train: 0.5945 acc_train: 0.8077 loss_val: 0.8161 acc_val: 0.7267\n",
            "Epoch: 0040 loss_train: 0.5721 acc_train: 0.8162 loss_val: 0.8159 acc_val: 0.7342\n",
            "Epoch: 0041 loss_train: 0.5482 acc_train: 0.8192 loss_val: 0.8174 acc_val: 0.7357\n",
            "Epoch: 0042 loss_train: 0.5262 acc_train: 0.8242 loss_val: 0.8208 acc_val: 0.7402\n",
            "Epoch: 0043 loss_train: 0.5055 acc_train: 0.8307 loss_val: 0.8260 acc_val: 0.7402\n",
            "Epoch: 0044 loss_train: 0.4866 acc_train: 0.8373 loss_val: 0.8322 acc_val: 0.7417\n",
            "Epoch: 0045 loss_train: 0.4651 acc_train: 0.8398 loss_val: 0.8394 acc_val: 0.7417\n",
            "Epoch: 0046 loss_train: 0.4493 acc_train: 0.8438 loss_val: 0.8479 acc_val: 0.7402\n",
            "Epoch: 0047 loss_train: 0.4313 acc_train: 0.8503 loss_val: 0.8580 acc_val: 0.7387\n",
            "Epoch: 0048 loss_train: 0.4151 acc_train: 0.8498 loss_val: 0.8695 acc_val: 0.7372\n",
            "Epoch: 0049 loss_train: 0.3986 acc_train: 0.8588 loss_val: 0.8814 acc_val: 0.7387\n",
            "Epoch: 0050 loss_train: 0.3795 acc_train: 0.8663 loss_val: 0.8928 acc_val: 0.7372\n",
            "Epoch: 0051 loss_train: 0.3618 acc_train: 0.8773 loss_val: 0.9051 acc_val: 0.7387\n",
            "Epoch: 0052 loss_train: 0.3460 acc_train: 0.8768 loss_val: 0.9196 acc_val: 0.7417\n",
            "Epoch: 0053 loss_train: 0.3297 acc_train: 0.8868 loss_val: 0.9352 acc_val: 0.7462\n",
            "Epoch: 0054 loss_train: 0.3097 acc_train: 0.8918 loss_val: 0.9509 acc_val: 0.7432\n",
            "Epoch: 0055 loss_train: 0.2957 acc_train: 0.8948 loss_val: 0.9676 acc_val: 0.7417\n",
            "Epoch: 0056 loss_train: 0.2788 acc_train: 0.8958 loss_val: 0.9865 acc_val: 0.7402\n",
            "Epoch: 0057 loss_train: 0.2623 acc_train: 0.9084 loss_val: 1.0086 acc_val: 0.7342\n",
            "Epoch: 0058 loss_train: 0.2470 acc_train: 0.9184 loss_val: 1.0314 acc_val: 0.7372\n",
            "Epoch: 0059 loss_train: 0.2307 acc_train: 0.9244 loss_val: 1.0557 acc_val: 0.7387\n",
            "Epoch: 0060 loss_train: 0.2169 acc_train: 0.9244 loss_val: 1.0818 acc_val: 0.7297\n",
            "Epoch: 0061 loss_train: 0.1990 acc_train: 0.9354 loss_val: 1.1075 acc_val: 0.7297\n",
            "Epoch: 0062 loss_train: 0.1862 acc_train: 0.9419 loss_val: 1.1342 acc_val: 0.7222\n",
            "Epoch: 0063 loss_train: 0.1721 acc_train: 0.9484 loss_val: 1.1625 acc_val: 0.7162\n",
            "Epoch: 0064 loss_train: 0.1577 acc_train: 0.9514 loss_val: 1.1897 acc_val: 0.7192\n",
            "Epoch: 0065 loss_train: 0.1443 acc_train: 0.9554 loss_val: 1.2133 acc_val: 0.7192\n",
            "Epoch: 0066 loss_train: 0.1314 acc_train: 0.9599 loss_val: 1.2422 acc_val: 0.7207\n",
            "Epoch: 0067 loss_train: 0.1196 acc_train: 0.9664 loss_val: 1.2716 acc_val: 0.7237\n",
            "Epoch: 0068 loss_train: 0.1080 acc_train: 0.9675 loss_val: 1.2992 acc_val: 0.7222\n",
            "Epoch: 0069 loss_train: 0.0970 acc_train: 0.9735 loss_val: 1.3257 acc_val: 0.7192\n",
            "Epoch: 0070 loss_train: 0.0892 acc_train: 0.9785 loss_val: 1.3585 acc_val: 0.7237\n",
            "Epoch: 0071 loss_train: 0.0816 acc_train: 0.9790 loss_val: 1.3900 acc_val: 0.7192\n",
            "Epoch: 0072 loss_train: 0.0730 acc_train: 0.9790 loss_val: 1.4157 acc_val: 0.7162\n",
            "Epoch: 0073 loss_train: 0.0662 acc_train: 0.9840 loss_val: 1.4437 acc_val: 0.7177\n",
            "Epoch: 0074 loss_train: 0.0590 acc_train: 0.9860 loss_val: 1.4726 acc_val: 0.7192\n",
            "Epoch: 0075 loss_train: 0.0539 acc_train: 0.9885 loss_val: 1.4985 acc_val: 0.7207\n",
            "Epoch: 0076 loss_train: 0.0499 acc_train: 0.9880 loss_val: 1.5233 acc_val: 0.7192\n",
            "Epoch: 0077 loss_train: 0.0457 acc_train: 0.9870 loss_val: 1.5496 acc_val: 0.7177\n",
            "Epoch: 0078 loss_train: 0.0412 acc_train: 0.9900 loss_val: 1.5766 acc_val: 0.7192\n",
            "Epoch: 0079 loss_train: 0.0387 acc_train: 0.9870 loss_val: 1.6022 acc_val: 0.7177\n",
            "Epoch: 0080 loss_train: 0.0348 acc_train: 0.9905 loss_val: 1.6276 acc_val: 0.7192\n",
            "Epoch: 0081 loss_train: 0.0313 acc_train: 0.9920 loss_val: 1.6537 acc_val: 0.7192\n",
            "Epoch: 0082 loss_train: 0.0293 acc_train: 0.9925 loss_val: 1.6810 acc_val: 0.7177\n",
            "Epoch: 0083 loss_train: 0.0266 acc_train: 0.9915 loss_val: 1.7071 acc_val: 0.7192\n",
            "Epoch: 0084 loss_train: 0.0262 acc_train: 0.9935 loss_val: 1.7255 acc_val: 0.7177\n",
            "Epoch: 0085 loss_train: 0.0241 acc_train: 0.9925 loss_val: 1.7460 acc_val: 0.7207\n",
            "Epoch: 0086 loss_train: 0.0220 acc_train: 0.9940 loss_val: 1.7704 acc_val: 0.7207\n",
            "Epoch: 0087 loss_train: 0.0210 acc_train: 0.9920 loss_val: 1.7904 acc_val: 0.7222\n",
            "Epoch: 0088 loss_train: 0.0193 acc_train: 0.9935 loss_val: 1.8073 acc_val: 0.7207\n",
            "Epoch: 0089 loss_train: 0.0186 acc_train: 0.9940 loss_val: 1.8269 acc_val: 0.7282\n",
            "Epoch: 0090 loss_train: 0.0167 acc_train: 0.9965 loss_val: 1.8458 acc_val: 0.7267\n",
            "Epoch: 0091 loss_train: 0.0163 acc_train: 0.9945 loss_val: 1.8642 acc_val: 0.7177\n",
            "Epoch: 0092 loss_train: 0.0152 acc_train: 0.9950 loss_val: 1.8794 acc_val: 0.7177\n",
            "Epoch: 0093 loss_train: 0.0132 acc_train: 0.9965 loss_val: 1.8923 acc_val: 0.7297\n",
            "Epoch: 0094 loss_train: 0.0132 acc_train: 0.9950 loss_val: 1.9015 acc_val: 0.7252\n",
            "Epoch: 0095 loss_train: 0.0114 acc_train: 0.9965 loss_val: 1.9126 acc_val: 0.7177\n",
            "Epoch: 0096 loss_train: 0.0115 acc_train: 0.9955 loss_val: 1.9286 acc_val: 0.7252\n",
            "Epoch: 0097 loss_train: 0.0091 acc_train: 0.9980 loss_val: 1.9427 acc_val: 0.7267\n",
            "Epoch: 0098 loss_train: 0.0083 acc_train: 0.9985 loss_val: 1.9557 acc_val: 0.7252\n",
            "Epoch: 0099 loss_train: 0.0077 acc_train: 0.9985 loss_val: 1.9687 acc_val: 0.7237\n",
            "Epoch: 0100 loss_train: 0.0075 acc_train: 0.9990 loss_val: 1.9806 acc_val: 0.7252\n",
            "Epoch: 0101 loss_train: 0.0068 acc_train: 0.9980 loss_val: 1.9913 acc_val: 0.7267\n",
            "Epoch: 0102 loss_train: 0.0064 acc_train: 0.9990 loss_val: 2.0013 acc_val: 0.7252\n",
            "Epoch: 0103 loss_train: 0.0061 acc_train: 0.9990 loss_val: 2.0115 acc_val: 0.7222\n",
            "Optimization Finished!\n",
            "Train cost: 33.1065s\n",
            "Loading 53th epoch\n",
            "Test set results: loss= 0.7487 accuracy= 0.7831\n",
            "  NumNodes: 3327\n",
            "  NumEdges: 9228\n",
            "  NumFeats: 3703\n",
            "  NumClasses: 6\n",
            "  NumTrainingSamples: 120\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=3718, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=6, bias=True)\n",
            ")\n",
            "total params: 4141832\n",
            "Epoch: 0001 loss_train: 1.8150 acc_train: 0.1282 loss_val: 1.8131 acc_val: 0.1186\n",
            "Epoch: 0002 loss_train: 1.8111 acc_train: 0.1277 loss_val: 1.8018 acc_val: 0.1291\n",
            "Epoch: 0003 loss_train: 1.8005 acc_train: 0.1352 loss_val: 1.7852 acc_val: 0.1336\n",
            "Epoch: 0004 loss_train: 1.7856 acc_train: 0.1487 loss_val: 1.7635 acc_val: 0.1607\n",
            "Epoch: 0005 loss_train: 1.7654 acc_train: 0.1763 loss_val: 1.7372 acc_val: 0.2492\n",
            "Epoch: 0006 loss_train: 1.7398 acc_train: 0.2649 loss_val: 1.7069 acc_val: 0.3709\n",
            "Epoch: 0007 loss_train: 1.7108 acc_train: 0.3540 loss_val: 1.6733 acc_val: 0.4595\n",
            "Epoch: 0008 loss_train: 1.6768 acc_train: 0.4557 loss_val: 1.6374 acc_val: 0.5030\n",
            "Epoch: 0009 loss_train: 1.6438 acc_train: 0.5103 loss_val: 1.6001 acc_val: 0.5435\n",
            "Epoch: 0010 loss_train: 1.6064 acc_train: 0.5443 loss_val: 1.5625 acc_val: 0.5616\n",
            "Epoch: 0011 loss_train: 1.5677 acc_train: 0.5729 loss_val: 1.5245 acc_val: 0.5721\n",
            "Epoch: 0012 loss_train: 1.5280 acc_train: 0.5899 loss_val: 1.4863 acc_val: 0.5736\n",
            "Epoch: 0013 loss_train: 1.4891 acc_train: 0.5949 loss_val: 1.4478 acc_val: 0.5856\n",
            "Epoch: 0014 loss_train: 1.4490 acc_train: 0.6119 loss_val: 1.4088 acc_val: 0.5901\n",
            "Epoch: 0015 loss_train: 1.4090 acc_train: 0.6124 loss_val: 1.3698 acc_val: 0.6006\n",
            "Epoch: 0016 loss_train: 1.3676 acc_train: 0.6284 loss_val: 1.3310 acc_val: 0.6111\n",
            "Epoch: 0017 loss_train: 1.3260 acc_train: 0.6450 loss_val: 1.2928 acc_val: 0.6291\n",
            "Epoch: 0018 loss_train: 1.2848 acc_train: 0.6505 loss_val: 1.2555 acc_val: 0.6366\n",
            "Epoch: 0019 loss_train: 1.2447 acc_train: 0.6635 loss_val: 1.2190 acc_val: 0.6426\n",
            "Epoch: 0020 loss_train: 1.2040 acc_train: 0.6725 loss_val: 1.1837 acc_val: 0.6486\n",
            "Epoch: 0021 loss_train: 1.1641 acc_train: 0.6825 loss_val: 1.1497 acc_val: 0.6562\n",
            "Epoch: 0022 loss_train: 1.1260 acc_train: 0.6890 loss_val: 1.1171 acc_val: 0.6637\n",
            "Epoch: 0023 loss_train: 1.0881 acc_train: 0.6965 loss_val: 1.0863 acc_val: 0.6697\n",
            "Epoch: 0024 loss_train: 1.0529 acc_train: 0.7051 loss_val: 1.0573 acc_val: 0.6742\n",
            "Epoch: 0025 loss_train: 1.0174 acc_train: 0.7106 loss_val: 1.0304 acc_val: 0.6802\n",
            "Epoch: 0026 loss_train: 0.9825 acc_train: 0.7171 loss_val: 1.0053 acc_val: 0.6832\n",
            "Epoch: 0027 loss_train: 0.9507 acc_train: 0.7231 loss_val: 0.9819 acc_val: 0.6892\n",
            "Epoch: 0028 loss_train: 0.9189 acc_train: 0.7296 loss_val: 0.9600 acc_val: 0.6892\n",
            "Epoch: 0029 loss_train: 0.8883 acc_train: 0.7371 loss_val: 0.9396 acc_val: 0.6952\n",
            "Epoch: 0030 loss_train: 0.8563 acc_train: 0.7436 loss_val: 0.9207 acc_val: 0.6967\n",
            "Epoch: 0031 loss_train: 0.8287 acc_train: 0.7481 loss_val: 0.9035 acc_val: 0.7027\n",
            "Epoch: 0032 loss_train: 0.8008 acc_train: 0.7606 loss_val: 0.8878 acc_val: 0.7102\n",
            "Epoch: 0033 loss_train: 0.7751 acc_train: 0.7651 loss_val: 0.8737 acc_val: 0.7102\n",
            "Epoch: 0034 loss_train: 0.7463 acc_train: 0.7707 loss_val: 0.8616 acc_val: 0.7087\n",
            "Epoch: 0035 loss_train: 0.7243 acc_train: 0.7772 loss_val: 0.8517 acc_val: 0.7117\n",
            "Epoch: 0036 loss_train: 0.7008 acc_train: 0.7817 loss_val: 0.8439 acc_val: 0.7132\n",
            "Epoch: 0037 loss_train: 0.6769 acc_train: 0.7862 loss_val: 0.8379 acc_val: 0.7192\n",
            "Epoch: 0038 loss_train: 0.6536 acc_train: 0.7922 loss_val: 0.8337 acc_val: 0.7192\n",
            "Epoch: 0039 loss_train: 0.6302 acc_train: 0.7967 loss_val: 0.8311 acc_val: 0.7237\n",
            "Epoch: 0040 loss_train: 0.6067 acc_train: 0.8032 loss_val: 0.8302 acc_val: 0.7297\n",
            "Epoch: 0041 loss_train: 0.5851 acc_train: 0.8092 loss_val: 0.8312 acc_val: 0.7297\n",
            "Epoch: 0042 loss_train: 0.5627 acc_train: 0.8187 loss_val: 0.8338 acc_val: 0.7372\n",
            "Epoch: 0043 loss_train: 0.5425 acc_train: 0.8202 loss_val: 0.8378 acc_val: 0.7402\n",
            "Epoch: 0044 loss_train: 0.5219 acc_train: 0.8232 loss_val: 0.8430 acc_val: 0.7402\n",
            "Epoch: 0045 loss_train: 0.5018 acc_train: 0.8302 loss_val: 0.8488 acc_val: 0.7432\n",
            "Epoch: 0046 loss_train: 0.4849 acc_train: 0.8353 loss_val: 0.8555 acc_val: 0.7357\n",
            "Epoch: 0047 loss_train: 0.4673 acc_train: 0.8398 loss_val: 0.8631 acc_val: 0.7327\n",
            "Epoch: 0048 loss_train: 0.4488 acc_train: 0.8463 loss_val: 0.8710 acc_val: 0.7327\n",
            "Epoch: 0049 loss_train: 0.4327 acc_train: 0.8448 loss_val: 0.8783 acc_val: 0.7312\n",
            "Epoch: 0050 loss_train: 0.4128 acc_train: 0.8528 loss_val: 0.8861 acc_val: 0.7312\n",
            "Epoch: 0051 loss_train: 0.3955 acc_train: 0.8568 loss_val: 0.8961 acc_val: 0.7372\n",
            "Epoch: 0052 loss_train: 0.3771 acc_train: 0.8698 loss_val: 0.9079 acc_val: 0.7402\n",
            "Epoch: 0053 loss_train: 0.3595 acc_train: 0.8688 loss_val: 0.9206 acc_val: 0.7402\n",
            "Epoch: 0054 loss_train: 0.3425 acc_train: 0.8788 loss_val: 0.9330 acc_val: 0.7372\n",
            "Epoch: 0055 loss_train: 0.3231 acc_train: 0.8828 loss_val: 0.9470 acc_val: 0.7357\n",
            "Epoch: 0056 loss_train: 0.3074 acc_train: 0.8863 loss_val: 0.9626 acc_val: 0.7387\n",
            "Epoch: 0057 loss_train: 0.2919 acc_train: 0.8913 loss_val: 0.9807 acc_val: 0.7342\n",
            "Epoch: 0058 loss_train: 0.2711 acc_train: 0.9104 loss_val: 1.0011 acc_val: 0.7342\n",
            "Epoch: 0059 loss_train: 0.2554 acc_train: 0.9109 loss_val: 1.0221 acc_val: 0.7327\n",
            "Epoch: 0060 loss_train: 0.2397 acc_train: 0.9169 loss_val: 1.0444 acc_val: 0.7282\n",
            "Epoch: 0061 loss_train: 0.2246 acc_train: 0.9234 loss_val: 1.0672 acc_val: 0.7297\n",
            "Epoch: 0062 loss_train: 0.2104 acc_train: 0.9294 loss_val: 1.0915 acc_val: 0.7312\n",
            "Epoch: 0063 loss_train: 0.1917 acc_train: 0.9364 loss_val: 1.1146 acc_val: 0.7342\n",
            "Epoch: 0064 loss_train: 0.1787 acc_train: 0.9409 loss_val: 1.1404 acc_val: 0.7252\n",
            "Epoch: 0065 loss_train: 0.1653 acc_train: 0.9464 loss_val: 1.1691 acc_val: 0.7222\n",
            "Epoch: 0066 loss_train: 0.1508 acc_train: 0.9499 loss_val: 1.1967 acc_val: 0.7207\n",
            "Epoch: 0067 loss_train: 0.1350 acc_train: 0.9574 loss_val: 1.2235 acc_val: 0.7237\n",
            "Epoch: 0068 loss_train: 0.1224 acc_train: 0.9634 loss_val: 1.2522 acc_val: 0.7207\n",
            "Epoch: 0069 loss_train: 0.1107 acc_train: 0.9690 loss_val: 1.2825 acc_val: 0.7222\n",
            "Epoch: 0070 loss_train: 0.0987 acc_train: 0.9750 loss_val: 1.3105 acc_val: 0.7192\n",
            "Epoch: 0071 loss_train: 0.0900 acc_train: 0.9765 loss_val: 1.3377 acc_val: 0.7222\n",
            "Epoch: 0072 loss_train: 0.0818 acc_train: 0.9775 loss_val: 1.3663 acc_val: 0.7207\n",
            "Epoch: 0073 loss_train: 0.0725 acc_train: 0.9800 loss_val: 1.3947 acc_val: 0.7222\n",
            "Epoch: 0074 loss_train: 0.0663 acc_train: 0.9815 loss_val: 1.4226 acc_val: 0.7267\n",
            "Epoch: 0075 loss_train: 0.0589 acc_train: 0.9850 loss_val: 1.4505 acc_val: 0.7252\n",
            "Epoch: 0076 loss_train: 0.0539 acc_train: 0.9880 loss_val: 1.4787 acc_val: 0.7237\n",
            "Epoch: 0077 loss_train: 0.0501 acc_train: 0.9880 loss_val: 1.5060 acc_val: 0.7222\n",
            "Epoch: 0078 loss_train: 0.0438 acc_train: 0.9915 loss_val: 1.5309 acc_val: 0.7252\n",
            "Epoch: 0079 loss_train: 0.0383 acc_train: 0.9915 loss_val: 1.5558 acc_val: 0.7237\n",
            "Epoch: 0080 loss_train: 0.0361 acc_train: 0.9905 loss_val: 1.5841 acc_val: 0.7237\n",
            "Epoch: 0081 loss_train: 0.0330 acc_train: 0.9925 loss_val: 1.6085 acc_val: 0.7252\n",
            "Epoch: 0082 loss_train: 0.0303 acc_train: 0.9930 loss_val: 1.6327 acc_val: 0.7267\n",
            "Epoch: 0083 loss_train: 0.0271 acc_train: 0.9940 loss_val: 1.6590 acc_val: 0.7252\n",
            "Epoch: 0084 loss_train: 0.0248 acc_train: 0.9935 loss_val: 1.6874 acc_val: 0.7237\n",
            "Epoch: 0085 loss_train: 0.0245 acc_train: 0.9955 loss_val: 1.7118 acc_val: 0.7267\n",
            "Epoch: 0086 loss_train: 0.0217 acc_train: 0.9945 loss_val: 1.7299 acc_val: 0.7237\n",
            "Epoch: 0087 loss_train: 0.0199 acc_train: 0.9945 loss_val: 1.7525 acc_val: 0.7222\n",
            "Epoch: 0088 loss_train: 0.0192 acc_train: 0.9925 loss_val: 1.7763 acc_val: 0.7267\n",
            "Epoch: 0089 loss_train: 0.0168 acc_train: 0.9950 loss_val: 1.7910 acc_val: 0.7282\n",
            "Epoch: 0090 loss_train: 0.0157 acc_train: 0.9965 loss_val: 1.8036 acc_val: 0.7207\n",
            "Epoch: 0091 loss_train: 0.0155 acc_train: 0.9940 loss_val: 1.8288 acc_val: 0.7312\n",
            "Epoch: 0092 loss_train: 0.0127 acc_train: 0.9980 loss_val: 1.8518 acc_val: 0.7297\n",
            "Epoch: 0093 loss_train: 0.0134 acc_train: 0.9970 loss_val: 1.8636 acc_val: 0.7252\n",
            "Epoch: 0094 loss_train: 0.0117 acc_train: 0.9970 loss_val: 1.8723 acc_val: 0.7207\n",
            "Epoch: 0095 loss_train: 0.0102 acc_train: 0.9980 loss_val: 1.8859 acc_val: 0.7267\n",
            "Optimization Finished!\n",
            "Train cost: 39.8077s\n",
            "Loading 45th epoch\n",
            "Test set results: loss= 0.6811 accuracy= 0.7937\n",
            "  NumNodes: 3327\n",
            "  NumEdges: 9228\n",
            "  NumFeats: 3703\n",
            "  NumClasses: 6\n",
            "  NumTrainingSamples: 120\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=3718, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=6, bias=True)\n",
            ")\n",
            "total params: 4141832\n",
            "Epoch: 0001 loss_train: 1.8157 acc_train: 0.1237 loss_val: 1.8102 acc_val: 0.1246\n",
            "Epoch: 0002 loss_train: 1.8102 acc_train: 0.1307 loss_val: 1.7991 acc_val: 0.1276\n",
            "Epoch: 0003 loss_train: 1.7990 acc_train: 0.1407 loss_val: 1.7827 acc_val: 0.1366\n",
            "Epoch: 0004 loss_train: 1.7841 acc_train: 0.1487 loss_val: 1.7615 acc_val: 0.1532\n",
            "Epoch: 0005 loss_train: 1.7641 acc_train: 0.1813 loss_val: 1.7357 acc_val: 0.2372\n",
            "Epoch: 0006 loss_train: 1.7397 acc_train: 0.2559 loss_val: 1.7061 acc_val: 0.3724\n",
            "Epoch: 0007 loss_train: 1.7120 acc_train: 0.3716 loss_val: 1.6736 acc_val: 0.4565\n",
            "Epoch: 0008 loss_train: 1.6790 acc_train: 0.4627 loss_val: 1.6392 acc_val: 0.4895\n",
            "Epoch: 0009 loss_train: 1.6450 acc_train: 0.5103 loss_val: 1.6037 acc_val: 0.5060\n",
            "Epoch: 0010 loss_train: 1.6096 acc_train: 0.5353 loss_val: 1.5686 acc_val: 0.5375\n",
            "Epoch: 0011 loss_train: 1.5751 acc_train: 0.5533 loss_val: 1.5337 acc_val: 0.5526\n",
            "Epoch: 0012 loss_train: 1.5373 acc_train: 0.5669 loss_val: 1.4984 acc_val: 0.5556\n",
            "Epoch: 0013 loss_train: 1.4996 acc_train: 0.5829 loss_val: 1.4628 acc_val: 0.5676\n",
            "Epoch: 0014 loss_train: 1.4614 acc_train: 0.5944 loss_val: 1.4269 acc_val: 0.5721\n",
            "Epoch: 0015 loss_train: 1.4239 acc_train: 0.6104 loss_val: 1.3907 acc_val: 0.5856\n",
            "Epoch: 0016 loss_train: 1.3850 acc_train: 0.6169 loss_val: 1.3543 acc_val: 0.6006\n",
            "Epoch: 0017 loss_train: 1.3456 acc_train: 0.6269 loss_val: 1.3180 acc_val: 0.6126\n",
            "Epoch: 0018 loss_train: 1.3045 acc_train: 0.6455 loss_val: 1.2824 acc_val: 0.6201\n",
            "Epoch: 0019 loss_train: 1.2668 acc_train: 0.6540 loss_val: 1.2477 acc_val: 0.6291\n",
            "Epoch: 0020 loss_train: 1.2281 acc_train: 0.6645 loss_val: 1.2139 acc_val: 0.6351\n",
            "Epoch: 0021 loss_train: 1.1909 acc_train: 0.6740 loss_val: 1.1810 acc_val: 0.6471\n",
            "Epoch: 0022 loss_train: 1.1527 acc_train: 0.6795 loss_val: 1.1491 acc_val: 0.6532\n",
            "Epoch: 0023 loss_train: 1.1171 acc_train: 0.6880 loss_val: 1.1185 acc_val: 0.6592\n",
            "Epoch: 0024 loss_train: 1.0799 acc_train: 0.6975 loss_val: 1.0894 acc_val: 0.6652\n",
            "Epoch: 0025 loss_train: 1.0460 acc_train: 0.7051 loss_val: 1.0621 acc_val: 0.6667\n",
            "Epoch: 0026 loss_train: 1.0127 acc_train: 0.7061 loss_val: 1.0366 acc_val: 0.6697\n",
            "Epoch: 0027 loss_train: 0.9805 acc_train: 0.7126 loss_val: 1.0126 acc_val: 0.6712\n",
            "Epoch: 0028 loss_train: 0.9474 acc_train: 0.7201 loss_val: 0.9899 acc_val: 0.6847\n",
            "Epoch: 0029 loss_train: 0.9184 acc_train: 0.7266 loss_val: 0.9685 acc_val: 0.6922\n",
            "Epoch: 0030 loss_train: 0.8890 acc_train: 0.7356 loss_val: 0.9483 acc_val: 0.6907\n",
            "Epoch: 0031 loss_train: 0.8591 acc_train: 0.7411 loss_val: 0.9294 acc_val: 0.6877\n",
            "Epoch: 0032 loss_train: 0.8326 acc_train: 0.7526 loss_val: 0.9117 acc_val: 0.6892\n",
            "Epoch: 0033 loss_train: 0.8070 acc_train: 0.7576 loss_val: 0.8956 acc_val: 0.6967\n",
            "Epoch: 0034 loss_train: 0.7794 acc_train: 0.7651 loss_val: 0.8816 acc_val: 0.6952\n",
            "Epoch: 0035 loss_train: 0.7554 acc_train: 0.7712 loss_val: 0.8703 acc_val: 0.6952\n",
            "Epoch: 0036 loss_train: 0.7309 acc_train: 0.7772 loss_val: 0.8616 acc_val: 0.7027\n",
            "Epoch: 0037 loss_train: 0.7091 acc_train: 0.7802 loss_val: 0.8553 acc_val: 0.7057\n",
            "Epoch: 0038 loss_train: 0.6841 acc_train: 0.7867 loss_val: 0.8509 acc_val: 0.7162\n",
            "Epoch: 0039 loss_train: 0.6634 acc_train: 0.7867 loss_val: 0.8479 acc_val: 0.7177\n",
            "Epoch: 0040 loss_train: 0.6406 acc_train: 0.7937 loss_val: 0.8461 acc_val: 0.7177\n",
            "Epoch: 0041 loss_train: 0.6175 acc_train: 0.7997 loss_val: 0.8454 acc_val: 0.7192\n",
            "Epoch: 0042 loss_train: 0.5956 acc_train: 0.8072 loss_val: 0.8460 acc_val: 0.7312\n",
            "Epoch: 0043 loss_train: 0.5740 acc_train: 0.8162 loss_val: 0.8473 acc_val: 0.7357\n",
            "Epoch: 0044 loss_train: 0.5546 acc_train: 0.8192 loss_val: 0.8493 acc_val: 0.7402\n",
            "Epoch: 0045 loss_train: 0.5353 acc_train: 0.8237 loss_val: 0.8525 acc_val: 0.7372\n",
            "Epoch: 0046 loss_train: 0.5147 acc_train: 0.8292 loss_val: 0.8571 acc_val: 0.7357\n",
            "Epoch: 0047 loss_train: 0.4974 acc_train: 0.8358 loss_val: 0.8629 acc_val: 0.7327\n",
            "Epoch: 0048 loss_train: 0.4789 acc_train: 0.8383 loss_val: 0.8687 acc_val: 0.7282\n",
            "Epoch: 0049 loss_train: 0.4621 acc_train: 0.8433 loss_val: 0.8732 acc_val: 0.7282\n",
            "Epoch: 0050 loss_train: 0.4455 acc_train: 0.8488 loss_val: 0.8794 acc_val: 0.7312\n",
            "Epoch: 0051 loss_train: 0.4237 acc_train: 0.8558 loss_val: 0.8873 acc_val: 0.7312\n",
            "Epoch: 0052 loss_train: 0.4065 acc_train: 0.8578 loss_val: 0.8956 acc_val: 0.7312\n",
            "Epoch: 0053 loss_train: 0.3863 acc_train: 0.8658 loss_val: 0.9043 acc_val: 0.7357\n",
            "Epoch: 0054 loss_train: 0.3707 acc_train: 0.8733 loss_val: 0.9145 acc_val: 0.7387\n",
            "Epoch: 0055 loss_train: 0.3500 acc_train: 0.8818 loss_val: 0.9269 acc_val: 0.7372\n",
            "Epoch: 0056 loss_train: 0.3327 acc_train: 0.8838 loss_val: 0.9411 acc_val: 0.7342\n",
            "Epoch: 0057 loss_train: 0.3128 acc_train: 0.8923 loss_val: 0.9567 acc_val: 0.7327\n",
            "Epoch: 0058 loss_train: 0.2954 acc_train: 0.8983 loss_val: 0.9747 acc_val: 0.7372\n",
            "Epoch: 0059 loss_train: 0.2760 acc_train: 0.9089 loss_val: 0.9943 acc_val: 0.7387\n",
            "Epoch: 0060 loss_train: 0.2600 acc_train: 0.9149 loss_val: 1.0136 acc_val: 0.7372\n",
            "Epoch: 0061 loss_train: 0.2433 acc_train: 0.9204 loss_val: 1.0332 acc_val: 0.7342\n",
            "Epoch: 0062 loss_train: 0.2263 acc_train: 0.9249 loss_val: 1.0539 acc_val: 0.7282\n",
            "Epoch: 0063 loss_train: 0.2109 acc_train: 0.9319 loss_val: 1.0780 acc_val: 0.7267\n",
            "Epoch: 0064 loss_train: 0.1953 acc_train: 0.9369 loss_val: 1.1013 acc_val: 0.7252\n",
            "Epoch: 0065 loss_train: 0.1781 acc_train: 0.9409 loss_val: 1.1233 acc_val: 0.7312\n",
            "Epoch: 0066 loss_train: 0.1657 acc_train: 0.9439 loss_val: 1.1512 acc_val: 0.7282\n",
            "Epoch: 0067 loss_train: 0.1479 acc_train: 0.9549 loss_val: 1.1810 acc_val: 0.7297\n",
            "Epoch: 0068 loss_train: 0.1345 acc_train: 0.9564 loss_val: 1.2048 acc_val: 0.7267\n",
            "Epoch: 0069 loss_train: 0.1197 acc_train: 0.9664 loss_val: 1.2331 acc_val: 0.7282\n",
            "Epoch: 0070 loss_train: 0.1086 acc_train: 0.9695 loss_val: 1.2655 acc_val: 0.7267\n",
            "Epoch: 0071 loss_train: 0.0961 acc_train: 0.9750 loss_val: 1.2915 acc_val: 0.7222\n",
            "Epoch: 0072 loss_train: 0.0844 acc_train: 0.9800 loss_val: 1.3165 acc_val: 0.7237\n",
            "Epoch: 0073 loss_train: 0.0755 acc_train: 0.9860 loss_val: 1.3457 acc_val: 0.7282\n",
            "Epoch: 0074 loss_train: 0.0670 acc_train: 0.9855 loss_val: 1.3761 acc_val: 0.7297\n",
            "Epoch: 0075 loss_train: 0.0586 acc_train: 0.9885 loss_val: 1.4018 acc_val: 0.7252\n",
            "Epoch: 0076 loss_train: 0.0513 acc_train: 0.9920 loss_val: 1.4250 acc_val: 0.7252\n",
            "Epoch: 0077 loss_train: 0.0464 acc_train: 0.9905 loss_val: 1.4522 acc_val: 0.7222\n",
            "Epoch: 0078 loss_train: 0.0411 acc_train: 0.9935 loss_val: 1.4813 acc_val: 0.7237\n",
            "Epoch: 0079 loss_train: 0.0357 acc_train: 0.9935 loss_val: 1.5064 acc_val: 0.7267\n",
            "Epoch: 0080 loss_train: 0.0315 acc_train: 0.9945 loss_val: 1.5278 acc_val: 0.7327\n",
            "Epoch: 0081 loss_train: 0.0290 acc_train: 0.9945 loss_val: 1.5490 acc_val: 0.7297\n",
            "Epoch: 0082 loss_train: 0.0247 acc_train: 0.9950 loss_val: 1.5739 acc_val: 0.7312\n",
            "Epoch: 0083 loss_train: 0.0216 acc_train: 0.9965 loss_val: 1.6009 acc_val: 0.7327\n",
            "Epoch: 0084 loss_train: 0.0200 acc_train: 0.9970 loss_val: 1.6269 acc_val: 0.7312\n",
            "Epoch: 0085 loss_train: 0.0178 acc_train: 0.9970 loss_val: 1.6506 acc_val: 0.7297\n",
            "Epoch: 0086 loss_train: 0.0159 acc_train: 0.9960 loss_val: 1.6721 acc_val: 0.7297\n",
            "Epoch: 0087 loss_train: 0.0149 acc_train: 0.9960 loss_val: 1.6935 acc_val: 0.7282\n",
            "Epoch: 0088 loss_train: 0.0130 acc_train: 0.9975 loss_val: 1.7151 acc_val: 0.7297\n",
            "Epoch: 0089 loss_train: 0.0118 acc_train: 0.9975 loss_val: 1.7383 acc_val: 0.7282\n",
            "Epoch: 0090 loss_train: 0.0105 acc_train: 0.9975 loss_val: 1.7605 acc_val: 0.7297\n",
            "Epoch: 0091 loss_train: 0.0092 acc_train: 0.9985 loss_val: 1.7806 acc_val: 0.7282\n",
            "Epoch: 0092 loss_train: 0.0088 acc_train: 0.9985 loss_val: 1.7981 acc_val: 0.7267\n",
            "Epoch: 0093 loss_train: 0.0084 acc_train: 0.9985 loss_val: 1.8161 acc_val: 0.7282\n",
            "Epoch: 0094 loss_train: 0.0068 acc_train: 0.9990 loss_val: 1.8331 acc_val: 0.7282\n",
            "Optimization Finished!\n",
            "Train cost: 52.5349s\n",
            "Loading 44th epoch\n",
            "Test set results: loss= 0.6916 accuracy= 0.7877\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --dataset cora --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 2  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack none --attperc 20 --TrainTest train --weight_decay=1e-05\n",
        "\n",
        "!python train.py --dataset cora --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 5  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack none --attperc 20 --TrainTest train --weight_decay=1e-05\n",
        "\n",
        "!python train.py --dataset cora --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 7  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack none --attperc 20 --TrainTest train --weight_decay=1e-05\n",
        "\n",
        "!python train.py --dataset cora --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 10  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack none --attperc 20 --TrainTest train --weight_decay=1e-05"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frLvygWPTcoz",
        "outputId": "f84b8864-5eca-4c92-ec03-b1f7d5169673"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading /root/.dgl/cora_v2.zip from https://data.dgl.ai/dataset/cora_v2.zip...\n",
            "Extracting file to /root/.dgl/cora_v2\n",
            "Finished data loading and preprocessing.\n",
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done saving data into cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=1448, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=7, bias=True)\n",
            ")\n",
            "total params: 2979849\n",
            "Epoch: 0001 loss_train: 1.9633 acc_train: 0.1335 loss_val: 1.9600 acc_val: 0.1328\n",
            "Epoch: 0002 loss_train: 1.9575 acc_train: 0.1415 loss_val: 1.9474 acc_val: 0.1458\n",
            "Epoch: 0003 loss_train: 1.9488 acc_train: 0.1550 loss_val: 1.9287 acc_val: 0.1790\n",
            "Epoch: 0004 loss_train: 1.9295 acc_train: 0.1759 loss_val: 1.9041 acc_val: 0.2362\n",
            "Epoch: 0005 loss_train: 1.9079 acc_train: 0.2343 loss_val: 1.8739 acc_val: 0.3137\n",
            "Epoch: 0006 loss_train: 1.8790 acc_train: 0.2786 loss_val: 1.8386 acc_val: 0.3856\n",
            "Epoch: 0007 loss_train: 1.8471 acc_train: 0.3622 loss_val: 1.7987 acc_val: 0.4428\n",
            "Epoch: 0008 loss_train: 1.8135 acc_train: 0.4065 loss_val: 1.7549 acc_val: 0.5018\n",
            "Epoch: 0009 loss_train: 1.7723 acc_train: 0.4723 loss_val: 1.7079 acc_val: 0.5277\n",
            "Epoch: 0010 loss_train: 1.7293 acc_train: 0.5062 loss_val: 1.6581 acc_val: 0.5406\n",
            "Epoch: 0011 loss_train: 1.6806 acc_train: 0.5338 loss_val: 1.6063 acc_val: 0.5590\n",
            "Epoch: 0012 loss_train: 1.6307 acc_train: 0.5437 loss_val: 1.5530 acc_val: 0.5664\n",
            "Epoch: 0013 loss_train: 1.5816 acc_train: 0.5578 loss_val: 1.4984 acc_val: 0.5775\n",
            "Epoch: 0014 loss_train: 1.5274 acc_train: 0.5689 loss_val: 1.4429 acc_val: 0.5941\n",
            "Epoch: 0015 loss_train: 1.4712 acc_train: 0.5800 loss_val: 1.3862 acc_val: 0.5978\n",
            "Epoch: 0016 loss_train: 1.4144 acc_train: 0.5959 loss_val: 1.3285 acc_val: 0.6292\n",
            "Epoch: 0017 loss_train: 1.3578 acc_train: 0.6218 loss_val: 1.2694 acc_val: 0.6568\n",
            "Epoch: 0018 loss_train: 1.2988 acc_train: 0.6433 loss_val: 1.2095 acc_val: 0.6790\n",
            "Epoch: 0019 loss_train: 1.2369 acc_train: 0.6753 loss_val: 1.1492 acc_val: 0.6863\n",
            "Epoch: 0020 loss_train: 1.1760 acc_train: 0.7023 loss_val: 1.0897 acc_val: 0.7011\n",
            "Epoch: 0021 loss_train: 1.1152 acc_train: 0.7294 loss_val: 1.0322 acc_val: 0.7066\n",
            "Epoch: 0022 loss_train: 1.0517 acc_train: 0.7528 loss_val: 0.9778 acc_val: 0.7214\n",
            "Epoch: 0023 loss_train: 0.9908 acc_train: 0.7737 loss_val: 0.9269 acc_val: 0.7362\n",
            "Epoch: 0024 loss_train: 0.9363 acc_train: 0.7934 loss_val: 0.8796 acc_val: 0.7509\n",
            "Epoch: 0025 loss_train: 0.8838 acc_train: 0.8155 loss_val: 0.8351 acc_val: 0.7731\n",
            "Epoch: 0026 loss_train: 0.8311 acc_train: 0.8198 loss_val: 0.7925 acc_val: 0.7823\n",
            "Epoch: 0027 loss_train: 0.7840 acc_train: 0.8333 loss_val: 0.7516 acc_val: 0.7841\n",
            "Epoch: 0028 loss_train: 0.7390 acc_train: 0.8413 loss_val: 0.7127 acc_val: 0.7878\n",
            "Epoch: 0029 loss_train: 0.6952 acc_train: 0.8518 loss_val: 0.6763 acc_val: 0.7970\n",
            "Epoch: 0030 loss_train: 0.6560 acc_train: 0.8518 loss_val: 0.6430 acc_val: 0.8044\n",
            "Epoch: 0031 loss_train: 0.6185 acc_train: 0.8512 loss_val: 0.6125 acc_val: 0.8100\n",
            "Epoch: 0032 loss_train: 0.5829 acc_train: 0.8592 loss_val: 0.5843 acc_val: 0.8155\n",
            "Epoch: 0033 loss_train: 0.5485 acc_train: 0.8647 loss_val: 0.5582 acc_val: 0.8173\n",
            "Epoch: 0034 loss_train: 0.5204 acc_train: 0.8702 loss_val: 0.5347 acc_val: 0.8247\n",
            "Epoch: 0035 loss_train: 0.4881 acc_train: 0.8788 loss_val: 0.5139 acc_val: 0.8358\n",
            "Epoch: 0036 loss_train: 0.4631 acc_train: 0.8813 loss_val: 0.4954 acc_val: 0.8376\n",
            "Epoch: 0037 loss_train: 0.4361 acc_train: 0.8838 loss_val: 0.4780 acc_val: 0.8413\n",
            "Epoch: 0038 loss_train: 0.4123 acc_train: 0.8905 loss_val: 0.4601 acc_val: 0.8469\n",
            "Epoch: 0039 loss_train: 0.3919 acc_train: 0.8911 loss_val: 0.4418 acc_val: 0.8469\n",
            "Epoch: 0040 loss_train: 0.3707 acc_train: 0.8973 loss_val: 0.4238 acc_val: 0.8598\n",
            "Epoch: 0041 loss_train: 0.3487 acc_train: 0.9059 loss_val: 0.4076 acc_val: 0.8690\n",
            "Epoch: 0042 loss_train: 0.3289 acc_train: 0.9084 loss_val: 0.3943 acc_val: 0.8727\n",
            "Epoch: 0043 loss_train: 0.3122 acc_train: 0.9114 loss_val: 0.3839 acc_val: 0.8690\n",
            "Epoch: 0044 loss_train: 0.2942 acc_train: 0.9170 loss_val: 0.3764 acc_val: 0.8745\n",
            "Epoch: 0045 loss_train: 0.2799 acc_train: 0.9200 loss_val: 0.3715 acc_val: 0.8801\n",
            "Epoch: 0046 loss_train: 0.2623 acc_train: 0.9244 loss_val: 0.3681 acc_val: 0.8801\n",
            "Epoch: 0047 loss_train: 0.2485 acc_train: 0.9287 loss_val: 0.3644 acc_val: 0.8782\n",
            "Epoch: 0048 loss_train: 0.2353 acc_train: 0.9323 loss_val: 0.3602 acc_val: 0.8782\n",
            "Epoch: 0049 loss_train: 0.2208 acc_train: 0.9391 loss_val: 0.3561 acc_val: 0.8838\n",
            "Epoch: 0050 loss_train: 0.2090 acc_train: 0.9428 loss_val: 0.3532 acc_val: 0.8838\n",
            "Epoch: 0051 loss_train: 0.1969 acc_train: 0.9477 loss_val: 0.3522 acc_val: 0.8782\n",
            "Epoch: 0052 loss_train: 0.1840 acc_train: 0.9514 loss_val: 0.3537 acc_val: 0.8893\n",
            "Epoch: 0053 loss_train: 0.1704 acc_train: 0.9545 loss_val: 0.3569 acc_val: 0.8875\n",
            "Epoch: 0054 loss_train: 0.1607 acc_train: 0.9563 loss_val: 0.3597 acc_val: 0.8875\n",
            "Epoch: 0055 loss_train: 0.1495 acc_train: 0.9643 loss_val: 0.3621 acc_val: 0.8838\n",
            "Epoch: 0056 loss_train: 0.1376 acc_train: 0.9699 loss_val: 0.3632 acc_val: 0.8801\n",
            "Epoch: 0057 loss_train: 0.1258 acc_train: 0.9705 loss_val: 0.3654 acc_val: 0.8801\n",
            "Epoch: 0058 loss_train: 0.1185 acc_train: 0.9742 loss_val: 0.3703 acc_val: 0.8764\n",
            "Epoch: 0059 loss_train: 0.1074 acc_train: 0.9760 loss_val: 0.3787 acc_val: 0.8727\n",
            "Epoch: 0060 loss_train: 0.0988 acc_train: 0.9772 loss_val: 0.3902 acc_val: 0.8745\n",
            "Epoch: 0061 loss_train: 0.0894 acc_train: 0.9840 loss_val: 0.4016 acc_val: 0.8745\n",
            "Epoch: 0062 loss_train: 0.0824 acc_train: 0.9865 loss_val: 0.4107 acc_val: 0.8764\n",
            "Epoch: 0063 loss_train: 0.0733 acc_train: 0.9889 loss_val: 0.4164 acc_val: 0.8801\n",
            "Epoch: 0064 loss_train: 0.0684 acc_train: 0.9895 loss_val: 0.4219 acc_val: 0.8782\n",
            "Epoch: 0065 loss_train: 0.0598 acc_train: 0.9908 loss_val: 0.4293 acc_val: 0.8745\n",
            "Epoch: 0066 loss_train: 0.0540 acc_train: 0.9932 loss_val: 0.4397 acc_val: 0.8782\n",
            "Epoch: 0067 loss_train: 0.0494 acc_train: 0.9926 loss_val: 0.4515 acc_val: 0.8801\n",
            "Epoch: 0068 loss_train: 0.0441 acc_train: 0.9957 loss_val: 0.4623 acc_val: 0.8801\n",
            "Epoch: 0069 loss_train: 0.0410 acc_train: 0.9957 loss_val: 0.4695 acc_val: 0.8764\n",
            "Epoch: 0070 loss_train: 0.0359 acc_train: 0.9969 loss_val: 0.4736 acc_val: 0.8764\n",
            "Epoch: 0071 loss_train: 0.0325 acc_train: 0.9975 loss_val: 0.4780 acc_val: 0.8745\n",
            "Epoch: 0072 loss_train: 0.0290 acc_train: 0.9963 loss_val: 0.4826 acc_val: 0.8727\n",
            "Epoch: 0073 loss_train: 0.0259 acc_train: 0.9982 loss_val: 0.4886 acc_val: 0.8727\n",
            "Epoch: 0074 loss_train: 0.0228 acc_train: 0.9994 loss_val: 0.4973 acc_val: 0.8745\n",
            "Epoch: 0075 loss_train: 0.0211 acc_train: 0.9988 loss_val: 0.5076 acc_val: 0.8727\n",
            "Epoch: 0076 loss_train: 0.0180 acc_train: 0.9994 loss_val: 0.5182 acc_val: 0.8690\n",
            "Epoch: 0077 loss_train: 0.0167 acc_train: 0.9994 loss_val: 0.5271 acc_val: 0.8672\n",
            "Epoch: 0078 loss_train: 0.0143 acc_train: 1.0000 loss_val: 0.5343 acc_val: 0.8672\n",
            "Epoch: 0079 loss_train: 0.0132 acc_train: 1.0000 loss_val: 0.5399 acc_val: 0.8690\n",
            "Epoch: 0080 loss_train: 0.0121 acc_train: 0.9994 loss_val: 0.5445 acc_val: 0.8708\n",
            "Epoch: 0081 loss_train: 0.0109 acc_train: 1.0000 loss_val: 0.5490 acc_val: 0.8727\n",
            "Epoch: 0082 loss_train: 0.0100 acc_train: 1.0000 loss_val: 0.5532 acc_val: 0.8727\n",
            "Epoch: 0083 loss_train: 0.0093 acc_train: 1.0000 loss_val: 0.5581 acc_val: 0.8727\n",
            "Epoch: 0084 loss_train: 0.0084 acc_train: 1.0000 loss_val: 0.5638 acc_val: 0.8708\n",
            "Epoch: 0085 loss_train: 0.0080 acc_train: 1.0000 loss_val: 0.5700 acc_val: 0.8727\n",
            "Epoch: 0086 loss_train: 0.0071 acc_train: 1.0000 loss_val: 0.5761 acc_val: 0.8690\n",
            "Epoch: 0087 loss_train: 0.0065 acc_train: 1.0000 loss_val: 0.5821 acc_val: 0.8690\n",
            "Epoch: 0088 loss_train: 0.0062 acc_train: 1.0000 loss_val: 0.5875 acc_val: 0.8690\n",
            "Epoch: 0089 loss_train: 0.0059 acc_train: 1.0000 loss_val: 0.5915 acc_val: 0.8690\n",
            "Epoch: 0090 loss_train: 0.0052 acc_train: 1.0000 loss_val: 0.5946 acc_val: 0.8690\n",
            "Epoch: 0091 loss_train: 0.0050 acc_train: 1.0000 loss_val: 0.5970 acc_val: 0.8672\n",
            "Epoch: 0092 loss_train: 0.0047 acc_train: 1.0000 loss_val: 0.5990 acc_val: 0.8672\n",
            "Epoch: 0093 loss_train: 0.0045 acc_train: 1.0000 loss_val: 0.6005 acc_val: 0.8690\n",
            "Epoch: 0094 loss_train: 0.0043 acc_train: 1.0000 loss_val: 0.6030 acc_val: 0.8690\n",
            "Epoch: 0095 loss_train: 0.0039 acc_train: 1.0000 loss_val: 0.6059 acc_val: 0.8708\n",
            "Epoch: 0096 loss_train: 0.0037 acc_train: 1.0000 loss_val: 0.6097 acc_val: 0.8708\n",
            "Epoch: 0097 loss_train: 0.0035 acc_train: 1.0000 loss_val: 0.6142 acc_val: 0.8690\n",
            "Epoch: 0098 loss_train: 0.0033 acc_train: 1.0000 loss_val: 0.6190 acc_val: 0.8672\n",
            "Epoch: 0099 loss_train: 0.0032 acc_train: 1.0000 loss_val: 0.6238 acc_val: 0.8672\n",
            "Epoch: 0100 loss_train: 0.0030 acc_train: 1.0000 loss_val: 0.6284 acc_val: 0.8672\n",
            "Epoch: 0101 loss_train: 0.0028 acc_train: 1.0000 loss_val: 0.6327 acc_val: 0.8653\n",
            "Epoch: 0102 loss_train: 0.0027 acc_train: 1.0000 loss_val: 0.6365 acc_val: 0.8653\n",
            "Optimization Finished!\n",
            "Train cost: 7.5519s\n",
            "Loading 52th epoch\n",
            "Test set results: loss= 0.3043 accuracy= 0.9111\n",
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=1448, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=7, bias=True)\n",
            ")\n",
            "total params: 2979849\n",
            "Epoch: 0001 loss_train: 1.9727 acc_train: 0.1156 loss_val: 1.9686 acc_val: 0.1310\n",
            "Epoch: 0002 loss_train: 1.9669 acc_train: 0.1279 loss_val: 1.9561 acc_val: 0.1384\n",
            "Epoch: 0003 loss_train: 1.9547 acc_train: 0.1396 loss_val: 1.9375 acc_val: 0.1587\n",
            "Epoch: 0004 loss_train: 1.9373 acc_train: 0.1525 loss_val: 1.9130 acc_val: 0.2122\n",
            "Epoch: 0005 loss_train: 1.9160 acc_train: 0.1925 loss_val: 1.8830 acc_val: 0.3118\n",
            "Epoch: 0006 loss_train: 1.8886 acc_train: 0.2768 loss_val: 1.8478 acc_val: 0.4428\n",
            "Epoch: 0007 loss_train: 1.8561 acc_train: 0.3930 loss_val: 1.8078 acc_val: 0.4815\n",
            "Epoch: 0008 loss_train: 1.8199 acc_train: 0.4736 loss_val: 1.7640 acc_val: 0.5055\n",
            "Epoch: 0009 loss_train: 1.7805 acc_train: 0.4889 loss_val: 1.7170 acc_val: 0.5055\n",
            "Epoch: 0010 loss_train: 1.7355 acc_train: 0.5092 loss_val: 1.6678 acc_val: 0.5166\n",
            "Epoch: 0011 loss_train: 1.6880 acc_train: 0.5203 loss_val: 1.6169 acc_val: 0.5277\n",
            "Epoch: 0012 loss_train: 1.6405 acc_train: 0.5295 loss_val: 1.5647 acc_val: 0.5461\n",
            "Epoch: 0013 loss_train: 1.5898 acc_train: 0.5387 loss_val: 1.5112 acc_val: 0.5517\n",
            "Epoch: 0014 loss_train: 1.5383 acc_train: 0.5541 loss_val: 1.4567 acc_val: 0.5627\n",
            "Epoch: 0015 loss_train: 1.4837 acc_train: 0.5658 loss_val: 1.4014 acc_val: 0.5701\n",
            "Epoch: 0016 loss_train: 1.4274 acc_train: 0.5824 loss_val: 1.3452 acc_val: 0.5849\n",
            "Epoch: 0017 loss_train: 1.3709 acc_train: 0.5996 loss_val: 1.2880 acc_val: 0.6070\n",
            "Epoch: 0018 loss_train: 1.3128 acc_train: 0.6193 loss_val: 1.2305 acc_val: 0.6494\n",
            "Epoch: 0019 loss_train: 1.2546 acc_train: 0.6513 loss_val: 1.1740 acc_val: 0.6734\n",
            "Epoch: 0020 loss_train: 1.1946 acc_train: 0.6845 loss_val: 1.1196 acc_val: 0.6882\n",
            "Epoch: 0021 loss_train: 1.1373 acc_train: 0.7103 loss_val: 1.0686 acc_val: 0.6993\n",
            "Epoch: 0022 loss_train: 1.0810 acc_train: 0.7269 loss_val: 1.0214 acc_val: 0.6974\n",
            "Epoch: 0023 loss_train: 1.0270 acc_train: 0.7386 loss_val: 0.9779 acc_val: 0.7103\n",
            "Epoch: 0024 loss_train: 0.9762 acc_train: 0.7491 loss_val: 0.9369 acc_val: 0.7214\n",
            "Epoch: 0025 loss_train: 0.9308 acc_train: 0.7558 loss_val: 0.8978 acc_val: 0.7288\n",
            "Epoch: 0026 loss_train: 0.8845 acc_train: 0.7706 loss_val: 0.8600 acc_val: 0.7306\n",
            "Epoch: 0027 loss_train: 0.8421 acc_train: 0.7811 loss_val: 0.8239 acc_val: 0.7472\n",
            "Epoch: 0028 loss_train: 0.8022 acc_train: 0.8007 loss_val: 0.7899 acc_val: 0.7509\n",
            "Epoch: 0029 loss_train: 0.7630 acc_train: 0.8093 loss_val: 0.7580 acc_val: 0.7638\n",
            "Epoch: 0030 loss_train: 0.7271 acc_train: 0.8167 loss_val: 0.7286 acc_val: 0.7768\n",
            "Epoch: 0031 loss_train: 0.6939 acc_train: 0.8266 loss_val: 0.7010 acc_val: 0.7804\n",
            "Epoch: 0032 loss_train: 0.6632 acc_train: 0.8284 loss_val: 0.6747 acc_val: 0.7823\n",
            "Epoch: 0033 loss_train: 0.6327 acc_train: 0.8346 loss_val: 0.6496 acc_val: 0.7823\n",
            "Epoch: 0034 loss_train: 0.6044 acc_train: 0.8432 loss_val: 0.6263 acc_val: 0.7878\n",
            "Epoch: 0035 loss_train: 0.5759 acc_train: 0.8481 loss_val: 0.6049 acc_val: 0.8044\n",
            "Epoch: 0036 loss_train: 0.5496 acc_train: 0.8524 loss_val: 0.5844 acc_val: 0.8081\n",
            "Epoch: 0037 loss_train: 0.5268 acc_train: 0.8598 loss_val: 0.5635 acc_val: 0.8118\n",
            "Epoch: 0038 loss_train: 0.5036 acc_train: 0.8598 loss_val: 0.5414 acc_val: 0.8192\n",
            "Epoch: 0039 loss_train: 0.4787 acc_train: 0.8647 loss_val: 0.5187 acc_val: 0.8266\n",
            "Epoch: 0040 loss_train: 0.4569 acc_train: 0.8684 loss_val: 0.4966 acc_val: 0.8266\n",
            "Epoch: 0041 loss_train: 0.4365 acc_train: 0.8739 loss_val: 0.4760 acc_val: 0.8303\n",
            "Epoch: 0042 loss_train: 0.4171 acc_train: 0.8733 loss_val: 0.4578 acc_val: 0.8413\n",
            "Epoch: 0043 loss_train: 0.3960 acc_train: 0.8819 loss_val: 0.4426 acc_val: 0.8561\n",
            "Epoch: 0044 loss_train: 0.3774 acc_train: 0.8887 loss_val: 0.4296 acc_val: 0.8635\n",
            "Epoch: 0045 loss_train: 0.3595 acc_train: 0.8918 loss_val: 0.4170 acc_val: 0.8690\n",
            "Epoch: 0046 loss_train: 0.3405 acc_train: 0.8998 loss_val: 0.4041 acc_val: 0.8727\n",
            "Epoch: 0047 loss_train: 0.3229 acc_train: 0.9090 loss_val: 0.3923 acc_val: 0.8745\n",
            "Epoch: 0048 loss_train: 0.3065 acc_train: 0.9157 loss_val: 0.3836 acc_val: 0.8838\n",
            "Epoch: 0049 loss_train: 0.2879 acc_train: 0.9194 loss_val: 0.3782 acc_val: 0.8875\n",
            "Epoch: 0050 loss_train: 0.2725 acc_train: 0.9274 loss_val: 0.3758 acc_val: 0.8893\n",
            "Epoch: 0051 loss_train: 0.2565 acc_train: 0.9293 loss_val: 0.3755 acc_val: 0.8838\n",
            "Epoch: 0052 loss_train: 0.2410 acc_train: 0.9342 loss_val: 0.3742 acc_val: 0.8819\n",
            "Epoch: 0053 loss_train: 0.2288 acc_train: 0.9403 loss_val: 0.3732 acc_val: 0.8838\n",
            "Epoch: 0054 loss_train: 0.2129 acc_train: 0.9434 loss_val: 0.3749 acc_val: 0.8819\n",
            "Epoch: 0055 loss_train: 0.1947 acc_train: 0.9508 loss_val: 0.3805 acc_val: 0.8782\n",
            "Epoch: 0056 loss_train: 0.1813 acc_train: 0.9533 loss_val: 0.3863 acc_val: 0.8801\n",
            "Epoch: 0057 loss_train: 0.1677 acc_train: 0.9576 loss_val: 0.3874 acc_val: 0.8782\n",
            "Epoch: 0058 loss_train: 0.1540 acc_train: 0.9619 loss_val: 0.3872 acc_val: 0.8782\n",
            "Epoch: 0059 loss_train: 0.1432 acc_train: 0.9656 loss_val: 0.3902 acc_val: 0.8782\n",
            "Epoch: 0060 loss_train: 0.1315 acc_train: 0.9686 loss_val: 0.3960 acc_val: 0.8801\n",
            "Epoch: 0061 loss_train: 0.1190 acc_train: 0.9729 loss_val: 0.4023 acc_val: 0.8782\n",
            "Epoch: 0062 loss_train: 0.1102 acc_train: 0.9760 loss_val: 0.4080 acc_val: 0.8764\n",
            "Epoch: 0063 loss_train: 0.1000 acc_train: 0.9822 loss_val: 0.4138 acc_val: 0.8764\n",
            "Epoch: 0064 loss_train: 0.0907 acc_train: 0.9859 loss_val: 0.4196 acc_val: 0.8764\n",
            "Epoch: 0065 loss_train: 0.0822 acc_train: 0.9865 loss_val: 0.4272 acc_val: 0.8801\n",
            "Epoch: 0066 loss_train: 0.0764 acc_train: 0.9852 loss_val: 0.4338 acc_val: 0.8745\n",
            "Epoch: 0067 loss_train: 0.0687 acc_train: 0.9871 loss_val: 0.4419 acc_val: 0.8727\n",
            "Epoch: 0068 loss_train: 0.0618 acc_train: 0.9895 loss_val: 0.4517 acc_val: 0.8708\n",
            "Epoch: 0069 loss_train: 0.0564 acc_train: 0.9914 loss_val: 0.4612 acc_val: 0.8727\n",
            "Epoch: 0070 loss_train: 0.0516 acc_train: 0.9908 loss_val: 0.4702 acc_val: 0.8708\n",
            "Epoch: 0071 loss_train: 0.0470 acc_train: 0.9920 loss_val: 0.4785 acc_val: 0.8690\n",
            "Epoch: 0072 loss_train: 0.0425 acc_train: 0.9926 loss_val: 0.4848 acc_val: 0.8690\n",
            "Epoch: 0073 loss_train: 0.0387 acc_train: 0.9945 loss_val: 0.4902 acc_val: 0.8690\n",
            "Epoch: 0074 loss_train: 0.0355 acc_train: 0.9938 loss_val: 0.4948 acc_val: 0.8672\n",
            "Epoch: 0075 loss_train: 0.0312 acc_train: 0.9945 loss_val: 0.4993 acc_val: 0.8672\n",
            "Epoch: 0076 loss_train: 0.0288 acc_train: 0.9963 loss_val: 0.5046 acc_val: 0.8690\n",
            "Epoch: 0077 loss_train: 0.0262 acc_train: 0.9951 loss_val: 0.5107 acc_val: 0.8690\n",
            "Epoch: 0078 loss_train: 0.0229 acc_train: 0.9988 loss_val: 0.5159 acc_val: 0.8708\n",
            "Epoch: 0079 loss_train: 0.0213 acc_train: 0.9982 loss_val: 0.5216 acc_val: 0.8708\n",
            "Epoch: 0080 loss_train: 0.0198 acc_train: 0.9975 loss_val: 0.5286 acc_val: 0.8708\n",
            "Epoch: 0081 loss_train: 0.0173 acc_train: 0.9982 loss_val: 0.5363 acc_val: 0.8690\n",
            "Epoch: 0082 loss_train: 0.0152 acc_train: 0.9994 loss_val: 0.5455 acc_val: 0.8708\n",
            "Epoch: 0083 loss_train: 0.0139 acc_train: 0.9988 loss_val: 0.5557 acc_val: 0.8727\n",
            "Epoch: 0084 loss_train: 0.0124 acc_train: 0.9994 loss_val: 0.5655 acc_val: 0.8708\n",
            "Epoch: 0085 loss_train: 0.0114 acc_train: 0.9994 loss_val: 0.5740 acc_val: 0.8690\n",
            "Epoch: 0086 loss_train: 0.0101 acc_train: 1.0000 loss_val: 0.5804 acc_val: 0.8690\n",
            "Epoch: 0087 loss_train: 0.0090 acc_train: 1.0000 loss_val: 0.5846 acc_val: 0.8708\n",
            "Epoch: 0088 loss_train: 0.0084 acc_train: 1.0000 loss_val: 0.5882 acc_val: 0.8690\n",
            "Epoch: 0089 loss_train: 0.0082 acc_train: 0.9994 loss_val: 0.5925 acc_val: 0.8708\n",
            "Epoch: 0090 loss_train: 0.0073 acc_train: 1.0000 loss_val: 0.5984 acc_val: 0.8708\n",
            "Epoch: 0091 loss_train: 0.0067 acc_train: 1.0000 loss_val: 0.6056 acc_val: 0.8690\n",
            "Epoch: 0092 loss_train: 0.0060 acc_train: 1.0000 loss_val: 0.6134 acc_val: 0.8690\n",
            "Epoch: 0093 loss_train: 0.0053 acc_train: 1.0000 loss_val: 0.6213 acc_val: 0.8708\n",
            "Epoch: 0094 loss_train: 0.0052 acc_train: 1.0000 loss_val: 0.6275 acc_val: 0.8690\n",
            "Epoch: 0095 loss_train: 0.0047 acc_train: 1.0000 loss_val: 0.6331 acc_val: 0.8672\n",
            "Epoch: 0096 loss_train: 0.0043 acc_train: 1.0000 loss_val: 0.6373 acc_val: 0.8672\n",
            "Epoch: 0097 loss_train: 0.0041 acc_train: 1.0000 loss_val: 0.6404 acc_val: 0.8690\n",
            "Epoch: 0098 loss_train: 0.0040 acc_train: 1.0000 loss_val: 0.6428 acc_val: 0.8672\n",
            "Epoch: 0099 loss_train: 0.0035 acc_train: 1.0000 loss_val: 0.6451 acc_val: 0.8672\n",
            "Epoch: 0100 loss_train: 0.0035 acc_train: 1.0000 loss_val: 0.6471 acc_val: 0.8635\n",
            "Epoch: 0101 loss_train: 0.0032 acc_train: 1.0000 loss_val: 0.6496 acc_val: 0.8635\n",
            "Epoch: 0102 loss_train: 0.0031 acc_train: 1.0000 loss_val: 0.6526 acc_val: 0.8635\n",
            "Epoch: 0103 loss_train: 0.0029 acc_train: 1.0000 loss_val: 0.6555 acc_val: 0.8635\n",
            "Optimization Finished!\n",
            "Train cost: 13.6603s\n",
            "Loading 50th epoch\n",
            "Test set results: loss= 0.3476 accuracy= 0.9019\n",
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=1448, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=7, bias=True)\n",
            ")\n",
            "total params: 2979849\n",
            "Epoch: 0001 loss_train: 1.9728 acc_train: 0.1181 loss_val: 1.9696 acc_val: 0.1292\n",
            "Epoch: 0002 loss_train: 1.9676 acc_train: 0.1224 loss_val: 1.9576 acc_val: 0.1328\n",
            "Epoch: 0003 loss_train: 1.9555 acc_train: 0.1335 loss_val: 1.9396 acc_val: 0.1494\n",
            "Epoch: 0004 loss_train: 1.9401 acc_train: 0.1408 loss_val: 1.9160 acc_val: 0.1900\n",
            "Epoch: 0005 loss_train: 1.9207 acc_train: 0.1691 loss_val: 1.8871 acc_val: 0.2841\n",
            "Epoch: 0006 loss_train: 1.8928 acc_train: 0.2737 loss_val: 1.8534 acc_val: 0.4391\n",
            "Epoch: 0007 loss_train: 1.8617 acc_train: 0.3868 loss_val: 1.8154 acc_val: 0.4889\n",
            "Epoch: 0008 loss_train: 1.8266 acc_train: 0.4643 loss_val: 1.7737 acc_val: 0.4926\n",
            "Epoch: 0009 loss_train: 1.7898 acc_train: 0.4803 loss_val: 1.7294 acc_val: 0.5000\n",
            "Epoch: 0010 loss_train: 1.7486 acc_train: 0.4852 loss_val: 1.6833 acc_val: 0.4982\n",
            "Epoch: 0011 loss_train: 1.7045 acc_train: 0.4951 loss_val: 1.6359 acc_val: 0.5074\n",
            "Epoch: 0012 loss_train: 1.6584 acc_train: 0.5025 loss_val: 1.5869 acc_val: 0.5258\n",
            "Epoch: 0013 loss_train: 1.6110 acc_train: 0.5123 loss_val: 1.5362 acc_val: 0.5369\n",
            "Epoch: 0014 loss_train: 1.5621 acc_train: 0.5277 loss_val: 1.4841 acc_val: 0.5443\n",
            "Epoch: 0015 loss_train: 1.5086 acc_train: 0.5400 loss_val: 1.4308 acc_val: 0.5517\n",
            "Epoch: 0016 loss_train: 1.4569 acc_train: 0.5590 loss_val: 1.3765 acc_val: 0.5664\n",
            "Epoch: 0017 loss_train: 1.4009 acc_train: 0.5720 loss_val: 1.3209 acc_val: 0.5886\n",
            "Epoch: 0018 loss_train: 1.3440 acc_train: 0.5867 loss_val: 1.2648 acc_val: 0.6162\n",
            "Epoch: 0019 loss_train: 1.2865 acc_train: 0.6187 loss_val: 1.2096 acc_val: 0.6476\n",
            "Epoch: 0020 loss_train: 1.2281 acc_train: 0.6667 loss_val: 1.1571 acc_val: 0.6716\n",
            "Epoch: 0021 loss_train: 1.1716 acc_train: 0.6980 loss_val: 1.1082 acc_val: 0.6845\n",
            "Epoch: 0022 loss_train: 1.1186 acc_train: 0.7140 loss_val: 1.0629 acc_val: 0.6863\n",
            "Epoch: 0023 loss_train: 1.0670 acc_train: 0.7294 loss_val: 1.0204 acc_val: 0.6937\n",
            "Epoch: 0024 loss_train: 1.0172 acc_train: 0.7374 loss_val: 0.9796 acc_val: 0.7011\n",
            "Epoch: 0025 loss_train: 0.9698 acc_train: 0.7448 loss_val: 0.9404 acc_val: 0.7140\n",
            "Epoch: 0026 loss_train: 0.9250 acc_train: 0.7528 loss_val: 0.9028 acc_val: 0.7140\n",
            "Epoch: 0027 loss_train: 0.8804 acc_train: 0.7571 loss_val: 0.8673 acc_val: 0.7196\n",
            "Epoch: 0028 loss_train: 0.8415 acc_train: 0.7681 loss_val: 0.8345 acc_val: 0.7306\n",
            "Epoch: 0029 loss_train: 0.8027 acc_train: 0.7860 loss_val: 0.8041 acc_val: 0.7509\n",
            "Epoch: 0030 loss_train: 0.7666 acc_train: 0.8038 loss_val: 0.7756 acc_val: 0.7509\n",
            "Epoch: 0031 loss_train: 0.7349 acc_train: 0.8081 loss_val: 0.7482 acc_val: 0.7601\n",
            "Epoch: 0032 loss_train: 0.7022 acc_train: 0.8167 loss_val: 0.7220 acc_val: 0.7712\n",
            "Epoch: 0033 loss_train: 0.6729 acc_train: 0.8223 loss_val: 0.6974 acc_val: 0.7768\n",
            "Epoch: 0034 loss_train: 0.6426 acc_train: 0.8290 loss_val: 0.6744 acc_val: 0.7804\n",
            "Epoch: 0035 loss_train: 0.6143 acc_train: 0.8339 loss_val: 0.6515 acc_val: 0.7878\n",
            "Epoch: 0036 loss_train: 0.5888 acc_train: 0.8358 loss_val: 0.6277 acc_val: 0.7934\n",
            "Epoch: 0037 loss_train: 0.5615 acc_train: 0.8444 loss_val: 0.6027 acc_val: 0.8007\n",
            "Epoch: 0038 loss_train: 0.5367 acc_train: 0.8481 loss_val: 0.5775 acc_val: 0.8063\n",
            "Epoch: 0039 loss_train: 0.5121 acc_train: 0.8567 loss_val: 0.5526 acc_val: 0.8155\n",
            "Epoch: 0040 loss_train: 0.4885 acc_train: 0.8604 loss_val: 0.5287 acc_val: 0.8155\n",
            "Epoch: 0041 loss_train: 0.4642 acc_train: 0.8641 loss_val: 0.5062 acc_val: 0.8192\n",
            "Epoch: 0042 loss_train: 0.4419 acc_train: 0.8727 loss_val: 0.4858 acc_val: 0.8284\n",
            "Epoch: 0043 loss_train: 0.4212 acc_train: 0.8807 loss_val: 0.4661 acc_val: 0.8358\n",
            "Epoch: 0044 loss_train: 0.4022 acc_train: 0.8838 loss_val: 0.4460 acc_val: 0.8598\n",
            "Epoch: 0045 loss_train: 0.3811 acc_train: 0.8856 loss_val: 0.4268 acc_val: 0.8708\n",
            "Epoch: 0046 loss_train: 0.3592 acc_train: 0.8985 loss_val: 0.4109 acc_val: 0.8727\n",
            "Epoch: 0047 loss_train: 0.3401 acc_train: 0.8985 loss_val: 0.3993 acc_val: 0.8782\n",
            "Epoch: 0048 loss_train: 0.3222 acc_train: 0.9114 loss_val: 0.3918 acc_val: 0.8782\n",
            "Epoch: 0049 loss_train: 0.3052 acc_train: 0.9170 loss_val: 0.3864 acc_val: 0.8801\n",
            "Epoch: 0050 loss_train: 0.2876 acc_train: 0.9237 loss_val: 0.3802 acc_val: 0.8838\n",
            "Epoch: 0051 loss_train: 0.2712 acc_train: 0.9293 loss_val: 0.3756 acc_val: 0.8764\n",
            "Epoch: 0052 loss_train: 0.2548 acc_train: 0.9367 loss_val: 0.3746 acc_val: 0.8801\n",
            "Epoch: 0053 loss_train: 0.2357 acc_train: 0.9397 loss_val: 0.3766 acc_val: 0.8764\n",
            "Epoch: 0054 loss_train: 0.2187 acc_train: 0.9459 loss_val: 0.3791 acc_val: 0.8819\n",
            "Epoch: 0055 loss_train: 0.2021 acc_train: 0.9496 loss_val: 0.3802 acc_val: 0.8819\n",
            "Epoch: 0056 loss_train: 0.1876 acc_train: 0.9557 loss_val: 0.3797 acc_val: 0.8801\n",
            "Epoch: 0057 loss_train: 0.1714 acc_train: 0.9588 loss_val: 0.3814 acc_val: 0.8801\n",
            "Epoch: 0058 loss_train: 0.1570 acc_train: 0.9606 loss_val: 0.3851 acc_val: 0.8764\n",
            "Epoch: 0059 loss_train: 0.1457 acc_train: 0.9625 loss_val: 0.3911 acc_val: 0.8727\n",
            "Epoch: 0060 loss_train: 0.1333 acc_train: 0.9662 loss_val: 0.3964 acc_val: 0.8764\n",
            "Epoch: 0061 loss_train: 0.1227 acc_train: 0.9717 loss_val: 0.4023 acc_val: 0.8782\n",
            "Epoch: 0062 loss_train: 0.1113 acc_train: 0.9760 loss_val: 0.4099 acc_val: 0.8764\n",
            "Epoch: 0063 loss_train: 0.1029 acc_train: 0.9785 loss_val: 0.4203 acc_val: 0.8727\n",
            "Epoch: 0064 loss_train: 0.0919 acc_train: 0.9834 loss_val: 0.4294 acc_val: 0.8745\n",
            "Epoch: 0065 loss_train: 0.0847 acc_train: 0.9852 loss_val: 0.4345 acc_val: 0.8745\n",
            "Epoch: 0066 loss_train: 0.0759 acc_train: 0.9871 loss_val: 0.4387 acc_val: 0.8764\n",
            "Epoch: 0067 loss_train: 0.0705 acc_train: 0.9865 loss_val: 0.4446 acc_val: 0.8764\n",
            "Epoch: 0068 loss_train: 0.0631 acc_train: 0.9883 loss_val: 0.4549 acc_val: 0.8764\n",
            "Epoch: 0069 loss_train: 0.0579 acc_train: 0.9895 loss_val: 0.4656 acc_val: 0.8727\n",
            "Epoch: 0070 loss_train: 0.0517 acc_train: 0.9914 loss_val: 0.4741 acc_val: 0.8727\n",
            "Epoch: 0071 loss_train: 0.0478 acc_train: 0.9926 loss_val: 0.4778 acc_val: 0.8672\n",
            "Epoch: 0072 loss_train: 0.0429 acc_train: 0.9926 loss_val: 0.4800 acc_val: 0.8635\n",
            "Epoch: 0073 loss_train: 0.0389 acc_train: 0.9938 loss_val: 0.4849 acc_val: 0.8672\n",
            "Epoch: 0074 loss_train: 0.0352 acc_train: 0.9938 loss_val: 0.4926 acc_val: 0.8653\n",
            "Epoch: 0075 loss_train: 0.0314 acc_train: 0.9938 loss_val: 0.5027 acc_val: 0.8672\n",
            "Epoch: 0076 loss_train: 0.0287 acc_train: 0.9957 loss_val: 0.5130 acc_val: 0.8672\n",
            "Epoch: 0077 loss_train: 0.0266 acc_train: 0.9969 loss_val: 0.5193 acc_val: 0.8653\n",
            "Epoch: 0078 loss_train: 0.0238 acc_train: 0.9982 loss_val: 0.5221 acc_val: 0.8672\n",
            "Epoch: 0079 loss_train: 0.0216 acc_train: 0.9988 loss_val: 0.5228 acc_val: 0.8672\n",
            "Epoch: 0080 loss_train: 0.0199 acc_train: 0.9982 loss_val: 0.5258 acc_val: 0.8690\n",
            "Epoch: 0081 loss_train: 0.0179 acc_train: 0.9975 loss_val: 0.5325 acc_val: 0.8672\n",
            "Epoch: 0082 loss_train: 0.0161 acc_train: 0.9988 loss_val: 0.5423 acc_val: 0.8672\n",
            "Epoch: 0083 loss_train: 0.0142 acc_train: 0.9994 loss_val: 0.5545 acc_val: 0.8635\n",
            "Epoch: 0084 loss_train: 0.0127 acc_train: 0.9994 loss_val: 0.5660 acc_val: 0.8635\n",
            "Epoch: 0085 loss_train: 0.0118 acc_train: 0.9994 loss_val: 0.5745 acc_val: 0.8653\n",
            "Epoch: 0086 loss_train: 0.0101 acc_train: 0.9994 loss_val: 0.5798 acc_val: 0.8672\n",
            "Epoch: 0087 loss_train: 0.0100 acc_train: 0.9994 loss_val: 0.5826 acc_val: 0.8653\n",
            "Epoch: 0088 loss_train: 0.0086 acc_train: 0.9994 loss_val: 0.5862 acc_val: 0.8653\n",
            "Epoch: 0089 loss_train: 0.0080 acc_train: 0.9994 loss_val: 0.5914 acc_val: 0.8653\n",
            "Epoch: 0090 loss_train: 0.0071 acc_train: 0.9994 loss_val: 0.5987 acc_val: 0.8672\n",
            "Epoch: 0091 loss_train: 0.0066 acc_train: 0.9994 loss_val: 0.6060 acc_val: 0.8690\n",
            "Epoch: 0092 loss_train: 0.0060 acc_train: 1.0000 loss_val: 0.6127 acc_val: 0.8708\n",
            "Epoch: 0093 loss_train: 0.0054 acc_train: 1.0000 loss_val: 0.6192 acc_val: 0.8672\n",
            "Epoch: 0094 loss_train: 0.0050 acc_train: 1.0000 loss_val: 0.6254 acc_val: 0.8635\n",
            "Epoch: 0095 loss_train: 0.0047 acc_train: 1.0000 loss_val: 0.6320 acc_val: 0.8635\n",
            "Epoch: 0096 loss_train: 0.0041 acc_train: 1.0000 loss_val: 0.6377 acc_val: 0.8635\n",
            "Epoch: 0097 loss_train: 0.0039 acc_train: 1.0000 loss_val: 0.6425 acc_val: 0.8635\n",
            "Epoch: 0098 loss_train: 0.0037 acc_train: 1.0000 loss_val: 0.6470 acc_val: 0.8635\n",
            "Epoch: 0099 loss_train: 0.0034 acc_train: 1.0000 loss_val: 0.6508 acc_val: 0.8635\n",
            "Epoch: 0100 loss_train: 0.0031 acc_train: 1.0000 loss_val: 0.6542 acc_val: 0.8653\n",
            "Epoch: 0101 loss_train: 0.0030 acc_train: 1.0000 loss_val: 0.6573 acc_val: 0.8672\n",
            "Epoch: 0102 loss_train: 0.0029 acc_train: 1.0000 loss_val: 0.6602 acc_val: 0.8672\n",
            "Optimization Finished!\n",
            "Train cost: 17.1276s\n",
            "Loading 50th epoch\n",
            "Test set results: loss= 0.3530 accuracy= 0.9000\n",
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=1448, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=7, bias=True)\n",
            ")\n",
            "total params: 2979849\n",
            "Epoch: 0001 loss_train: 1.9720 acc_train: 0.1212 loss_val: 1.9669 acc_val: 0.1310\n",
            "Epoch: 0002 loss_train: 1.9652 acc_train: 0.1236 loss_val: 1.9555 acc_val: 0.1310\n",
            "Epoch: 0003 loss_train: 1.9571 acc_train: 0.1267 loss_val: 1.9385 acc_val: 0.1439\n",
            "Epoch: 0004 loss_train: 1.9400 acc_train: 0.1427 loss_val: 1.9162 acc_val: 0.1661\n",
            "Epoch: 0005 loss_train: 1.9218 acc_train: 0.1691 loss_val: 1.8890 acc_val: 0.2638\n",
            "Epoch: 0006 loss_train: 1.8958 acc_train: 0.2509 loss_val: 1.8574 acc_val: 0.4280\n",
            "Epoch: 0007 loss_train: 1.8660 acc_train: 0.3739 loss_val: 1.8223 acc_val: 0.4797\n",
            "Epoch: 0008 loss_train: 1.8348 acc_train: 0.4416 loss_val: 1.7842 acc_val: 0.4742\n",
            "Epoch: 0009 loss_train: 1.8001 acc_train: 0.4557 loss_val: 1.7440 acc_val: 0.4742\n",
            "Epoch: 0010 loss_train: 1.7631 acc_train: 0.4563 loss_val: 1.7028 acc_val: 0.4742\n",
            "Epoch: 0011 loss_train: 1.7226 acc_train: 0.4594 loss_val: 1.6610 acc_val: 0.4852\n",
            "Epoch: 0012 loss_train: 1.6832 acc_train: 0.4668 loss_val: 1.6179 acc_val: 0.4982\n",
            "Epoch: 0013 loss_train: 1.6399 acc_train: 0.4815 loss_val: 1.5730 acc_val: 0.5148\n",
            "Epoch: 0014 loss_train: 1.5955 acc_train: 0.5037 loss_val: 1.5259 acc_val: 0.5240\n",
            "Epoch: 0015 loss_train: 1.5495 acc_train: 0.5141 loss_val: 1.4763 acc_val: 0.5351\n",
            "Epoch: 0016 loss_train: 1.5024 acc_train: 0.5271 loss_val: 1.4248 acc_val: 0.5480\n",
            "Epoch: 0017 loss_train: 1.4486 acc_train: 0.5461 loss_val: 1.3713 acc_val: 0.5609\n",
            "Epoch: 0018 loss_train: 1.3933 acc_train: 0.5683 loss_val: 1.3166 acc_val: 0.5867\n",
            "Epoch: 0019 loss_train: 1.3374 acc_train: 0.5879 loss_val: 1.2623 acc_val: 0.6033\n",
            "Epoch: 0020 loss_train: 1.2801 acc_train: 0.6193 loss_val: 1.2101 acc_val: 0.6494\n",
            "Epoch: 0021 loss_train: 1.2257 acc_train: 0.6679 loss_val: 1.1616 acc_val: 0.6661\n",
            "Epoch: 0022 loss_train: 1.1706 acc_train: 0.6974 loss_val: 1.1163 acc_val: 0.6753\n",
            "Epoch: 0023 loss_train: 1.1212 acc_train: 0.7146 loss_val: 1.0731 acc_val: 0.6808\n",
            "Epoch: 0024 loss_train: 1.0701 acc_train: 0.7294 loss_val: 1.0307 acc_val: 0.6919\n",
            "Epoch: 0025 loss_train: 1.0209 acc_train: 0.7380 loss_val: 0.9893 acc_val: 0.6919\n",
            "Epoch: 0026 loss_train: 0.9746 acc_train: 0.7460 loss_val: 0.9501 acc_val: 0.6937\n",
            "Epoch: 0027 loss_train: 0.9275 acc_train: 0.7448 loss_val: 0.9139 acc_val: 0.7011\n",
            "Epoch: 0028 loss_train: 0.8853 acc_train: 0.7442 loss_val: 0.8805 acc_val: 0.7066\n",
            "Epoch: 0029 loss_train: 0.8470 acc_train: 0.7534 loss_val: 0.8495 acc_val: 0.7214\n",
            "Epoch: 0030 loss_train: 0.8105 acc_train: 0.7632 loss_val: 0.8203 acc_val: 0.7306\n",
            "Epoch: 0031 loss_train: 0.7759 acc_train: 0.7804 loss_val: 0.7927 acc_val: 0.7472\n",
            "Epoch: 0032 loss_train: 0.7401 acc_train: 0.8044 loss_val: 0.7670 acc_val: 0.7583\n",
            "Epoch: 0033 loss_train: 0.7101 acc_train: 0.8124 loss_val: 0.7422 acc_val: 0.7638\n",
            "Epoch: 0034 loss_train: 0.6801 acc_train: 0.8180 loss_val: 0.7168 acc_val: 0.7731\n",
            "Epoch: 0035 loss_train: 0.6494 acc_train: 0.8247 loss_val: 0.6903 acc_val: 0.7897\n",
            "Epoch: 0036 loss_train: 0.6206 acc_train: 0.8296 loss_val: 0.6636 acc_val: 0.7952\n",
            "Epoch: 0037 loss_train: 0.5930 acc_train: 0.8376 loss_val: 0.6370 acc_val: 0.8007\n",
            "Epoch: 0038 loss_train: 0.5664 acc_train: 0.8426 loss_val: 0.6107 acc_val: 0.8026\n",
            "Epoch: 0039 loss_train: 0.5402 acc_train: 0.8456 loss_val: 0.5851 acc_val: 0.8063\n",
            "Epoch: 0040 loss_train: 0.5160 acc_train: 0.8567 loss_val: 0.5604 acc_val: 0.8100\n",
            "Epoch: 0041 loss_train: 0.4890 acc_train: 0.8659 loss_val: 0.5360 acc_val: 0.8155\n",
            "Epoch: 0042 loss_train: 0.4641 acc_train: 0.8690 loss_val: 0.5109 acc_val: 0.8229\n",
            "Epoch: 0043 loss_train: 0.4408 acc_train: 0.8770 loss_val: 0.4855 acc_val: 0.8376\n",
            "Epoch: 0044 loss_train: 0.4187 acc_train: 0.8807 loss_val: 0.4613 acc_val: 0.8506\n",
            "Epoch: 0045 loss_train: 0.3971 acc_train: 0.8875 loss_val: 0.4401 acc_val: 0.8616\n",
            "Epoch: 0046 loss_train: 0.3767 acc_train: 0.8924 loss_val: 0.4232 acc_val: 0.8764\n",
            "Epoch: 0047 loss_train: 0.3565 acc_train: 0.8948 loss_val: 0.4101 acc_val: 0.8764\n",
            "Epoch: 0048 loss_train: 0.3368 acc_train: 0.9022 loss_val: 0.3994 acc_val: 0.8782\n",
            "Epoch: 0049 loss_train: 0.3190 acc_train: 0.9114 loss_val: 0.3902 acc_val: 0.8782\n",
            "Epoch: 0050 loss_train: 0.3000 acc_train: 0.9170 loss_val: 0.3837 acc_val: 0.8764\n",
            "Epoch: 0051 loss_train: 0.2804 acc_train: 0.9274 loss_val: 0.3796 acc_val: 0.8745\n",
            "Epoch: 0052 loss_train: 0.2625 acc_train: 0.9311 loss_val: 0.3787 acc_val: 0.8764\n",
            "Epoch: 0053 loss_train: 0.2434 acc_train: 0.9354 loss_val: 0.3785 acc_val: 0.8782\n",
            "Epoch: 0054 loss_train: 0.2254 acc_train: 0.9453 loss_val: 0.3767 acc_val: 0.8764\n",
            "Epoch: 0055 loss_train: 0.2072 acc_train: 0.9514 loss_val: 0.3763 acc_val: 0.8801\n",
            "Epoch: 0056 loss_train: 0.1925 acc_train: 0.9520 loss_val: 0.3777 acc_val: 0.8819\n",
            "Epoch: 0057 loss_train: 0.1759 acc_train: 0.9563 loss_val: 0.3814 acc_val: 0.8856\n",
            "Epoch: 0058 loss_train: 0.1623 acc_train: 0.9613 loss_val: 0.3867 acc_val: 0.8856\n",
            "Epoch: 0059 loss_train: 0.1491 acc_train: 0.9643 loss_val: 0.3919 acc_val: 0.8838\n",
            "Epoch: 0060 loss_train: 0.1380 acc_train: 0.9705 loss_val: 0.3971 acc_val: 0.8727\n",
            "Epoch: 0061 loss_train: 0.1258 acc_train: 0.9711 loss_val: 0.4047 acc_val: 0.8708\n",
            "Epoch: 0062 loss_train: 0.1135 acc_train: 0.9772 loss_val: 0.4127 acc_val: 0.8727\n",
            "Epoch: 0063 loss_train: 0.1037 acc_train: 0.9785 loss_val: 0.4236 acc_val: 0.8727\n",
            "Epoch: 0064 loss_train: 0.0937 acc_train: 0.9834 loss_val: 0.4327 acc_val: 0.8745\n",
            "Epoch: 0065 loss_train: 0.0860 acc_train: 0.9852 loss_val: 0.4382 acc_val: 0.8764\n",
            "Epoch: 0066 loss_train: 0.0808 acc_train: 0.9865 loss_val: 0.4428 acc_val: 0.8764\n",
            "Epoch: 0067 loss_train: 0.0719 acc_train: 0.9883 loss_val: 0.4509 acc_val: 0.8782\n",
            "Epoch: 0068 loss_train: 0.0657 acc_train: 0.9883 loss_val: 0.4609 acc_val: 0.8764\n",
            "Epoch: 0069 loss_train: 0.0582 acc_train: 0.9902 loss_val: 0.4698 acc_val: 0.8745\n",
            "Epoch: 0070 loss_train: 0.0534 acc_train: 0.9908 loss_val: 0.4767 acc_val: 0.8727\n",
            "Epoch: 0071 loss_train: 0.0492 acc_train: 0.9902 loss_val: 0.4824 acc_val: 0.8727\n",
            "Epoch: 0072 loss_train: 0.0439 acc_train: 0.9938 loss_val: 0.4871 acc_val: 0.8727\n",
            "Epoch: 0073 loss_train: 0.0408 acc_train: 0.9938 loss_val: 0.4949 acc_val: 0.8708\n",
            "Epoch: 0074 loss_train: 0.0356 acc_train: 0.9938 loss_val: 0.5039 acc_val: 0.8727\n",
            "Epoch: 0075 loss_train: 0.0328 acc_train: 0.9951 loss_val: 0.5131 acc_val: 0.8690\n",
            "Epoch: 0076 loss_train: 0.0297 acc_train: 0.9957 loss_val: 0.5202 acc_val: 0.8690\n",
            "Epoch: 0077 loss_train: 0.0273 acc_train: 0.9963 loss_val: 0.5247 acc_val: 0.8708\n",
            "Epoch: 0078 loss_train: 0.0245 acc_train: 0.9982 loss_val: 0.5285 acc_val: 0.8708\n",
            "Epoch: 0079 loss_train: 0.0223 acc_train: 0.9982 loss_val: 0.5334 acc_val: 0.8708\n",
            "Epoch: 0080 loss_train: 0.0204 acc_train: 0.9982 loss_val: 0.5410 acc_val: 0.8690\n",
            "Epoch: 0081 loss_train: 0.0183 acc_train: 0.9988 loss_val: 0.5483 acc_val: 0.8690\n",
            "Epoch: 0082 loss_train: 0.0164 acc_train: 0.9988 loss_val: 0.5541 acc_val: 0.8690\n",
            "Epoch: 0083 loss_train: 0.0145 acc_train: 0.9988 loss_val: 0.5594 acc_val: 0.8690\n",
            "Epoch: 0084 loss_train: 0.0131 acc_train: 0.9994 loss_val: 0.5650 acc_val: 0.8672\n",
            "Epoch: 0085 loss_train: 0.0117 acc_train: 0.9994 loss_val: 0.5707 acc_val: 0.8672\n",
            "Epoch: 0086 loss_train: 0.0109 acc_train: 0.9994 loss_val: 0.5770 acc_val: 0.8635\n",
            "Epoch: 0087 loss_train: 0.0098 acc_train: 0.9994 loss_val: 0.5846 acc_val: 0.8672\n",
            "Epoch: 0088 loss_train: 0.0092 acc_train: 0.9994 loss_val: 0.5924 acc_val: 0.8672\n",
            "Epoch: 0089 loss_train: 0.0082 acc_train: 0.9994 loss_val: 0.6004 acc_val: 0.8653\n",
            "Epoch: 0090 loss_train: 0.0078 acc_train: 0.9994 loss_val: 0.6083 acc_val: 0.8672\n",
            "Epoch: 0091 loss_train: 0.0075 acc_train: 0.9994 loss_val: 0.6157 acc_val: 0.8653\n",
            "Epoch: 0092 loss_train: 0.0064 acc_train: 0.9994 loss_val: 0.6224 acc_val: 0.8635\n",
            "Epoch: 0093 loss_train: 0.0062 acc_train: 0.9994 loss_val: 0.6282 acc_val: 0.8635\n",
            "Epoch: 0094 loss_train: 0.0054 acc_train: 0.9994 loss_val: 0.6326 acc_val: 0.8598\n",
            "Epoch: 0095 loss_train: 0.0050 acc_train: 0.9994 loss_val: 0.6370 acc_val: 0.8598\n",
            "Epoch: 0096 loss_train: 0.0045 acc_train: 1.0000 loss_val: 0.6409 acc_val: 0.8561\n",
            "Epoch: 0097 loss_train: 0.0040 acc_train: 1.0000 loss_val: 0.6436 acc_val: 0.8579\n",
            "Epoch: 0098 loss_train: 0.0038 acc_train: 1.0000 loss_val: 0.6466 acc_val: 0.8598\n",
            "Epoch: 0099 loss_train: 0.0034 acc_train: 1.0000 loss_val: 0.6497 acc_val: 0.8598\n",
            "Epoch: 0100 loss_train: 0.0032 acc_train: 1.0000 loss_val: 0.6540 acc_val: 0.8635\n",
            "Epoch: 0101 loss_train: 0.0030 acc_train: 1.0000 loss_val: 0.6589 acc_val: 0.8653\n",
            "Epoch: 0102 loss_train: 0.0028 acc_train: 1.0000 loss_val: 0.6648 acc_val: 0.8653\n",
            "Epoch: 0103 loss_train: 0.0027 acc_train: 1.0000 loss_val: 0.6712 acc_val: 0.8672\n",
            "Epoch: 0104 loss_train: 0.0025 acc_train: 1.0000 loss_val: 0.6776 acc_val: 0.8672\n",
            "Epoch: 0105 loss_train: 0.0024 acc_train: 1.0000 loss_val: 0.6834 acc_val: 0.8653\n",
            "Epoch: 0106 loss_train: 0.0022 acc_train: 1.0000 loss_val: 0.6881 acc_val: 0.8653\n",
            "Epoch: 0107 loss_train: 0.0021 acc_train: 1.0000 loss_val: 0.6913 acc_val: 0.8653\n",
            "Epoch: 0108 loss_train: 0.0020 acc_train: 1.0000 loss_val: 0.6934 acc_val: 0.8635\n",
            "Optimization Finished!\n",
            "Train cost: 25.7904s\n",
            "Loading 57th epoch\n",
            "Test set results: loss= 0.3251 accuracy= 0.9000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --dataset cora --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 5  --n_heads 8 --n_layers 1 --pe_dim 5 --peak_lr 0.001  --attack none --attperc 20 --TrainTest train --weight_decay=1e-05\n",
        "\n",
        "!python train.py --dataset cora --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 5  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack none --attperc 20 --TrainTest train --weight_decay=1e-05\n",
        "\n",
        "!python train.py --dataset cora --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 5  --n_heads 8 --n_layers 1 --pe_dim 40 --peak_lr 0.001  --attack none --attperc 20 --TrainTest train --weight_decay=1e-05\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKxcLYnsU7TX",
        "outputId": "6afb9c62-b96f-4da5-ecc3-7d94fb23ca17"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=1438, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=7, bias=True)\n",
            ")\n",
            "total params: 2974729\n",
            "Epoch: 0001 loss_train: 1.9269 acc_train: 0.2232 loss_val: 1.9198 acc_val: 0.2509\n",
            "Epoch: 0002 loss_train: 1.9219 acc_train: 0.2331 loss_val: 1.9084 acc_val: 0.2583\n",
            "Epoch: 0003 loss_train: 1.9114 acc_train: 0.2435 loss_val: 1.8915 acc_val: 0.2675\n",
            "Epoch: 0004 loss_train: 1.8946 acc_train: 0.2718 loss_val: 1.8694 acc_val: 0.2731\n",
            "Epoch: 0005 loss_train: 1.8736 acc_train: 0.2780 loss_val: 1.8425 acc_val: 0.2768\n",
            "Epoch: 0006 loss_train: 1.8502 acc_train: 0.2897 loss_val: 1.8111 acc_val: 0.2897\n",
            "Epoch: 0007 loss_train: 1.8202 acc_train: 0.3057 loss_val: 1.7759 acc_val: 0.3155\n",
            "Epoch: 0008 loss_train: 1.7866 acc_train: 0.3266 loss_val: 1.7371 acc_val: 0.3598\n",
            "Epoch: 0009 loss_train: 1.7533 acc_train: 0.3475 loss_val: 1.6954 acc_val: 0.3893\n",
            "Epoch: 0010 loss_train: 1.7137 acc_train: 0.3739 loss_val: 1.6511 acc_val: 0.4225\n",
            "Epoch: 0011 loss_train: 1.6725 acc_train: 0.4010 loss_val: 1.6041 acc_val: 0.4410\n",
            "Epoch: 0012 loss_train: 1.6272 acc_train: 0.4262 loss_val: 1.5543 acc_val: 0.4557\n",
            "Epoch: 0013 loss_train: 1.5785 acc_train: 0.4533 loss_val: 1.5015 acc_val: 0.4852\n",
            "Epoch: 0014 loss_train: 1.5289 acc_train: 0.4772 loss_val: 1.4455 acc_val: 0.5111\n",
            "Epoch: 0015 loss_train: 1.4735 acc_train: 0.5141 loss_val: 1.3863 acc_val: 0.5646\n",
            "Epoch: 0016 loss_train: 1.4136 acc_train: 0.5566 loss_val: 1.3243 acc_val: 0.6015\n",
            "Epoch: 0017 loss_train: 1.3536 acc_train: 0.6039 loss_val: 1.2605 acc_val: 0.6402\n",
            "Epoch: 0018 loss_train: 1.2900 acc_train: 0.6494 loss_val: 1.1968 acc_val: 0.6937\n",
            "Epoch: 0019 loss_train: 1.2249 acc_train: 0.7066 loss_val: 1.1349 acc_val: 0.7011\n",
            "Epoch: 0020 loss_train: 1.1622 acc_train: 0.7343 loss_val: 1.0757 acc_val: 0.7177\n",
            "Epoch: 0021 loss_train: 1.1012 acc_train: 0.7558 loss_val: 1.0194 acc_val: 0.7251\n",
            "Epoch: 0022 loss_train: 1.0378 acc_train: 0.7774 loss_val: 0.9663 acc_val: 0.7509\n",
            "Epoch: 0023 loss_train: 0.9819 acc_train: 0.7872 loss_val: 0.9165 acc_val: 0.7638\n",
            "Epoch: 0024 loss_train: 0.9254 acc_train: 0.8020 loss_val: 0.8696 acc_val: 0.7731\n",
            "Epoch: 0025 loss_train: 0.8731 acc_train: 0.8100 loss_val: 0.8262 acc_val: 0.7768\n",
            "Epoch: 0026 loss_train: 0.8257 acc_train: 0.8106 loss_val: 0.7861 acc_val: 0.7768\n",
            "Epoch: 0027 loss_train: 0.7814 acc_train: 0.8186 loss_val: 0.7492 acc_val: 0.7786\n",
            "Epoch: 0028 loss_train: 0.7391 acc_train: 0.8216 loss_val: 0.7149 acc_val: 0.7860\n",
            "Epoch: 0029 loss_train: 0.7010 acc_train: 0.8290 loss_val: 0.6830 acc_val: 0.7970\n",
            "Epoch: 0030 loss_train: 0.6674 acc_train: 0.8376 loss_val: 0.6534 acc_val: 0.8044\n",
            "Epoch: 0031 loss_train: 0.6335 acc_train: 0.8413 loss_val: 0.6262 acc_val: 0.8044\n",
            "Epoch: 0032 loss_train: 0.6017 acc_train: 0.8450 loss_val: 0.6009 acc_val: 0.8137\n",
            "Epoch: 0033 loss_train: 0.5715 acc_train: 0.8512 loss_val: 0.5769 acc_val: 0.8266\n",
            "Epoch: 0034 loss_train: 0.5455 acc_train: 0.8536 loss_val: 0.5540 acc_val: 0.8266\n",
            "Epoch: 0035 loss_train: 0.5197 acc_train: 0.8573 loss_val: 0.5322 acc_val: 0.8321\n",
            "Epoch: 0036 loss_train: 0.4948 acc_train: 0.8604 loss_val: 0.5117 acc_val: 0.8339\n",
            "Epoch: 0037 loss_train: 0.4719 acc_train: 0.8641 loss_val: 0.4927 acc_val: 0.8358\n",
            "Epoch: 0038 loss_train: 0.4539 acc_train: 0.8647 loss_val: 0.4751 acc_val: 0.8450\n",
            "Epoch: 0039 loss_train: 0.4319 acc_train: 0.8702 loss_val: 0.4590 acc_val: 0.8524\n",
            "Epoch: 0040 loss_train: 0.4101 acc_train: 0.8764 loss_val: 0.4442 acc_val: 0.8616\n",
            "Epoch: 0041 loss_train: 0.3924 acc_train: 0.8875 loss_val: 0.4304 acc_val: 0.8635\n",
            "Epoch: 0042 loss_train: 0.3739 acc_train: 0.8918 loss_val: 0.4170 acc_val: 0.8635\n",
            "Epoch: 0043 loss_train: 0.3568 acc_train: 0.8948 loss_val: 0.4047 acc_val: 0.8672\n",
            "Epoch: 0044 loss_train: 0.3392 acc_train: 0.9010 loss_val: 0.3934 acc_val: 0.8764\n",
            "Epoch: 0045 loss_train: 0.3205 acc_train: 0.9071 loss_val: 0.3829 acc_val: 0.8801\n",
            "Epoch: 0046 loss_train: 0.3022 acc_train: 0.9121 loss_val: 0.3739 acc_val: 0.8856\n",
            "Epoch: 0047 loss_train: 0.2861 acc_train: 0.9182 loss_val: 0.3676 acc_val: 0.8930\n",
            "Epoch: 0048 loss_train: 0.2706 acc_train: 0.9225 loss_val: 0.3632 acc_val: 0.8893\n",
            "Epoch: 0049 loss_train: 0.2514 acc_train: 0.9287 loss_val: 0.3614 acc_val: 0.8893\n",
            "Epoch: 0050 loss_train: 0.2372 acc_train: 0.9299 loss_val: 0.3622 acc_val: 0.8893\n",
            "Epoch: 0051 loss_train: 0.2209 acc_train: 0.9410 loss_val: 0.3645 acc_val: 0.8819\n",
            "Epoch: 0052 loss_train: 0.2077 acc_train: 0.9428 loss_val: 0.3691 acc_val: 0.8801\n",
            "Epoch: 0053 loss_train: 0.1912 acc_train: 0.9496 loss_val: 0.3758 acc_val: 0.8764\n",
            "Epoch: 0054 loss_train: 0.1789 acc_train: 0.9526 loss_val: 0.3824 acc_val: 0.8764\n",
            "Epoch: 0055 loss_train: 0.1639 acc_train: 0.9551 loss_val: 0.3879 acc_val: 0.8727\n",
            "Epoch: 0056 loss_train: 0.1531 acc_train: 0.9594 loss_val: 0.3935 acc_val: 0.8708\n",
            "Epoch: 0057 loss_train: 0.1388 acc_train: 0.9662 loss_val: 0.3998 acc_val: 0.8690\n",
            "Epoch: 0058 loss_train: 0.1258 acc_train: 0.9680 loss_val: 0.4062 acc_val: 0.8672\n",
            "Epoch: 0059 loss_train: 0.1123 acc_train: 0.9736 loss_val: 0.4144 acc_val: 0.8690\n",
            "Epoch: 0060 loss_train: 0.1041 acc_train: 0.9754 loss_val: 0.4225 acc_val: 0.8690\n",
            "Epoch: 0061 loss_train: 0.0947 acc_train: 0.9766 loss_val: 0.4311 acc_val: 0.8690\n",
            "Epoch: 0062 loss_train: 0.0864 acc_train: 0.9803 loss_val: 0.4410 acc_val: 0.8690\n",
            "Epoch: 0063 loss_train: 0.0802 acc_train: 0.9815 loss_val: 0.4539 acc_val: 0.8672\n",
            "Epoch: 0064 loss_train: 0.0711 acc_train: 0.9840 loss_val: 0.4690 acc_val: 0.8672\n",
            "Epoch: 0065 loss_train: 0.0633 acc_train: 0.9877 loss_val: 0.4819 acc_val: 0.8653\n",
            "Epoch: 0066 loss_train: 0.0566 acc_train: 0.9920 loss_val: 0.4921 acc_val: 0.8635\n",
            "Epoch: 0067 loss_train: 0.0511 acc_train: 0.9932 loss_val: 0.5015 acc_val: 0.8635\n",
            "Epoch: 0068 loss_train: 0.0470 acc_train: 0.9932 loss_val: 0.5133 acc_val: 0.8635\n",
            "Epoch: 0069 loss_train: 0.0422 acc_train: 0.9932 loss_val: 0.5252 acc_val: 0.8616\n",
            "Epoch: 0070 loss_train: 0.0374 acc_train: 0.9926 loss_val: 0.5383 acc_val: 0.8653\n",
            "Epoch: 0071 loss_train: 0.0337 acc_train: 0.9963 loss_val: 0.5495 acc_val: 0.8653\n",
            "Epoch: 0072 loss_train: 0.0313 acc_train: 0.9932 loss_val: 0.5539 acc_val: 0.8616\n",
            "Epoch: 0073 loss_train: 0.0283 acc_train: 0.9957 loss_val: 0.5555 acc_val: 0.8542\n",
            "Epoch: 0074 loss_train: 0.0248 acc_train: 0.9945 loss_val: 0.5583 acc_val: 0.8561\n",
            "Epoch: 0075 loss_train: 0.0220 acc_train: 0.9969 loss_val: 0.5663 acc_val: 0.8542\n",
            "Epoch: 0076 loss_train: 0.0193 acc_train: 0.9982 loss_val: 0.5774 acc_val: 0.8542\n",
            "Epoch: 0077 loss_train: 0.0175 acc_train: 0.9975 loss_val: 0.5893 acc_val: 0.8561\n",
            "Epoch: 0078 loss_train: 0.0162 acc_train: 0.9969 loss_val: 0.6007 acc_val: 0.8561\n",
            "Epoch: 0079 loss_train: 0.0148 acc_train: 0.9982 loss_val: 0.6084 acc_val: 0.8579\n",
            "Epoch: 0080 loss_train: 0.0132 acc_train: 0.9988 loss_val: 0.6107 acc_val: 0.8561\n",
            "Epoch: 0081 loss_train: 0.0119 acc_train: 0.9988 loss_val: 0.6104 acc_val: 0.8561\n",
            "Epoch: 0082 loss_train: 0.0113 acc_train: 0.9982 loss_val: 0.6112 acc_val: 0.8579\n",
            "Epoch: 0083 loss_train: 0.0101 acc_train: 0.9994 loss_val: 0.6145 acc_val: 0.8579\n",
            "Epoch: 0084 loss_train: 0.0097 acc_train: 0.9988 loss_val: 0.6201 acc_val: 0.8579\n",
            "Epoch: 0085 loss_train: 0.0090 acc_train: 0.9988 loss_val: 0.6273 acc_val: 0.8561\n",
            "Epoch: 0086 loss_train: 0.0081 acc_train: 1.0000 loss_val: 0.6342 acc_val: 0.8561\n",
            "Epoch: 0087 loss_train: 0.0079 acc_train: 0.9994 loss_val: 0.6406 acc_val: 0.8579\n",
            "Epoch: 0088 loss_train: 0.0068 acc_train: 1.0000 loss_val: 0.6466 acc_val: 0.8579\n",
            "Epoch: 0089 loss_train: 0.0067 acc_train: 1.0000 loss_val: 0.6510 acc_val: 0.8561\n",
            "Epoch: 0090 loss_train: 0.0062 acc_train: 1.0000 loss_val: 0.6542 acc_val: 0.8561\n",
            "Epoch: 0091 loss_train: 0.0057 acc_train: 0.9994 loss_val: 0.6573 acc_val: 0.8561\n",
            "Epoch: 0092 loss_train: 0.0056 acc_train: 0.9994 loss_val: 0.6606 acc_val: 0.8561\n",
            "Epoch: 0093 loss_train: 0.0053 acc_train: 1.0000 loss_val: 0.6646 acc_val: 0.8598\n",
            "Epoch: 0094 loss_train: 0.0049 acc_train: 0.9994 loss_val: 0.6693 acc_val: 0.8598\n",
            "Epoch: 0095 loss_train: 0.0047 acc_train: 1.0000 loss_val: 0.6746 acc_val: 0.8616\n",
            "Epoch: 0096 loss_train: 0.0042 acc_train: 1.0000 loss_val: 0.6803 acc_val: 0.8598\n",
            "Epoch: 0097 loss_train: 0.0041 acc_train: 1.0000 loss_val: 0.6855 acc_val: 0.8598\n",
            "Epoch: 0098 loss_train: 0.0039 acc_train: 1.0000 loss_val: 0.6898 acc_val: 0.8579\n",
            "Epoch: 0099 loss_train: 0.0036 acc_train: 1.0000 loss_val: 0.6935 acc_val: 0.8598\n",
            "Optimization Finished!\n",
            "Train cost: 15.8339s\n",
            "Loading 47th epoch\n",
            "Test set results: loss= 0.3485 accuracy= 0.8889\n",
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=1448, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=7, bias=True)\n",
            ")\n",
            "total params: 2979849\n",
            "Epoch: 0001 loss_train: 1.9727 acc_train: 0.1156 loss_val: 1.9686 acc_val: 0.1310\n",
            "Epoch: 0002 loss_train: 1.9669 acc_train: 0.1279 loss_val: 1.9561 acc_val: 0.1384\n",
            "Epoch: 0003 loss_train: 1.9547 acc_train: 0.1396 loss_val: 1.9375 acc_val: 0.1587\n",
            "Epoch: 0004 loss_train: 1.9373 acc_train: 0.1525 loss_val: 1.9130 acc_val: 0.2122\n",
            "Epoch: 0005 loss_train: 1.9160 acc_train: 0.1925 loss_val: 1.8830 acc_val: 0.3118\n",
            "Epoch: 0006 loss_train: 1.8886 acc_train: 0.2768 loss_val: 1.8478 acc_val: 0.4428\n",
            "Epoch: 0007 loss_train: 1.8561 acc_train: 0.3930 loss_val: 1.8078 acc_val: 0.4815\n",
            "Epoch: 0008 loss_train: 1.8199 acc_train: 0.4736 loss_val: 1.7640 acc_val: 0.5055\n",
            "Epoch: 0009 loss_train: 1.7805 acc_train: 0.4889 loss_val: 1.7170 acc_val: 0.5055\n",
            "Epoch: 0010 loss_train: 1.7355 acc_train: 0.5092 loss_val: 1.6678 acc_val: 0.5166\n",
            "Epoch: 0011 loss_train: 1.6880 acc_train: 0.5203 loss_val: 1.6169 acc_val: 0.5277\n",
            "Epoch: 0012 loss_train: 1.6405 acc_train: 0.5295 loss_val: 1.5647 acc_val: 0.5461\n",
            "Epoch: 0013 loss_train: 1.5898 acc_train: 0.5387 loss_val: 1.5112 acc_val: 0.5517\n",
            "Epoch: 0014 loss_train: 1.5383 acc_train: 0.5541 loss_val: 1.4567 acc_val: 0.5627\n",
            "Epoch: 0015 loss_train: 1.4837 acc_train: 0.5658 loss_val: 1.4014 acc_val: 0.5701\n",
            "Epoch: 0016 loss_train: 1.4274 acc_train: 0.5824 loss_val: 1.3452 acc_val: 0.5849\n",
            "Epoch: 0017 loss_train: 1.3709 acc_train: 0.5996 loss_val: 1.2880 acc_val: 0.6070\n",
            "Epoch: 0018 loss_train: 1.3128 acc_train: 0.6193 loss_val: 1.2305 acc_val: 0.6494\n",
            "Epoch: 0019 loss_train: 1.2546 acc_train: 0.6513 loss_val: 1.1740 acc_val: 0.6734\n",
            "Epoch: 0020 loss_train: 1.1946 acc_train: 0.6845 loss_val: 1.1196 acc_val: 0.6882\n",
            "Epoch: 0021 loss_train: 1.1373 acc_train: 0.7103 loss_val: 1.0686 acc_val: 0.6993\n",
            "Epoch: 0022 loss_train: 1.0810 acc_train: 0.7269 loss_val: 1.0214 acc_val: 0.6974\n",
            "Epoch: 0023 loss_train: 1.0270 acc_train: 0.7386 loss_val: 0.9779 acc_val: 0.7103\n",
            "Epoch: 0024 loss_train: 0.9762 acc_train: 0.7491 loss_val: 0.9369 acc_val: 0.7214\n",
            "Epoch: 0025 loss_train: 0.9308 acc_train: 0.7558 loss_val: 0.8978 acc_val: 0.7288\n",
            "Epoch: 0026 loss_train: 0.8845 acc_train: 0.7706 loss_val: 0.8600 acc_val: 0.7306\n",
            "Epoch: 0027 loss_train: 0.8421 acc_train: 0.7811 loss_val: 0.8239 acc_val: 0.7472\n",
            "Epoch: 0028 loss_train: 0.8022 acc_train: 0.8007 loss_val: 0.7899 acc_val: 0.7509\n",
            "Epoch: 0029 loss_train: 0.7630 acc_train: 0.8093 loss_val: 0.7580 acc_val: 0.7638\n",
            "Epoch: 0030 loss_train: 0.7271 acc_train: 0.8167 loss_val: 0.7286 acc_val: 0.7768\n",
            "Epoch: 0031 loss_train: 0.6939 acc_train: 0.8266 loss_val: 0.7010 acc_val: 0.7804\n",
            "Epoch: 0032 loss_train: 0.6632 acc_train: 0.8284 loss_val: 0.6747 acc_val: 0.7823\n",
            "Epoch: 0033 loss_train: 0.6327 acc_train: 0.8346 loss_val: 0.6496 acc_val: 0.7823\n",
            "Epoch: 0034 loss_train: 0.6044 acc_train: 0.8432 loss_val: 0.6263 acc_val: 0.7878\n",
            "Epoch: 0035 loss_train: 0.5759 acc_train: 0.8481 loss_val: 0.6049 acc_val: 0.8044\n",
            "Epoch: 0036 loss_train: 0.5496 acc_train: 0.8524 loss_val: 0.5844 acc_val: 0.8081\n",
            "Epoch: 0037 loss_train: 0.5268 acc_train: 0.8598 loss_val: 0.5635 acc_val: 0.8118\n",
            "Epoch: 0038 loss_train: 0.5036 acc_train: 0.8598 loss_val: 0.5414 acc_val: 0.8192\n",
            "Epoch: 0039 loss_train: 0.4787 acc_train: 0.8647 loss_val: 0.5187 acc_val: 0.8266\n",
            "Epoch: 0040 loss_train: 0.4569 acc_train: 0.8684 loss_val: 0.4966 acc_val: 0.8266\n",
            "Epoch: 0041 loss_train: 0.4365 acc_train: 0.8739 loss_val: 0.4760 acc_val: 0.8303\n",
            "Epoch: 0042 loss_train: 0.4171 acc_train: 0.8733 loss_val: 0.4578 acc_val: 0.8413\n",
            "Epoch: 0043 loss_train: 0.3960 acc_train: 0.8819 loss_val: 0.4426 acc_val: 0.8561\n",
            "Epoch: 0044 loss_train: 0.3774 acc_train: 0.8887 loss_val: 0.4296 acc_val: 0.8635\n",
            "Epoch: 0045 loss_train: 0.3595 acc_train: 0.8918 loss_val: 0.4170 acc_val: 0.8690\n",
            "Epoch: 0046 loss_train: 0.3405 acc_train: 0.8998 loss_val: 0.4041 acc_val: 0.8727\n",
            "Epoch: 0047 loss_train: 0.3229 acc_train: 0.9090 loss_val: 0.3923 acc_val: 0.8745\n",
            "Epoch: 0048 loss_train: 0.3065 acc_train: 0.9157 loss_val: 0.3836 acc_val: 0.8838\n",
            "Epoch: 0049 loss_train: 0.2879 acc_train: 0.9194 loss_val: 0.3782 acc_val: 0.8875\n",
            "Epoch: 0050 loss_train: 0.2725 acc_train: 0.9274 loss_val: 0.3758 acc_val: 0.8893\n",
            "Epoch: 0051 loss_train: 0.2565 acc_train: 0.9293 loss_val: 0.3755 acc_val: 0.8838\n",
            "Epoch: 0052 loss_train: 0.2410 acc_train: 0.9342 loss_val: 0.3742 acc_val: 0.8819\n",
            "Epoch: 0053 loss_train: 0.2288 acc_train: 0.9403 loss_val: 0.3732 acc_val: 0.8838\n",
            "Epoch: 0054 loss_train: 0.2129 acc_train: 0.9434 loss_val: 0.3749 acc_val: 0.8819\n",
            "Epoch: 0055 loss_train: 0.1947 acc_train: 0.9508 loss_val: 0.3805 acc_val: 0.8782\n",
            "Epoch: 0056 loss_train: 0.1813 acc_train: 0.9533 loss_val: 0.3863 acc_val: 0.8801\n",
            "Epoch: 0057 loss_train: 0.1677 acc_train: 0.9576 loss_val: 0.3874 acc_val: 0.8782\n",
            "Epoch: 0058 loss_train: 0.1540 acc_train: 0.9619 loss_val: 0.3872 acc_val: 0.8782\n",
            "Epoch: 0059 loss_train: 0.1432 acc_train: 0.9656 loss_val: 0.3902 acc_val: 0.8782\n",
            "Epoch: 0060 loss_train: 0.1315 acc_train: 0.9686 loss_val: 0.3960 acc_val: 0.8801\n",
            "Epoch: 0061 loss_train: 0.1190 acc_train: 0.9729 loss_val: 0.4023 acc_val: 0.8782\n",
            "Epoch: 0062 loss_train: 0.1102 acc_train: 0.9760 loss_val: 0.4080 acc_val: 0.8764\n",
            "Epoch: 0063 loss_train: 0.1000 acc_train: 0.9822 loss_val: 0.4138 acc_val: 0.8764\n",
            "Epoch: 0064 loss_train: 0.0907 acc_train: 0.9859 loss_val: 0.4196 acc_val: 0.8764\n",
            "Epoch: 0065 loss_train: 0.0822 acc_train: 0.9865 loss_val: 0.4272 acc_val: 0.8801\n",
            "Epoch: 0066 loss_train: 0.0764 acc_train: 0.9852 loss_val: 0.4338 acc_val: 0.8745\n",
            "Epoch: 0067 loss_train: 0.0687 acc_train: 0.9871 loss_val: 0.4419 acc_val: 0.8727\n",
            "Epoch: 0068 loss_train: 0.0618 acc_train: 0.9895 loss_val: 0.4517 acc_val: 0.8708\n",
            "Epoch: 0069 loss_train: 0.0564 acc_train: 0.9914 loss_val: 0.4612 acc_val: 0.8727\n",
            "Epoch: 0070 loss_train: 0.0516 acc_train: 0.9908 loss_val: 0.4702 acc_val: 0.8708\n",
            "Epoch: 0071 loss_train: 0.0470 acc_train: 0.9920 loss_val: 0.4785 acc_val: 0.8690\n",
            "Epoch: 0072 loss_train: 0.0425 acc_train: 0.9926 loss_val: 0.4848 acc_val: 0.8690\n",
            "Epoch: 0073 loss_train: 0.0387 acc_train: 0.9945 loss_val: 0.4902 acc_val: 0.8690\n",
            "Epoch: 0074 loss_train: 0.0355 acc_train: 0.9938 loss_val: 0.4948 acc_val: 0.8672\n",
            "Epoch: 0075 loss_train: 0.0312 acc_train: 0.9945 loss_val: 0.4993 acc_val: 0.8672\n",
            "Epoch: 0076 loss_train: 0.0288 acc_train: 0.9963 loss_val: 0.5046 acc_val: 0.8690\n",
            "Epoch: 0077 loss_train: 0.0262 acc_train: 0.9951 loss_val: 0.5107 acc_val: 0.8690\n",
            "Epoch: 0078 loss_train: 0.0229 acc_train: 0.9988 loss_val: 0.5159 acc_val: 0.8708\n",
            "Epoch: 0079 loss_train: 0.0213 acc_train: 0.9982 loss_val: 0.5216 acc_val: 0.8708\n",
            "Epoch: 0080 loss_train: 0.0198 acc_train: 0.9975 loss_val: 0.5286 acc_val: 0.8708\n",
            "Epoch: 0081 loss_train: 0.0173 acc_train: 0.9982 loss_val: 0.5363 acc_val: 0.8690\n",
            "Epoch: 0082 loss_train: 0.0152 acc_train: 0.9994 loss_val: 0.5455 acc_val: 0.8708\n",
            "Epoch: 0083 loss_train: 0.0139 acc_train: 0.9988 loss_val: 0.5557 acc_val: 0.8727\n",
            "Epoch: 0084 loss_train: 0.0124 acc_train: 0.9994 loss_val: 0.5655 acc_val: 0.8708\n",
            "Epoch: 0085 loss_train: 0.0114 acc_train: 0.9994 loss_val: 0.5740 acc_val: 0.8690\n",
            "Epoch: 0086 loss_train: 0.0101 acc_train: 1.0000 loss_val: 0.5804 acc_val: 0.8690\n",
            "Epoch: 0087 loss_train: 0.0090 acc_train: 1.0000 loss_val: 0.5846 acc_val: 0.8708\n",
            "Epoch: 0088 loss_train: 0.0084 acc_train: 1.0000 loss_val: 0.5882 acc_val: 0.8690\n",
            "Epoch: 0089 loss_train: 0.0082 acc_train: 0.9994 loss_val: 0.5925 acc_val: 0.8708\n",
            "Epoch: 0090 loss_train: 0.0073 acc_train: 1.0000 loss_val: 0.5984 acc_val: 0.8708\n",
            "Epoch: 0091 loss_train: 0.0067 acc_train: 1.0000 loss_val: 0.6056 acc_val: 0.8690\n",
            "Epoch: 0092 loss_train: 0.0060 acc_train: 1.0000 loss_val: 0.6134 acc_val: 0.8690\n",
            "Epoch: 0093 loss_train: 0.0053 acc_train: 1.0000 loss_val: 0.6213 acc_val: 0.8708\n",
            "Epoch: 0094 loss_train: 0.0052 acc_train: 1.0000 loss_val: 0.6275 acc_val: 0.8690\n",
            "Epoch: 0095 loss_train: 0.0047 acc_train: 1.0000 loss_val: 0.6331 acc_val: 0.8672\n",
            "Epoch: 0096 loss_train: 0.0043 acc_train: 1.0000 loss_val: 0.6373 acc_val: 0.8672\n",
            "Epoch: 0097 loss_train: 0.0041 acc_train: 1.0000 loss_val: 0.6404 acc_val: 0.8690\n",
            "Epoch: 0098 loss_train: 0.0040 acc_train: 1.0000 loss_val: 0.6428 acc_val: 0.8672\n",
            "Epoch: 0099 loss_train: 0.0035 acc_train: 1.0000 loss_val: 0.6451 acc_val: 0.8672\n",
            "Epoch: 0100 loss_train: 0.0035 acc_train: 1.0000 loss_val: 0.6471 acc_val: 0.8635\n",
            "Epoch: 0101 loss_train: 0.0032 acc_train: 1.0000 loss_val: 0.6496 acc_val: 0.8635\n",
            "Epoch: 0102 loss_train: 0.0031 acc_train: 1.0000 loss_val: 0.6526 acc_val: 0.8635\n",
            "Epoch: 0103 loss_train: 0.0029 acc_train: 1.0000 loss_val: 0.6555 acc_val: 0.8635\n",
            "Optimization Finished!\n",
            "Train cost: 16.1016s\n",
            "Loading 50th epoch\n",
            "Test set results: loss= 0.3476 accuracy= 0.9019\n",
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=1473, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=7, bias=True)\n",
            ")\n",
            "total params: 2992649\n",
            "Epoch: 0001 loss_train: 2.0143 acc_train: 0.0584 loss_val: 2.0117 acc_val: 0.0609\n",
            "Epoch: 0002 loss_train: 2.0112 acc_train: 0.0738 loss_val: 1.9995 acc_val: 0.0793\n",
            "Epoch: 0003 loss_train: 1.9972 acc_train: 0.0738 loss_val: 1.9814 acc_val: 0.0959\n",
            "Epoch: 0004 loss_train: 1.9807 acc_train: 0.0867 loss_val: 1.9574 acc_val: 0.1107\n",
            "Epoch: 0005 loss_train: 1.9582 acc_train: 0.1076 loss_val: 1.9280 acc_val: 0.1476\n",
            "Epoch: 0006 loss_train: 1.9333 acc_train: 0.1531 loss_val: 1.8935 acc_val: 0.2177\n",
            "Epoch: 0007 loss_train: 1.9003 acc_train: 0.2085 loss_val: 1.8542 acc_val: 0.2878\n",
            "Epoch: 0008 loss_train: 1.8636 acc_train: 0.2970 loss_val: 1.8106 acc_val: 0.4262\n",
            "Epoch: 0009 loss_train: 1.8236 acc_train: 0.4053 loss_val: 1.7632 acc_val: 0.5037\n",
            "Epoch: 0010 loss_train: 1.7797 acc_train: 0.5080 loss_val: 1.7125 acc_val: 0.5535\n",
            "Epoch: 0011 loss_train: 1.7315 acc_train: 0.5726 loss_val: 1.6588 acc_val: 0.5923\n",
            "Epoch: 0012 loss_train: 1.6807 acc_train: 0.5972 loss_val: 1.6026 acc_val: 0.6199\n",
            "Epoch: 0013 loss_train: 1.6264 acc_train: 0.6365 loss_val: 1.5444 acc_val: 0.6421\n",
            "Epoch: 0014 loss_train: 1.5697 acc_train: 0.6624 loss_val: 1.4843 acc_val: 0.6661\n",
            "Epoch: 0015 loss_train: 1.5124 acc_train: 0.6777 loss_val: 1.4228 acc_val: 0.6753\n",
            "Epoch: 0016 loss_train: 1.4504 acc_train: 0.7079 loss_val: 1.3604 acc_val: 0.6937\n",
            "Epoch: 0017 loss_train: 1.3868 acc_train: 0.7159 loss_val: 1.2978 acc_val: 0.6993\n",
            "Epoch: 0018 loss_train: 1.3263 acc_train: 0.7312 loss_val: 1.2360 acc_val: 0.7122\n",
            "Epoch: 0019 loss_train: 1.2619 acc_train: 0.7362 loss_val: 1.1759 acc_val: 0.7214\n",
            "Epoch: 0020 loss_train: 1.1993 acc_train: 0.7509 loss_val: 1.1186 acc_val: 0.7251\n",
            "Epoch: 0021 loss_train: 1.1368 acc_train: 0.7638 loss_val: 1.0649 acc_val: 0.7362\n",
            "Epoch: 0022 loss_train: 1.0763 acc_train: 0.7798 loss_val: 1.0152 acc_val: 0.7472\n",
            "Epoch: 0023 loss_train: 1.0217 acc_train: 0.7934 loss_val: 0.9696 acc_val: 0.7583\n",
            "Epoch: 0024 loss_train: 0.9681 acc_train: 0.8001 loss_val: 0.9274 acc_val: 0.7657\n",
            "Epoch: 0025 loss_train: 0.9197 acc_train: 0.8081 loss_val: 0.8881 acc_val: 0.7749\n",
            "Epoch: 0026 loss_train: 0.8716 acc_train: 0.8124 loss_val: 0.8514 acc_val: 0.7768\n",
            "Epoch: 0027 loss_train: 0.8288 acc_train: 0.8216 loss_val: 0.8169 acc_val: 0.7823\n",
            "Epoch: 0028 loss_train: 0.7861 acc_train: 0.8229 loss_val: 0.7847 acc_val: 0.7841\n",
            "Epoch: 0029 loss_train: 0.7494 acc_train: 0.8241 loss_val: 0.7545 acc_val: 0.7915\n",
            "Epoch: 0030 loss_train: 0.7134 acc_train: 0.8290 loss_val: 0.7265 acc_val: 0.7860\n",
            "Epoch: 0031 loss_train: 0.6814 acc_train: 0.8352 loss_val: 0.7003 acc_val: 0.7970\n",
            "Epoch: 0032 loss_train: 0.6522 acc_train: 0.8370 loss_val: 0.6757 acc_val: 0.8026\n",
            "Epoch: 0033 loss_train: 0.6244 acc_train: 0.8438 loss_val: 0.6526 acc_val: 0.8063\n",
            "Epoch: 0034 loss_train: 0.5980 acc_train: 0.8475 loss_val: 0.6311 acc_val: 0.8137\n",
            "Epoch: 0035 loss_train: 0.5720 acc_train: 0.8518 loss_val: 0.6109 acc_val: 0.8155\n",
            "Epoch: 0036 loss_train: 0.5492 acc_train: 0.8555 loss_val: 0.5919 acc_val: 0.8155\n",
            "Epoch: 0037 loss_train: 0.5250 acc_train: 0.8567 loss_val: 0.5732 acc_val: 0.8247\n",
            "Epoch: 0038 loss_train: 0.5052 acc_train: 0.8598 loss_val: 0.5544 acc_val: 0.8266\n",
            "Epoch: 0039 loss_train: 0.4835 acc_train: 0.8641 loss_val: 0.5359 acc_val: 0.8339\n",
            "Epoch: 0040 loss_train: 0.4648 acc_train: 0.8665 loss_val: 0.5186 acc_val: 0.8358\n",
            "Epoch: 0041 loss_train: 0.4434 acc_train: 0.8727 loss_val: 0.5029 acc_val: 0.8432\n",
            "Epoch: 0042 loss_train: 0.4256 acc_train: 0.8727 loss_val: 0.4889 acc_val: 0.8487\n",
            "Epoch: 0043 loss_train: 0.4085 acc_train: 0.8788 loss_val: 0.4760 acc_val: 0.8542\n",
            "Epoch: 0044 loss_train: 0.3884 acc_train: 0.8899 loss_val: 0.4631 acc_val: 0.8616\n",
            "Epoch: 0045 loss_train: 0.3721 acc_train: 0.8961 loss_val: 0.4497 acc_val: 0.8616\n",
            "Epoch: 0046 loss_train: 0.3550 acc_train: 0.8967 loss_val: 0.4365 acc_val: 0.8635\n",
            "Epoch: 0047 loss_train: 0.3392 acc_train: 0.9004 loss_val: 0.4243 acc_val: 0.8690\n",
            "Epoch: 0048 loss_train: 0.3206 acc_train: 0.9034 loss_val: 0.4139 acc_val: 0.8745\n",
            "Epoch: 0049 loss_train: 0.3039 acc_train: 0.9096 loss_val: 0.4048 acc_val: 0.8801\n",
            "Epoch: 0050 loss_train: 0.2882 acc_train: 0.9127 loss_val: 0.3970 acc_val: 0.8838\n",
            "Epoch: 0051 loss_train: 0.2723 acc_train: 0.9164 loss_val: 0.3899 acc_val: 0.8801\n",
            "Epoch: 0052 loss_train: 0.2551 acc_train: 0.9225 loss_val: 0.3819 acc_val: 0.8819\n",
            "Epoch: 0053 loss_train: 0.2399 acc_train: 0.9293 loss_val: 0.3762 acc_val: 0.8856\n",
            "Epoch: 0054 loss_train: 0.2236 acc_train: 0.9342 loss_val: 0.3753 acc_val: 0.8893\n",
            "Epoch: 0055 loss_train: 0.2083 acc_train: 0.9422 loss_val: 0.3767 acc_val: 0.8893\n",
            "Epoch: 0056 loss_train: 0.1905 acc_train: 0.9483 loss_val: 0.3782 acc_val: 0.8875\n",
            "Epoch: 0057 loss_train: 0.1783 acc_train: 0.9545 loss_val: 0.3788 acc_val: 0.8819\n",
            "Epoch: 0058 loss_train: 0.1649 acc_train: 0.9539 loss_val: 0.3804 acc_val: 0.8856\n",
            "Epoch: 0059 loss_train: 0.1512 acc_train: 0.9582 loss_val: 0.3842 acc_val: 0.8856\n",
            "Epoch: 0060 loss_train: 0.1372 acc_train: 0.9668 loss_val: 0.3896 acc_val: 0.8838\n",
            "Epoch: 0061 loss_train: 0.1244 acc_train: 0.9692 loss_val: 0.3956 acc_val: 0.8782\n",
            "Epoch: 0062 loss_train: 0.1121 acc_train: 0.9742 loss_val: 0.4012 acc_val: 0.8764\n",
            "Epoch: 0063 loss_train: 0.1026 acc_train: 0.9785 loss_val: 0.4061 acc_val: 0.8745\n",
            "Epoch: 0064 loss_train: 0.0937 acc_train: 0.9828 loss_val: 0.4105 acc_val: 0.8782\n",
            "Epoch: 0065 loss_train: 0.0850 acc_train: 0.9852 loss_val: 0.4148 acc_val: 0.8782\n",
            "Epoch: 0066 loss_train: 0.0775 acc_train: 0.9852 loss_val: 0.4203 acc_val: 0.8782\n",
            "Epoch: 0067 loss_train: 0.0717 acc_train: 0.9865 loss_val: 0.4283 acc_val: 0.8745\n",
            "Epoch: 0068 loss_train: 0.0648 acc_train: 0.9883 loss_val: 0.4379 acc_val: 0.8745\n",
            "Epoch: 0069 loss_train: 0.0603 acc_train: 0.9889 loss_val: 0.4474 acc_val: 0.8782\n",
            "Epoch: 0070 loss_train: 0.0540 acc_train: 0.9889 loss_val: 0.4565 acc_val: 0.8764\n",
            "Epoch: 0071 loss_train: 0.0490 acc_train: 0.9914 loss_val: 0.4651 acc_val: 0.8727\n",
            "Epoch: 0072 loss_train: 0.0440 acc_train: 0.9920 loss_val: 0.4747 acc_val: 0.8708\n",
            "Epoch: 0073 loss_train: 0.0403 acc_train: 0.9926 loss_val: 0.4847 acc_val: 0.8708\n",
            "Epoch: 0074 loss_train: 0.0350 acc_train: 0.9938 loss_val: 0.4948 acc_val: 0.8653\n",
            "Epoch: 0075 loss_train: 0.0322 acc_train: 0.9938 loss_val: 0.5044 acc_val: 0.8616\n",
            "Epoch: 0076 loss_train: 0.0283 acc_train: 0.9945 loss_val: 0.5144 acc_val: 0.8635\n",
            "Epoch: 0077 loss_train: 0.0267 acc_train: 0.9957 loss_val: 0.5243 acc_val: 0.8616\n",
            "Epoch: 0078 loss_train: 0.0228 acc_train: 0.9963 loss_val: 0.5325 acc_val: 0.8616\n",
            "Epoch: 0079 loss_train: 0.0214 acc_train: 0.9963 loss_val: 0.5394 acc_val: 0.8616\n",
            "Epoch: 0080 loss_train: 0.0188 acc_train: 0.9969 loss_val: 0.5459 acc_val: 0.8616\n",
            "Epoch: 0081 loss_train: 0.0170 acc_train: 0.9975 loss_val: 0.5535 acc_val: 0.8635\n",
            "Epoch: 0082 loss_train: 0.0150 acc_train: 0.9994 loss_val: 0.5613 acc_val: 0.8635\n",
            "Epoch: 0083 loss_train: 0.0139 acc_train: 0.9982 loss_val: 0.5685 acc_val: 0.8635\n",
            "Epoch: 0084 loss_train: 0.0125 acc_train: 0.9988 loss_val: 0.5759 acc_val: 0.8616\n",
            "Epoch: 0085 loss_train: 0.0121 acc_train: 0.9988 loss_val: 0.5828 acc_val: 0.8598\n",
            "Epoch: 0086 loss_train: 0.0108 acc_train: 0.9988 loss_val: 0.5896 acc_val: 0.8616\n",
            "Epoch: 0087 loss_train: 0.0097 acc_train: 0.9988 loss_val: 0.5964 acc_val: 0.8598\n",
            "Epoch: 0088 loss_train: 0.0090 acc_train: 0.9994 loss_val: 0.6026 acc_val: 0.8598\n",
            "Epoch: 0089 loss_train: 0.0078 acc_train: 1.0000 loss_val: 0.6088 acc_val: 0.8579\n",
            "Epoch: 0090 loss_train: 0.0073 acc_train: 1.0000 loss_val: 0.6143 acc_val: 0.8561\n",
            "Epoch: 0091 loss_train: 0.0070 acc_train: 1.0000 loss_val: 0.6196 acc_val: 0.8561\n",
            "Epoch: 0092 loss_train: 0.0063 acc_train: 1.0000 loss_val: 0.6251 acc_val: 0.8542\n",
            "Epoch: 0093 loss_train: 0.0059 acc_train: 0.9994 loss_val: 0.6312 acc_val: 0.8561\n",
            "Epoch: 0094 loss_train: 0.0055 acc_train: 1.0000 loss_val: 0.6369 acc_val: 0.8542\n",
            "Epoch: 0095 loss_train: 0.0047 acc_train: 1.0000 loss_val: 0.6426 acc_val: 0.8542\n",
            "Epoch: 0096 loss_train: 0.0046 acc_train: 1.0000 loss_val: 0.6476 acc_val: 0.8598\n",
            "Epoch: 0097 loss_train: 0.0043 acc_train: 1.0000 loss_val: 0.6522 acc_val: 0.8616\n",
            "Epoch: 0098 loss_train: 0.0040 acc_train: 1.0000 loss_val: 0.6559 acc_val: 0.8635\n",
            "Epoch: 0099 loss_train: 0.0035 acc_train: 1.0000 loss_val: 0.6590 acc_val: 0.8635\n",
            "Epoch: 0100 loss_train: 0.0035 acc_train: 1.0000 loss_val: 0.6617 acc_val: 0.8635\n",
            "Epoch: 0101 loss_train: 0.0032 acc_train: 1.0000 loss_val: 0.6646 acc_val: 0.8635\n",
            "Epoch: 0102 loss_train: 0.0034 acc_train: 0.9994 loss_val: 0.6688 acc_val: 0.8635\n",
            "Epoch: 0103 loss_train: 0.0029 acc_train: 1.0000 loss_val: 0.6728 acc_val: 0.8635\n",
            "Epoch: 0104 loss_train: 0.0027 acc_train: 1.0000 loss_val: 0.6770 acc_val: 0.8579\n",
            "Epoch: 0105 loss_train: 0.0025 acc_train: 1.0000 loss_val: 0.6813 acc_val: 0.8561\n",
            "Optimization Finished!\n",
            "Train cost: 14.4970s\n",
            "Loading 54th epoch\n",
            "Test set results: loss= 0.3180 accuracy= 0.9093\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --dataset citeseer --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 5  --n_heads 8 --n_layers 1 --pe_dim 5 --peak_lr 0.001  --attack none --attperc 20 --TrainTest train --weight_decay=1e-05\n",
        "\n",
        "!python train.py --dataset citeseer --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 5  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack none --attperc 20 --TrainTest train --weight_decay=1e-05\n",
        "\n",
        "!python train.py --dataset citeseer --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 5  --n_heads 8 --n_layers 1 --pe_dim 40 --peak_lr 0.001  --attack none --attperc 20 --TrainTest train --weight_decay=1e-05\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yicZ-MtbWHN-",
        "outputId": "e0c7de68-0cf2-466f-8aea-2ebd7ac514b6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NumNodes: 3327\n",
            "  NumEdges: 9228\n",
            "  NumFeats: 3703\n",
            "  NumClasses: 6\n",
            "  NumTrainingSamples: 120\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=3708, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=6, bias=True)\n",
            ")\n",
            "total params: 4136712\n",
            "Epoch: 0001 loss_train: 1.7955 acc_train: 0.1517 loss_val: 1.7926 acc_val: 0.1517\n",
            "Epoch: 0002 loss_train: 1.7893 acc_train: 0.1482 loss_val: 1.7816 acc_val: 0.1637\n",
            "Epoch: 0003 loss_train: 1.7788 acc_train: 0.1788 loss_val: 1.7653 acc_val: 0.1997\n",
            "Epoch: 0004 loss_train: 1.7641 acc_train: 0.1978 loss_val: 1.7439 acc_val: 0.2808\n",
            "Epoch: 0005 loss_train: 1.7448 acc_train: 0.2604 loss_val: 1.7179 acc_val: 0.3649\n",
            "Epoch: 0006 loss_train: 1.7195 acc_train: 0.3545 loss_val: 1.6876 acc_val: 0.4474\n",
            "Epoch: 0007 loss_train: 1.6900 acc_train: 0.4306 loss_val: 1.6538 acc_val: 0.5045\n",
            "Epoch: 0008 loss_train: 1.6565 acc_train: 0.4972 loss_val: 1.6170 acc_val: 0.5556\n",
            "Epoch: 0009 loss_train: 1.6238 acc_train: 0.5408 loss_val: 1.5779 acc_val: 0.5751\n",
            "Epoch: 0010 loss_train: 1.5836 acc_train: 0.5618 loss_val: 1.5372 acc_val: 0.5931\n",
            "Epoch: 0011 loss_train: 1.5460 acc_train: 0.5729 loss_val: 1.4956 acc_val: 0.5976\n",
            "Epoch: 0012 loss_train: 1.5031 acc_train: 0.5979 loss_val: 1.4536 acc_val: 0.6006\n",
            "Epoch: 0013 loss_train: 1.4621 acc_train: 0.6119 loss_val: 1.4114 acc_val: 0.6171\n",
            "Epoch: 0014 loss_train: 1.4162 acc_train: 0.6365 loss_val: 1.3695 acc_val: 0.6276\n",
            "Epoch: 0015 loss_train: 1.3751 acc_train: 0.6400 loss_val: 1.3282 acc_val: 0.6336\n",
            "Epoch: 0016 loss_train: 1.3318 acc_train: 0.6535 loss_val: 1.2877 acc_val: 0.6411\n",
            "Epoch: 0017 loss_train: 1.2891 acc_train: 0.6635 loss_val: 1.2484 acc_val: 0.6471\n",
            "Epoch: 0018 loss_train: 1.2478 acc_train: 0.6760 loss_val: 1.2105 acc_val: 0.6517\n",
            "Epoch: 0019 loss_train: 1.2051 acc_train: 0.6745 loss_val: 1.1739 acc_val: 0.6577\n",
            "Epoch: 0020 loss_train: 1.1652 acc_train: 0.6835 loss_val: 1.1387 acc_val: 0.6697\n",
            "Epoch: 0021 loss_train: 1.1248 acc_train: 0.6890 loss_val: 1.1048 acc_val: 0.6787\n",
            "Epoch: 0022 loss_train: 1.0874 acc_train: 0.6995 loss_val: 1.0722 acc_val: 0.6862\n",
            "Epoch: 0023 loss_train: 1.0473 acc_train: 0.7066 loss_val: 1.0410 acc_val: 0.6922\n",
            "Epoch: 0024 loss_train: 1.0097 acc_train: 0.7126 loss_val: 1.0114 acc_val: 0.6937\n",
            "Epoch: 0025 loss_train: 0.9746 acc_train: 0.7141 loss_val: 0.9835 acc_val: 0.6937\n",
            "Epoch: 0026 loss_train: 0.9394 acc_train: 0.7226 loss_val: 0.9575 acc_val: 0.6937\n",
            "Epoch: 0027 loss_train: 0.9060 acc_train: 0.7291 loss_val: 0.9337 acc_val: 0.6967\n",
            "Epoch: 0028 loss_train: 0.8722 acc_train: 0.7386 loss_val: 0.9123 acc_val: 0.6997\n",
            "Epoch: 0029 loss_train: 0.8390 acc_train: 0.7451 loss_val: 0.8932 acc_val: 0.7027\n",
            "Epoch: 0030 loss_train: 0.8101 acc_train: 0.7531 loss_val: 0.8765 acc_val: 0.7042\n",
            "Epoch: 0031 loss_train: 0.7794 acc_train: 0.7601 loss_val: 0.8621 acc_val: 0.7117\n",
            "Epoch: 0032 loss_train: 0.7513 acc_train: 0.7682 loss_val: 0.8502 acc_val: 0.7162\n",
            "Epoch: 0033 loss_train: 0.7238 acc_train: 0.7727 loss_val: 0.8406 acc_val: 0.7222\n",
            "Epoch: 0034 loss_train: 0.6975 acc_train: 0.7792 loss_val: 0.8330 acc_val: 0.7252\n",
            "Epoch: 0035 loss_train: 0.6748 acc_train: 0.7822 loss_val: 0.8271 acc_val: 0.7297\n",
            "Epoch: 0036 loss_train: 0.6473 acc_train: 0.7897 loss_val: 0.8229 acc_val: 0.7312\n",
            "Epoch: 0037 loss_train: 0.6253 acc_train: 0.7937 loss_val: 0.8201 acc_val: 0.7282\n",
            "Epoch: 0038 loss_train: 0.6028 acc_train: 0.8052 loss_val: 0.8190 acc_val: 0.7282\n",
            "Epoch: 0039 loss_train: 0.5813 acc_train: 0.8157 loss_val: 0.8194 acc_val: 0.7312\n",
            "Epoch: 0040 loss_train: 0.5584 acc_train: 0.8207 loss_val: 0.8215 acc_val: 0.7387\n",
            "Epoch: 0041 loss_train: 0.5358 acc_train: 0.8272 loss_val: 0.8256 acc_val: 0.7387\n",
            "Epoch: 0042 loss_train: 0.5154 acc_train: 0.8317 loss_val: 0.8318 acc_val: 0.7402\n",
            "Epoch: 0043 loss_train: 0.4949 acc_train: 0.8408 loss_val: 0.8399 acc_val: 0.7372\n",
            "Epoch: 0044 loss_train: 0.4758 acc_train: 0.8418 loss_val: 0.8493 acc_val: 0.7372\n",
            "Epoch: 0045 loss_train: 0.4575 acc_train: 0.8418 loss_val: 0.8597 acc_val: 0.7342\n",
            "Epoch: 0046 loss_train: 0.4383 acc_train: 0.8468 loss_val: 0.8709 acc_val: 0.7342\n",
            "Epoch: 0047 loss_train: 0.4206 acc_train: 0.8543 loss_val: 0.8834 acc_val: 0.7357\n",
            "Epoch: 0048 loss_train: 0.4026 acc_train: 0.8603 loss_val: 0.8973 acc_val: 0.7357\n",
            "Epoch: 0049 loss_train: 0.3841 acc_train: 0.8693 loss_val: 0.9120 acc_val: 0.7342\n",
            "Epoch: 0050 loss_train: 0.3681 acc_train: 0.8728 loss_val: 0.9276 acc_val: 0.7297\n",
            "Epoch: 0051 loss_train: 0.3492 acc_train: 0.8793 loss_val: 0.9432 acc_val: 0.7222\n",
            "Epoch: 0052 loss_train: 0.3334 acc_train: 0.8848 loss_val: 0.9591 acc_val: 0.7192\n",
            "Epoch: 0053 loss_train: 0.3164 acc_train: 0.8903 loss_val: 0.9754 acc_val: 0.7237\n",
            "Epoch: 0054 loss_train: 0.2970 acc_train: 0.8988 loss_val: 0.9939 acc_val: 0.7252\n",
            "Epoch: 0055 loss_train: 0.2793 acc_train: 0.9064 loss_val: 1.0150 acc_val: 0.7237\n",
            "Epoch: 0056 loss_train: 0.2634 acc_train: 0.9094 loss_val: 1.0389 acc_val: 0.7237\n",
            "Epoch: 0057 loss_train: 0.2496 acc_train: 0.9129 loss_val: 1.0641 acc_val: 0.7267\n",
            "Epoch: 0058 loss_train: 0.2307 acc_train: 0.9194 loss_val: 1.0905 acc_val: 0.7237\n",
            "Epoch: 0059 loss_train: 0.2170 acc_train: 0.9294 loss_val: 1.1201 acc_val: 0.7207\n",
            "Epoch: 0060 loss_train: 0.1998 acc_train: 0.9374 loss_val: 1.1508 acc_val: 0.7177\n",
            "Epoch: 0061 loss_train: 0.1852 acc_train: 0.9409 loss_val: 1.1796 acc_val: 0.7222\n",
            "Epoch: 0062 loss_train: 0.1728 acc_train: 0.9489 loss_val: 1.2073 acc_val: 0.7312\n",
            "Epoch: 0063 loss_train: 0.1562 acc_train: 0.9519 loss_val: 1.2364 acc_val: 0.7297\n",
            "Epoch: 0064 loss_train: 0.1431 acc_train: 0.9584 loss_val: 1.2649 acc_val: 0.7267\n",
            "Epoch: 0065 loss_train: 0.1308 acc_train: 0.9599 loss_val: 1.2909 acc_val: 0.7267\n",
            "Epoch: 0066 loss_train: 0.1168 acc_train: 0.9670 loss_val: 1.3186 acc_val: 0.7267\n",
            "Epoch: 0067 loss_train: 0.1043 acc_train: 0.9705 loss_val: 1.3463 acc_val: 0.7222\n",
            "Epoch: 0068 loss_train: 0.0935 acc_train: 0.9770 loss_val: 1.3743 acc_val: 0.7222\n",
            "Epoch: 0069 loss_train: 0.0846 acc_train: 0.9815 loss_val: 1.4017 acc_val: 0.7192\n",
            "Epoch: 0070 loss_train: 0.0755 acc_train: 0.9810 loss_val: 1.4272 acc_val: 0.7222\n",
            "Epoch: 0071 loss_train: 0.0685 acc_train: 0.9865 loss_val: 1.4527 acc_val: 0.7207\n",
            "Epoch: 0072 loss_train: 0.0635 acc_train: 0.9845 loss_val: 1.4792 acc_val: 0.7222\n",
            "Epoch: 0073 loss_train: 0.0575 acc_train: 0.9830 loss_val: 1.5053 acc_val: 0.7252\n",
            "Epoch: 0074 loss_train: 0.0523 acc_train: 0.9860 loss_val: 1.5280 acc_val: 0.7267\n",
            "Epoch: 0075 loss_train: 0.0482 acc_train: 0.9895 loss_val: 1.5510 acc_val: 0.7237\n",
            "Epoch: 0076 loss_train: 0.0432 acc_train: 0.9880 loss_val: 1.5789 acc_val: 0.7237\n",
            "Epoch: 0077 loss_train: 0.0406 acc_train: 0.9890 loss_val: 1.6073 acc_val: 0.7252\n",
            "Epoch: 0078 loss_train: 0.0372 acc_train: 0.9885 loss_val: 1.6299 acc_val: 0.7282\n",
            "Epoch: 0079 loss_train: 0.0347 acc_train: 0.9905 loss_val: 1.6481 acc_val: 0.7267\n",
            "Epoch: 0080 loss_train: 0.0325 acc_train: 0.9900 loss_val: 1.6706 acc_val: 0.7282\n",
            "Epoch: 0081 loss_train: 0.0300 acc_train: 0.9910 loss_val: 1.6956 acc_val: 0.7297\n",
            "Epoch: 0082 loss_train: 0.0276 acc_train: 0.9925 loss_val: 1.7145 acc_val: 0.7267\n",
            "Epoch: 0083 loss_train: 0.0255 acc_train: 0.9920 loss_val: 1.7319 acc_val: 0.7267\n",
            "Epoch: 0084 loss_train: 0.0239 acc_train: 0.9935 loss_val: 1.7498 acc_val: 0.7267\n",
            "Epoch: 0085 loss_train: 0.0226 acc_train: 0.9920 loss_val: 1.7678 acc_val: 0.7282\n",
            "Epoch: 0086 loss_train: 0.0211 acc_train: 0.9955 loss_val: 1.7840 acc_val: 0.7312\n",
            "Epoch: 0087 loss_train: 0.0196 acc_train: 0.9935 loss_val: 1.8020 acc_val: 0.7312\n",
            "Epoch: 0088 loss_train: 0.0186 acc_train: 0.9950 loss_val: 1.8138 acc_val: 0.7327\n",
            "Epoch: 0089 loss_train: 0.0161 acc_train: 0.9970 loss_val: 1.8279 acc_val: 0.7297\n",
            "Epoch: 0090 loss_train: 0.0157 acc_train: 0.9965 loss_val: 1.8439 acc_val: 0.7312\n",
            "Epoch: 0091 loss_train: 0.0151 acc_train: 0.9960 loss_val: 1.8634 acc_val: 0.7297\n",
            "Epoch: 0092 loss_train: 0.0136 acc_train: 0.9985 loss_val: 1.8765 acc_val: 0.7282\n",
            "Optimization Finished!\n",
            "Train cost: 29.1511s\n",
            "Loading 42th epoch\n",
            "Test set results: loss= 0.6989 accuracy= 0.7831\n",
            "  NumNodes: 3327\n",
            "  NumEdges: 9228\n",
            "  NumFeats: 3703\n",
            "  NumClasses: 6\n",
            "  NumTrainingSamples: 120\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=3718, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=6, bias=True)\n",
            ")\n",
            "total params: 4141832\n",
            "Epoch: 0001 loss_train: 1.8160 acc_train: 0.1337 loss_val: 1.8150 acc_val: 0.1231\n",
            "Epoch: 0002 loss_train: 1.8117 acc_train: 0.1322 loss_val: 1.8035 acc_val: 0.1336\n",
            "Epoch: 0003 loss_train: 1.8011 acc_train: 0.1402 loss_val: 1.7865 acc_val: 0.1426\n",
            "Epoch: 0004 loss_train: 1.7845 acc_train: 0.1617 loss_val: 1.7644 acc_val: 0.1832\n",
            "Epoch: 0005 loss_train: 1.7631 acc_train: 0.2013 loss_val: 1.7376 acc_val: 0.2628\n",
            "Epoch: 0006 loss_train: 1.7378 acc_train: 0.2644 loss_val: 1.7066 acc_val: 0.3664\n",
            "Epoch: 0007 loss_train: 1.7095 acc_train: 0.3535 loss_val: 1.6723 acc_val: 0.4505\n",
            "Epoch: 0008 loss_train: 1.6756 acc_train: 0.4402 loss_val: 1.6353 acc_val: 0.4985\n",
            "Epoch: 0009 loss_train: 1.6403 acc_train: 0.4992 loss_val: 1.5966 acc_val: 0.5526\n",
            "Epoch: 0010 loss_train: 1.6017 acc_train: 0.5458 loss_val: 1.5571 acc_val: 0.5676\n",
            "Epoch: 0011 loss_train: 1.5628 acc_train: 0.5714 loss_val: 1.5172 acc_val: 0.5676\n",
            "Epoch: 0012 loss_train: 1.5217 acc_train: 0.5859 loss_val: 1.4771 acc_val: 0.5841\n",
            "Epoch: 0013 loss_train: 1.4837 acc_train: 0.6024 loss_val: 1.4368 acc_val: 0.5916\n",
            "Epoch: 0014 loss_train: 1.4411 acc_train: 0.6119 loss_val: 1.3962 acc_val: 0.6021\n",
            "Epoch: 0015 loss_train: 1.3992 acc_train: 0.6284 loss_val: 1.3558 acc_val: 0.6096\n",
            "Epoch: 0016 loss_train: 1.3564 acc_train: 0.6340 loss_val: 1.3157 acc_val: 0.6276\n",
            "Epoch: 0017 loss_train: 1.3127 acc_train: 0.6465 loss_val: 1.2760 acc_val: 0.6486\n",
            "Epoch: 0018 loss_train: 1.2719 acc_train: 0.6610 loss_val: 1.2370 acc_val: 0.6577\n",
            "Epoch: 0019 loss_train: 1.2290 acc_train: 0.6780 loss_val: 1.1989 acc_val: 0.6592\n",
            "Epoch: 0020 loss_train: 1.1875 acc_train: 0.6835 loss_val: 1.1618 acc_val: 0.6682\n",
            "Epoch: 0021 loss_train: 1.1446 acc_train: 0.6975 loss_val: 1.1261 acc_val: 0.6697\n",
            "Epoch: 0022 loss_train: 1.1065 acc_train: 0.6990 loss_val: 1.0921 acc_val: 0.6727\n",
            "Epoch: 0023 loss_train: 1.0657 acc_train: 0.7121 loss_val: 1.0601 acc_val: 0.6847\n",
            "Epoch: 0024 loss_train: 1.0285 acc_train: 0.7141 loss_val: 1.0302 acc_val: 0.6847\n",
            "Epoch: 0025 loss_train: 0.9924 acc_train: 0.7181 loss_val: 1.0026 acc_val: 0.6907\n",
            "Epoch: 0026 loss_train: 0.9561 acc_train: 0.7221 loss_val: 0.9773 acc_val: 0.6997\n",
            "Epoch: 0027 loss_train: 0.9214 acc_train: 0.7326 loss_val: 0.9539 acc_val: 0.7012\n",
            "Epoch: 0028 loss_train: 0.8885 acc_train: 0.7361 loss_val: 0.9325 acc_val: 0.7057\n",
            "Epoch: 0029 loss_train: 0.8584 acc_train: 0.7431 loss_val: 0.9131 acc_val: 0.7132\n",
            "Epoch: 0030 loss_train: 0.8271 acc_train: 0.7536 loss_val: 0.8957 acc_val: 0.7132\n",
            "Epoch: 0031 loss_train: 0.7984 acc_train: 0.7551 loss_val: 0.8800 acc_val: 0.7147\n",
            "Epoch: 0032 loss_train: 0.7682 acc_train: 0.7651 loss_val: 0.8661 acc_val: 0.7192\n",
            "Epoch: 0033 loss_train: 0.7403 acc_train: 0.7742 loss_val: 0.8538 acc_val: 0.7177\n",
            "Epoch: 0034 loss_train: 0.7152 acc_train: 0.7807 loss_val: 0.8432 acc_val: 0.7177\n",
            "Epoch: 0035 loss_train: 0.6884 acc_train: 0.7847 loss_val: 0.8342 acc_val: 0.7192\n",
            "Epoch: 0036 loss_train: 0.6631 acc_train: 0.7932 loss_val: 0.8271 acc_val: 0.7222\n",
            "Epoch: 0037 loss_train: 0.6387 acc_train: 0.7982 loss_val: 0.8217 acc_val: 0.7267\n",
            "Epoch: 0038 loss_train: 0.6150 acc_train: 0.8042 loss_val: 0.8180 acc_val: 0.7252\n",
            "Epoch: 0039 loss_train: 0.5945 acc_train: 0.8077 loss_val: 0.8161 acc_val: 0.7267\n",
            "Epoch: 0040 loss_train: 0.5721 acc_train: 0.8162 loss_val: 0.8159 acc_val: 0.7342\n",
            "Epoch: 0041 loss_train: 0.5482 acc_train: 0.8192 loss_val: 0.8174 acc_val: 0.7357\n",
            "Epoch: 0042 loss_train: 0.5262 acc_train: 0.8242 loss_val: 0.8208 acc_val: 0.7402\n",
            "Epoch: 0043 loss_train: 0.5055 acc_train: 0.8307 loss_val: 0.8260 acc_val: 0.7402\n",
            "Epoch: 0044 loss_train: 0.4866 acc_train: 0.8373 loss_val: 0.8322 acc_val: 0.7417\n",
            "Epoch: 0045 loss_train: 0.4651 acc_train: 0.8398 loss_val: 0.8394 acc_val: 0.7417\n",
            "Epoch: 0046 loss_train: 0.4493 acc_train: 0.8438 loss_val: 0.8479 acc_val: 0.7402\n",
            "Epoch: 0047 loss_train: 0.4313 acc_train: 0.8503 loss_val: 0.8580 acc_val: 0.7387\n",
            "Epoch: 0048 loss_train: 0.4151 acc_train: 0.8498 loss_val: 0.8695 acc_val: 0.7372\n",
            "Epoch: 0049 loss_train: 0.3986 acc_train: 0.8588 loss_val: 0.8814 acc_val: 0.7387\n",
            "Epoch: 0050 loss_train: 0.3795 acc_train: 0.8663 loss_val: 0.8928 acc_val: 0.7372\n",
            "Epoch: 0051 loss_train: 0.3618 acc_train: 0.8773 loss_val: 0.9051 acc_val: 0.7387\n",
            "Epoch: 0052 loss_train: 0.3460 acc_train: 0.8768 loss_val: 0.9196 acc_val: 0.7417\n",
            "Epoch: 0053 loss_train: 0.3297 acc_train: 0.8868 loss_val: 0.9352 acc_val: 0.7462\n",
            "Epoch: 0054 loss_train: 0.3097 acc_train: 0.8918 loss_val: 0.9509 acc_val: 0.7432\n",
            "Epoch: 0055 loss_train: 0.2957 acc_train: 0.8948 loss_val: 0.9676 acc_val: 0.7417\n",
            "Epoch: 0056 loss_train: 0.2788 acc_train: 0.8958 loss_val: 0.9865 acc_val: 0.7402\n",
            "Epoch: 0057 loss_train: 0.2623 acc_train: 0.9084 loss_val: 1.0086 acc_val: 0.7342\n",
            "Epoch: 0058 loss_train: 0.2470 acc_train: 0.9184 loss_val: 1.0314 acc_val: 0.7372\n",
            "Epoch: 0059 loss_train: 0.2307 acc_train: 0.9244 loss_val: 1.0557 acc_val: 0.7387\n",
            "Epoch: 0060 loss_train: 0.2169 acc_train: 0.9244 loss_val: 1.0818 acc_val: 0.7297\n",
            "Epoch: 0061 loss_train: 0.1990 acc_train: 0.9354 loss_val: 1.1075 acc_val: 0.7297\n",
            "Epoch: 0062 loss_train: 0.1862 acc_train: 0.9419 loss_val: 1.1342 acc_val: 0.7222\n",
            "Epoch: 0063 loss_train: 0.1721 acc_train: 0.9484 loss_val: 1.1625 acc_val: 0.7162\n",
            "Epoch: 0064 loss_train: 0.1577 acc_train: 0.9514 loss_val: 1.1897 acc_val: 0.7192\n",
            "Epoch: 0065 loss_train: 0.1443 acc_train: 0.9554 loss_val: 1.2133 acc_val: 0.7192\n",
            "Epoch: 0066 loss_train: 0.1314 acc_train: 0.9599 loss_val: 1.2422 acc_val: 0.7207\n",
            "Epoch: 0067 loss_train: 0.1196 acc_train: 0.9664 loss_val: 1.2716 acc_val: 0.7237\n",
            "Epoch: 0068 loss_train: 0.1080 acc_train: 0.9675 loss_val: 1.2992 acc_val: 0.7222\n",
            "Epoch: 0069 loss_train: 0.0970 acc_train: 0.9735 loss_val: 1.3257 acc_val: 0.7192\n",
            "Epoch: 0070 loss_train: 0.0892 acc_train: 0.9785 loss_val: 1.3585 acc_val: 0.7237\n",
            "Epoch: 0071 loss_train: 0.0816 acc_train: 0.9790 loss_val: 1.3900 acc_val: 0.7192\n",
            "Epoch: 0072 loss_train: 0.0730 acc_train: 0.9790 loss_val: 1.4157 acc_val: 0.7162\n",
            "Epoch: 0073 loss_train: 0.0662 acc_train: 0.9840 loss_val: 1.4437 acc_val: 0.7177\n",
            "Epoch: 0074 loss_train: 0.0590 acc_train: 0.9860 loss_val: 1.4726 acc_val: 0.7192\n",
            "Epoch: 0075 loss_train: 0.0539 acc_train: 0.9885 loss_val: 1.4985 acc_val: 0.7207\n",
            "Epoch: 0076 loss_train: 0.0499 acc_train: 0.9880 loss_val: 1.5233 acc_val: 0.7192\n",
            "Epoch: 0077 loss_train: 0.0457 acc_train: 0.9870 loss_val: 1.5496 acc_val: 0.7177\n",
            "Epoch: 0078 loss_train: 0.0412 acc_train: 0.9900 loss_val: 1.5766 acc_val: 0.7192\n",
            "Epoch: 0079 loss_train: 0.0387 acc_train: 0.9870 loss_val: 1.6022 acc_val: 0.7177\n",
            "Epoch: 0080 loss_train: 0.0348 acc_train: 0.9905 loss_val: 1.6276 acc_val: 0.7192\n",
            "Epoch: 0081 loss_train: 0.0313 acc_train: 0.9920 loss_val: 1.6537 acc_val: 0.7192\n",
            "Epoch: 0082 loss_train: 0.0293 acc_train: 0.9925 loss_val: 1.6810 acc_val: 0.7177\n",
            "Epoch: 0083 loss_train: 0.0266 acc_train: 0.9915 loss_val: 1.7071 acc_val: 0.7192\n",
            "Epoch: 0084 loss_train: 0.0262 acc_train: 0.9935 loss_val: 1.7255 acc_val: 0.7177\n",
            "Epoch: 0085 loss_train: 0.0241 acc_train: 0.9925 loss_val: 1.7460 acc_val: 0.7207\n",
            "Epoch: 0086 loss_train: 0.0220 acc_train: 0.9940 loss_val: 1.7704 acc_val: 0.7207\n",
            "Epoch: 0087 loss_train: 0.0210 acc_train: 0.9920 loss_val: 1.7904 acc_val: 0.7222\n",
            "Epoch: 0088 loss_train: 0.0193 acc_train: 0.9935 loss_val: 1.8073 acc_val: 0.7207\n",
            "Epoch: 0089 loss_train: 0.0186 acc_train: 0.9940 loss_val: 1.8269 acc_val: 0.7282\n",
            "Epoch: 0090 loss_train: 0.0167 acc_train: 0.9965 loss_val: 1.8458 acc_val: 0.7267\n",
            "Epoch: 0091 loss_train: 0.0163 acc_train: 0.9945 loss_val: 1.8642 acc_val: 0.7177\n",
            "Epoch: 0092 loss_train: 0.0152 acc_train: 0.9950 loss_val: 1.8794 acc_val: 0.7177\n",
            "Epoch: 0093 loss_train: 0.0132 acc_train: 0.9965 loss_val: 1.8923 acc_val: 0.7297\n",
            "Epoch: 0094 loss_train: 0.0132 acc_train: 0.9950 loss_val: 1.9015 acc_val: 0.7252\n",
            "Epoch: 0095 loss_train: 0.0114 acc_train: 0.9965 loss_val: 1.9126 acc_val: 0.7177\n",
            "Epoch: 0096 loss_train: 0.0115 acc_train: 0.9955 loss_val: 1.9286 acc_val: 0.7252\n",
            "Epoch: 0097 loss_train: 0.0091 acc_train: 0.9980 loss_val: 1.9427 acc_val: 0.7267\n",
            "Epoch: 0098 loss_train: 0.0083 acc_train: 0.9985 loss_val: 1.9557 acc_val: 0.7252\n",
            "Epoch: 0099 loss_train: 0.0077 acc_train: 0.9985 loss_val: 1.9687 acc_val: 0.7237\n",
            "Epoch: 0100 loss_train: 0.0075 acc_train: 0.9990 loss_val: 1.9806 acc_val: 0.7252\n",
            "Epoch: 0101 loss_train: 0.0068 acc_train: 0.9980 loss_val: 1.9913 acc_val: 0.7267\n",
            "Epoch: 0102 loss_train: 0.0064 acc_train: 0.9990 loss_val: 2.0013 acc_val: 0.7252\n",
            "Epoch: 0103 loss_train: 0.0061 acc_train: 0.9990 loss_val: 2.0115 acc_val: 0.7222\n",
            "Optimization Finished!\n",
            "Train cost: 33.4670s\n",
            "Loading 53th epoch\n",
            "Test set results: loss= 0.7487 accuracy= 0.7831\n",
            "  NumNodes: 3327\n",
            "  NumEdges: 9228\n",
            "  NumFeats: 3703\n",
            "  NumClasses: 6\n",
            "  NumTrainingSamples: 120\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=3743, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=6, bias=True)\n",
            ")\n",
            "total params: 4154632\n",
            "Epoch: 0001 loss_train: 1.8342 acc_train: 0.1412 loss_val: 1.8246 acc_val: 0.1757\n",
            "Epoch: 0002 loss_train: 1.8278 acc_train: 0.1527 loss_val: 1.8145 acc_val: 0.1892\n",
            "Epoch: 0003 loss_train: 1.8206 acc_train: 0.1592 loss_val: 1.7996 acc_val: 0.1877\n",
            "Epoch: 0004 loss_train: 1.8050 acc_train: 0.1738 loss_val: 1.7804 acc_val: 0.1982\n",
            "Epoch: 0005 loss_train: 1.7892 acc_train: 0.1948 loss_val: 1.7574 acc_val: 0.2598\n",
            "Epoch: 0006 loss_train: 1.7646 acc_train: 0.2323 loss_val: 1.7310 acc_val: 0.3273\n",
            "Epoch: 0007 loss_train: 1.7381 acc_train: 0.2969 loss_val: 1.7019 acc_val: 0.4054\n",
            "Epoch: 0008 loss_train: 1.7107 acc_train: 0.3796 loss_val: 1.6705 acc_val: 0.4745\n",
            "Epoch: 0009 loss_train: 1.6795 acc_train: 0.4687 loss_val: 1.6371 acc_val: 0.5180\n",
            "Epoch: 0010 loss_train: 1.6443 acc_train: 0.5238 loss_val: 1.6021 acc_val: 0.5450\n",
            "Epoch: 0011 loss_train: 1.6093 acc_train: 0.5633 loss_val: 1.5659 acc_val: 0.5631\n",
            "Epoch: 0012 loss_train: 1.5712 acc_train: 0.5779 loss_val: 1.5283 acc_val: 0.5751\n",
            "Epoch: 0013 loss_train: 1.5338 acc_train: 0.5939 loss_val: 1.4895 acc_val: 0.5856\n",
            "Epoch: 0014 loss_train: 1.4936 acc_train: 0.6134 loss_val: 1.4499 acc_val: 0.6006\n",
            "Epoch: 0015 loss_train: 1.4514 acc_train: 0.6254 loss_val: 1.4097 acc_val: 0.6126\n",
            "Epoch: 0016 loss_train: 1.4086 acc_train: 0.6304 loss_val: 1.3691 acc_val: 0.6201\n",
            "Epoch: 0017 loss_train: 1.3665 acc_train: 0.6485 loss_val: 1.3284 acc_val: 0.6276\n",
            "Epoch: 0018 loss_train: 1.3224 acc_train: 0.6550 loss_val: 1.2880 acc_val: 0.6396\n",
            "Epoch: 0019 loss_train: 1.2805 acc_train: 0.6660 loss_val: 1.2483 acc_val: 0.6456\n",
            "Epoch: 0020 loss_train: 1.2373 acc_train: 0.6730 loss_val: 1.2094 acc_val: 0.6562\n",
            "Epoch: 0021 loss_train: 1.1942 acc_train: 0.6790 loss_val: 1.1717 acc_val: 0.6682\n",
            "Epoch: 0022 loss_train: 1.1514 acc_train: 0.6960 loss_val: 1.1356 acc_val: 0.6892\n",
            "Epoch: 0023 loss_train: 1.1101 acc_train: 0.7031 loss_val: 1.1012 acc_val: 0.6922\n",
            "Epoch: 0024 loss_train: 1.0701 acc_train: 0.7111 loss_val: 1.0689 acc_val: 0.6937\n",
            "Epoch: 0025 loss_train: 1.0313 acc_train: 0.7171 loss_val: 1.0388 acc_val: 0.6982\n",
            "Epoch: 0026 loss_train: 0.9938 acc_train: 0.7266 loss_val: 1.0108 acc_val: 0.7042\n",
            "Epoch: 0027 loss_train: 0.9575 acc_train: 0.7276 loss_val: 0.9850 acc_val: 0.7042\n",
            "Epoch: 0028 loss_train: 0.9233 acc_train: 0.7336 loss_val: 0.9612 acc_val: 0.7132\n",
            "Epoch: 0029 loss_train: 0.8901 acc_train: 0.7391 loss_val: 0.9397 acc_val: 0.7162\n",
            "Epoch: 0030 loss_train: 0.8563 acc_train: 0.7456 loss_val: 0.9205 acc_val: 0.7192\n",
            "Epoch: 0031 loss_train: 0.8228 acc_train: 0.7531 loss_val: 0.9037 acc_val: 0.7207\n",
            "Epoch: 0032 loss_train: 0.7948 acc_train: 0.7616 loss_val: 0.8892 acc_val: 0.7192\n",
            "Epoch: 0033 loss_train: 0.7652 acc_train: 0.7682 loss_val: 0.8765 acc_val: 0.7207\n",
            "Epoch: 0034 loss_train: 0.7386 acc_train: 0.7722 loss_val: 0.8653 acc_val: 0.7207\n",
            "Epoch: 0035 loss_train: 0.7101 acc_train: 0.7777 loss_val: 0.8560 acc_val: 0.7237\n",
            "Epoch: 0036 loss_train: 0.6839 acc_train: 0.7837 loss_val: 0.8485 acc_val: 0.7267\n",
            "Epoch: 0037 loss_train: 0.6595 acc_train: 0.7867 loss_val: 0.8427 acc_val: 0.7237\n",
            "Epoch: 0038 loss_train: 0.6345 acc_train: 0.7972 loss_val: 0.8381 acc_val: 0.7357\n",
            "Epoch: 0039 loss_train: 0.6103 acc_train: 0.8067 loss_val: 0.8349 acc_val: 0.7372\n",
            "Epoch: 0040 loss_train: 0.5898 acc_train: 0.8137 loss_val: 0.8334 acc_val: 0.7462\n",
            "Epoch: 0041 loss_train: 0.5698 acc_train: 0.8167 loss_val: 0.8337 acc_val: 0.7462\n",
            "Epoch: 0042 loss_train: 0.5483 acc_train: 0.8212 loss_val: 0.8358 acc_val: 0.7477\n",
            "Epoch: 0043 loss_train: 0.5265 acc_train: 0.8247 loss_val: 0.8397 acc_val: 0.7508\n",
            "Epoch: 0044 loss_train: 0.5079 acc_train: 0.8292 loss_val: 0.8456 acc_val: 0.7492\n",
            "Epoch: 0045 loss_train: 0.4877 acc_train: 0.8322 loss_val: 0.8525 acc_val: 0.7447\n",
            "Epoch: 0046 loss_train: 0.4702 acc_train: 0.8388 loss_val: 0.8603 acc_val: 0.7477\n",
            "Epoch: 0047 loss_train: 0.4494 acc_train: 0.8478 loss_val: 0.8698 acc_val: 0.7462\n",
            "Epoch: 0048 loss_train: 0.4337 acc_train: 0.8533 loss_val: 0.8809 acc_val: 0.7417\n",
            "Epoch: 0049 loss_train: 0.4156 acc_train: 0.8578 loss_val: 0.8936 acc_val: 0.7402\n",
            "Epoch: 0050 loss_train: 0.3995 acc_train: 0.8653 loss_val: 0.9074 acc_val: 0.7417\n",
            "Epoch: 0051 loss_train: 0.3842 acc_train: 0.8643 loss_val: 0.9222 acc_val: 0.7372\n",
            "Epoch: 0052 loss_train: 0.3657 acc_train: 0.8773 loss_val: 0.9390 acc_val: 0.7357\n",
            "Epoch: 0053 loss_train: 0.3496 acc_train: 0.8808 loss_val: 0.9587 acc_val: 0.7372\n",
            "Epoch: 0054 loss_train: 0.3342 acc_train: 0.8848 loss_val: 0.9803 acc_val: 0.7312\n",
            "Epoch: 0055 loss_train: 0.3183 acc_train: 0.8918 loss_val: 1.0022 acc_val: 0.7282\n",
            "Epoch: 0056 loss_train: 0.2997 acc_train: 0.9004 loss_val: 1.0245 acc_val: 0.7297\n",
            "Epoch: 0057 loss_train: 0.2826 acc_train: 0.8998 loss_val: 1.0471 acc_val: 0.7282\n",
            "Epoch: 0058 loss_train: 0.2705 acc_train: 0.9014 loss_val: 1.0700 acc_val: 0.7312\n",
            "Epoch: 0059 loss_train: 0.2493 acc_train: 0.9119 loss_val: 1.0930 acc_val: 0.7342\n",
            "Epoch: 0060 loss_train: 0.2338 acc_train: 0.9204 loss_val: 1.1163 acc_val: 0.7342\n",
            "Epoch: 0061 loss_train: 0.2192 acc_train: 0.9274 loss_val: 1.1416 acc_val: 0.7357\n",
            "Epoch: 0062 loss_train: 0.2034 acc_train: 0.9359 loss_val: 1.1693 acc_val: 0.7387\n",
            "Epoch: 0063 loss_train: 0.1880 acc_train: 0.9439 loss_val: 1.1976 acc_val: 0.7432\n",
            "Epoch: 0064 loss_train: 0.1752 acc_train: 0.9464 loss_val: 1.2253 acc_val: 0.7387\n",
            "Epoch: 0065 loss_train: 0.1605 acc_train: 0.9549 loss_val: 1.2564 acc_val: 0.7402\n",
            "Epoch: 0066 loss_train: 0.1493 acc_train: 0.9514 loss_val: 1.2885 acc_val: 0.7387\n",
            "Epoch: 0067 loss_train: 0.1369 acc_train: 0.9629 loss_val: 1.3184 acc_val: 0.7402\n",
            "Epoch: 0068 loss_train: 0.1237 acc_train: 0.9619 loss_val: 1.3475 acc_val: 0.7387\n",
            "Epoch: 0069 loss_train: 0.1141 acc_train: 0.9670 loss_val: 1.3774 acc_val: 0.7372\n",
            "Epoch: 0070 loss_train: 0.1046 acc_train: 0.9685 loss_val: 1.4064 acc_val: 0.7372\n",
            "Epoch: 0071 loss_train: 0.0957 acc_train: 0.9710 loss_val: 1.4324 acc_val: 0.7327\n",
            "Epoch: 0072 loss_train: 0.0864 acc_train: 0.9725 loss_val: 1.4580 acc_val: 0.7282\n",
            "Epoch: 0073 loss_train: 0.0794 acc_train: 0.9745 loss_val: 1.4830 acc_val: 0.7297\n",
            "Epoch: 0074 loss_train: 0.0719 acc_train: 0.9775 loss_val: 1.5067 acc_val: 0.7282\n",
            "Epoch: 0075 loss_train: 0.0650 acc_train: 0.9810 loss_val: 1.5305 acc_val: 0.7297\n",
            "Epoch: 0076 loss_train: 0.0593 acc_train: 0.9825 loss_val: 1.5557 acc_val: 0.7282\n",
            "Epoch: 0077 loss_train: 0.0555 acc_train: 0.9840 loss_val: 1.5821 acc_val: 0.7252\n",
            "Epoch: 0078 loss_train: 0.0480 acc_train: 0.9870 loss_val: 1.6092 acc_val: 0.7267\n",
            "Epoch: 0079 loss_train: 0.0456 acc_train: 0.9875 loss_val: 1.6336 acc_val: 0.7282\n",
            "Epoch: 0080 loss_train: 0.0405 acc_train: 0.9875 loss_val: 1.6592 acc_val: 0.7267\n",
            "Epoch: 0081 loss_train: 0.0359 acc_train: 0.9900 loss_val: 1.6857 acc_val: 0.7282\n",
            "Epoch: 0082 loss_train: 0.0342 acc_train: 0.9925 loss_val: 1.7128 acc_val: 0.7267\n",
            "Epoch: 0083 loss_train: 0.0307 acc_train: 0.9915 loss_val: 1.7375 acc_val: 0.7252\n",
            "Epoch: 0084 loss_train: 0.0291 acc_train: 0.9915 loss_val: 1.7580 acc_val: 0.7237\n",
            "Epoch: 0085 loss_train: 0.0253 acc_train: 0.9950 loss_val: 1.7781 acc_val: 0.7252\n",
            "Epoch: 0086 loss_train: 0.0240 acc_train: 0.9915 loss_val: 1.8004 acc_val: 0.7267\n",
            "Epoch: 0087 loss_train: 0.0223 acc_train: 0.9945 loss_val: 1.8231 acc_val: 0.7222\n",
            "Epoch: 0088 loss_train: 0.0203 acc_train: 0.9950 loss_val: 1.8434 acc_val: 0.7237\n",
            "Epoch: 0089 loss_train: 0.0180 acc_train: 0.9960 loss_val: 1.8600 acc_val: 0.7237\n",
            "Epoch: 0090 loss_train: 0.0178 acc_train: 0.9945 loss_val: 1.8726 acc_val: 0.7267\n",
            "Epoch: 0091 loss_train: 0.0173 acc_train: 0.9950 loss_val: 1.8876 acc_val: 0.7297\n",
            "Epoch: 0092 loss_train: 0.0151 acc_train: 0.9950 loss_val: 1.9051 acc_val: 0.7282\n",
            "Epoch: 0093 loss_train: 0.0131 acc_train: 0.9965 loss_val: 1.9223 acc_val: 0.7282\n",
            "Optimization Finished!\n",
            "Train cost: 29.7221s\n",
            "Loading 43th epoch\n",
            "Test set results: loss= 0.7103 accuracy= 0.7831\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --dataset pubmed --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 7  --n_heads 8 --n_layers 1 --pe_dim 5 --peak_lr 0.001  --attack none --attperc 20 --TrainTest train --weight_decay=1e-05\n",
        "\n",
        "!python train.py --dataset pubmed --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 7  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack none --attperc 20 --TrainTest train --weight_decay=1e-05\n",
        "\n",
        "!python train.py --dataset pubmed --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 7  --n_heads 8 --n_layers 1 --pe_dim 40 --peak_lr 0.001  --attack none --attperc 20 --TrainTest train --weight_decay=1e-05\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIqXRKgFWMM5",
        "outputId": "d6c3bf01-9bff-491f-e830-81bf3c482976"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=505, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=3, bias=True)\n",
            ")\n",
            "total params: 2496005\n",
            "Epoch: 0001 loss_train: 6.3335 acc_train: 0.4491 loss_val: 1.9322 acc_val: 0.6242\n",
            "Epoch: 0002 loss_train: 5.4032 acc_train: 0.6368 loss_val: 1.5263 acc_val: 0.7130\n",
            "Epoch: 0003 loss_train: 4.2378 acc_train: 0.7438 loss_val: 1.1872 acc_val: 0.7835\n",
            "Epoch: 0004 loss_train: 3.3904 acc_train: 0.7843 loss_val: 1.0355 acc_val: 0.7992\n",
            "Epoch: 0005 loss_train: 3.0763 acc_train: 0.8014 loss_val: 0.9918 acc_val: 0.8147\n",
            "Epoch: 0006 loss_train: 2.8635 acc_train: 0.8168 loss_val: 0.9111 acc_val: 0.8235\n",
            "Epoch: 0007 loss_train: 2.6281 acc_train: 0.8287 loss_val: 0.8379 acc_val: 0.8403\n",
            "Epoch: 0008 loss_train: 2.3972 acc_train: 0.8432 loss_val: 0.7870 acc_val: 0.8502\n",
            "Epoch: 0009 loss_train: 2.1913 acc_train: 0.8571 loss_val: 0.7358 acc_val: 0.8595\n",
            "Epoch: 0010 loss_train: 2.0146 acc_train: 0.8681 loss_val: 0.7059 acc_val: 0.8656\n",
            "Epoch: 0011 loss_train: 1.8606 acc_train: 0.8826 loss_val: 0.6699 acc_val: 0.8709\n",
            "Epoch: 0012 loss_train: 1.7023 acc_train: 0.8936 loss_val: 0.6560 acc_val: 0.8755\n",
            "Epoch: 0013 loss_train: 1.6119 acc_train: 0.8980 loss_val: 0.7363 acc_val: 0.8638\n",
            "Epoch: 0014 loss_train: 1.6113 acc_train: 0.8981 loss_val: 0.6436 acc_val: 0.8796\n",
            "Epoch: 0015 loss_train: 1.4517 acc_train: 0.9084 loss_val: 0.6232 acc_val: 0.8874\n",
            "Epoch: 0016 loss_train: 1.3442 acc_train: 0.9170 loss_val: 0.6445 acc_val: 0.8859\n",
            "Epoch: 0017 loss_train: 1.2818 acc_train: 0.9191 loss_val: 0.6653 acc_val: 0.8821\n",
            "Epoch: 0018 loss_train: 1.2403 acc_train: 0.9205 loss_val: 0.7184 acc_val: 0.8725\n",
            "Epoch: 0019 loss_train: 1.1775 acc_train: 0.9265 loss_val: 0.6678 acc_val: 0.8775\n",
            "Epoch: 0020 loss_train: 1.1308 acc_train: 0.9278 loss_val: 0.6353 acc_val: 0.8922\n",
            "Epoch: 0021 loss_train: 1.0478 acc_train: 0.9349 loss_val: 0.7068 acc_val: 0.8854\n",
            "Epoch: 0022 loss_train: 0.9839 acc_train: 0.9400 loss_val: 0.7090 acc_val: 0.8824\n",
            "Epoch: 0023 loss_train: 0.9029 acc_train: 0.9456 loss_val: 0.7345 acc_val: 0.8760\n",
            "Epoch: 0024 loss_train: 0.8084 acc_train: 0.9510 loss_val: 0.7263 acc_val: 0.8846\n",
            "Epoch: 0025 loss_train: 0.7443 acc_train: 0.9535 loss_val: 0.7548 acc_val: 0.8897\n",
            "Epoch: 0026 loss_train: 0.6373 acc_train: 0.9611 loss_val: 0.7876 acc_val: 0.8874\n",
            "Epoch: 0027 loss_train: 0.5397 acc_train: 0.9699 loss_val: 0.8634 acc_val: 0.8798\n",
            "Epoch: 0028 loss_train: 0.4825 acc_train: 0.9731 loss_val: 0.9343 acc_val: 0.8791\n",
            "Epoch: 0029 loss_train: 0.4610 acc_train: 0.9733 loss_val: 0.9330 acc_val: 0.8791\n",
            "Epoch: 0030 loss_train: 0.3819 acc_train: 0.9783 loss_val: 1.0172 acc_val: 0.8839\n",
            "Epoch: 0031 loss_train: 0.3948 acc_train: 0.9754 loss_val: 1.1897 acc_val: 0.8621\n",
            "Epoch: 0032 loss_train: 0.4743 acc_train: 0.9706 loss_val: 1.0155 acc_val: 0.8813\n",
            "Epoch: 0033 loss_train: 0.3423 acc_train: 0.9772 loss_val: 1.1039 acc_val: 0.8846\n",
            "Epoch: 0034 loss_train: 0.3556 acc_train: 0.9788 loss_val: 1.0781 acc_val: 0.8836\n",
            "Epoch: 0035 loss_train: 0.2700 acc_train: 0.9829 loss_val: 1.1310 acc_val: 0.8821\n",
            "Epoch: 0036 loss_train: 0.1936 acc_train: 0.9887 loss_val: 1.1370 acc_val: 0.8775\n",
            "Epoch: 0037 loss_train: 0.1511 acc_train: 0.9909 loss_val: 1.2289 acc_val: 0.8785\n",
            "Epoch: 0038 loss_train: 0.1309 acc_train: 0.9925 loss_val: 1.3010 acc_val: 0.8775\n",
            "Epoch: 0039 loss_train: 0.1618 acc_train: 0.9893 loss_val: 1.3813 acc_val: 0.8806\n",
            "Epoch: 0040 loss_train: 0.1799 acc_train: 0.9889 loss_val: 1.4602 acc_val: 0.8722\n",
            "Epoch: 0041 loss_train: 0.2515 acc_train: 0.9855 loss_val: 1.3453 acc_val: 0.8674\n",
            "Epoch: 0042 loss_train: 0.1735 acc_train: 0.9902 loss_val: 1.4616 acc_val: 0.8742\n",
            "Epoch: 0043 loss_train: 0.2069 acc_train: 0.9877 loss_val: 1.3628 acc_val: 0.8773\n",
            "Epoch: 0044 loss_train: 0.1748 acc_train: 0.9910 loss_val: 1.3633 acc_val: 0.8735\n",
            "Epoch: 0045 loss_train: 0.1658 acc_train: 0.9905 loss_val: 1.4083 acc_val: 0.8770\n",
            "Epoch: 0046 loss_train: 0.1453 acc_train: 0.9915 loss_val: 1.4568 acc_val: 0.8763\n",
            "Epoch: 0047 loss_train: 0.1298 acc_train: 0.9930 loss_val: 1.4338 acc_val: 0.8659\n",
            "Epoch: 0048 loss_train: 0.1541 acc_train: 0.9915 loss_val: 1.3887 acc_val: 0.8785\n",
            "Epoch: 0049 loss_train: 0.1143 acc_train: 0.9932 loss_val: 1.3936 acc_val: 0.8742\n",
            "Epoch: 0050 loss_train: 0.0736 acc_train: 0.9959 loss_val: 1.5411 acc_val: 0.8808\n",
            "Epoch: 0051 loss_train: 0.0895 acc_train: 0.9948 loss_val: 1.6448 acc_val: 0.8742\n",
            "Epoch: 0052 loss_train: 0.2570 acc_train: 0.9865 loss_val: 1.7240 acc_val: 0.8697\n",
            "Epoch: 0053 loss_train: 1.3331 acc_train: 0.9497 loss_val: 8.0173 acc_val: 0.5591\n",
            "Epoch: 0054 loss_train: 5.3196 acc_train: 0.8346 loss_val: 0.8016 acc_val: 0.8613\n",
            "Epoch: 0055 loss_train: 1.8235 acc_train: 0.8855 loss_val: 0.7644 acc_val: 0.8674\n",
            "Epoch: 0056 loss_train: 1.4788 acc_train: 0.9111 loss_val: 0.6875 acc_val: 0.8742\n",
            "Epoch: 0057 loss_train: 1.2843 acc_train: 0.9232 loss_val: 0.7405 acc_val: 0.8801\n",
            "Epoch: 0058 loss_train: 1.0505 acc_train: 0.9363 loss_val: 0.6801 acc_val: 0.8849\n",
            "Epoch: 0059 loss_train: 0.8540 acc_train: 0.9511 loss_val: 0.7568 acc_val: 0.8803\n",
            "Epoch: 0060 loss_train: 0.8590 acc_train: 0.9484 loss_val: 0.7486 acc_val: 0.8844\n",
            "Epoch: 0061 loss_train: 0.7705 acc_train: 0.9533 loss_val: 0.7613 acc_val: 0.8829\n",
            "Epoch: 0062 loss_train: 0.6606 acc_train: 0.9624 loss_val: 0.9550 acc_val: 0.8631\n",
            "Epoch: 0063 loss_train: 0.6790 acc_train: 0.9590 loss_val: 0.8459 acc_val: 0.8851\n",
            "Epoch: 0064 loss_train: 0.5219 acc_train: 0.9713 loss_val: 0.9244 acc_val: 0.8826\n",
            "Epoch: 0065 loss_train: 0.4163 acc_train: 0.9762 loss_val: 0.9706 acc_val: 0.8725\n",
            "Epoch: 0066 loss_train: 0.3102 acc_train: 0.9842 loss_val: 1.0845 acc_val: 0.8816\n",
            "Epoch: 0067 loss_train: 0.2661 acc_train: 0.9864 loss_val: 1.1101 acc_val: 0.8722\n",
            "Epoch: 0068 loss_train: 0.2384 acc_train: 0.9882 loss_val: 1.1116 acc_val: 0.8831\n",
            "Epoch: 0069 loss_train: 0.1809 acc_train: 0.9905 loss_val: 1.1859 acc_val: 0.8745\n",
            "Epoch: 0070 loss_train: 0.1594 acc_train: 0.9922 loss_val: 1.2325 acc_val: 0.8783\n",
            "Optimization Finished!\n",
            "Train cost: 55.3248s\n",
            "Loading 20th epoch\n",
            "Test set results: loss= 0.6476 accuracy= 0.8935\n",
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=515, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=3, bias=True)\n",
            ")\n",
            "total params: 2501125\n",
            "Epoch: 0001 loss_train: 6.5268 acc_train: 0.3937 loss_val: 2.0050 acc_val: 0.5621\n",
            "Epoch: 0002 loss_train: 5.6107 acc_train: 0.6451 loss_val: 1.6404 acc_val: 0.6889\n",
            "Epoch: 0003 loss_train: 4.5798 acc_train: 0.7072 loss_val: 1.2995 acc_val: 0.7518\n",
            "Epoch: 0004 loss_train: 3.6574 acc_train: 0.7731 loss_val: 1.0907 acc_val: 0.7875\n",
            "Epoch: 0005 loss_train: 3.1752 acc_train: 0.7946 loss_val: 1.0121 acc_val: 0.8055\n",
            "Epoch: 0006 loss_train: 2.9188 acc_train: 0.8090 loss_val: 0.9214 acc_val: 0.8230\n",
            "Epoch: 0007 loss_train: 2.6477 acc_train: 0.8278 loss_val: 0.8484 acc_val: 0.8347\n",
            "Epoch: 0008 loss_train: 2.4033 acc_train: 0.8418 loss_val: 0.7762 acc_val: 0.8537\n",
            "Epoch: 0009 loss_train: 2.1739 acc_train: 0.8575 loss_val: 0.7444 acc_val: 0.8631\n",
            "Epoch: 0010 loss_train: 2.0221 acc_train: 0.8680 loss_val: 0.7093 acc_val: 0.8671\n",
            "Epoch: 0011 loss_train: 1.8280 acc_train: 0.8831 loss_val: 0.6645 acc_val: 0.8765\n",
            "Epoch: 0012 loss_train: 1.7502 acc_train: 0.8883 loss_val: 0.6576 acc_val: 0.8793\n",
            "Epoch: 0013 loss_train: 1.5969 acc_train: 0.8994 loss_val: 0.6499 acc_val: 0.8801\n",
            "Epoch: 0014 loss_train: 1.5122 acc_train: 0.9037 loss_val: 0.6885 acc_val: 0.8760\n",
            "Epoch: 0015 loss_train: 1.4774 acc_train: 0.9064 loss_val: 0.6343 acc_val: 0.8867\n",
            "Epoch: 0016 loss_train: 1.3783 acc_train: 0.9145 loss_val: 0.6252 acc_val: 0.8834\n",
            "Epoch: 0017 loss_train: 1.2856 acc_train: 0.9230 loss_val: 0.6251 acc_val: 0.8897\n",
            "Epoch: 0018 loss_train: 1.1536 acc_train: 0.9307 loss_val: 0.6529 acc_val: 0.8854\n",
            "Epoch: 0019 loss_train: 1.0949 acc_train: 0.9336 loss_val: 0.6890 acc_val: 0.8851\n",
            "Epoch: 0020 loss_train: 1.0392 acc_train: 0.9375 loss_val: 0.6600 acc_val: 0.8887\n",
            "Epoch: 0021 loss_train: 0.9177 acc_train: 0.9467 loss_val: 0.6957 acc_val: 0.8887\n",
            "Epoch: 0022 loss_train: 0.8809 acc_train: 0.9458 loss_val: 0.7379 acc_val: 0.8803\n",
            "Epoch: 0023 loss_train: 0.8144 acc_train: 0.9530 loss_val: 0.7723 acc_val: 0.8826\n",
            "Epoch: 0024 loss_train: 0.7890 acc_train: 0.9523 loss_val: 0.8411 acc_val: 0.8747\n",
            "Epoch: 0025 loss_train: 0.7780 acc_train: 0.9509 loss_val: 0.8303 acc_val: 0.8783\n",
            "Epoch: 0026 loss_train: 0.7116 acc_train: 0.9564 loss_val: 0.8051 acc_val: 0.8839\n",
            "Epoch: 0027 loss_train: 0.5610 acc_train: 0.9684 loss_val: 0.8101 acc_val: 0.8869\n",
            "Epoch: 0028 loss_train: 0.4670 acc_train: 0.9745 loss_val: 0.9474 acc_val: 0.8780\n",
            "Epoch: 0029 loss_train: 0.4295 acc_train: 0.9751 loss_val: 0.9985 acc_val: 0.8760\n",
            "Epoch: 0030 loss_train: 0.3867 acc_train: 0.9766 loss_val: 1.0297 acc_val: 0.8813\n",
            "Epoch: 0031 loss_train: 0.3107 acc_train: 0.9823 loss_val: 1.0278 acc_val: 0.8841\n",
            "Epoch: 0032 loss_train: 0.2900 acc_train: 0.9840 loss_val: 1.1981 acc_val: 0.8763\n",
            "Epoch: 0033 loss_train: 0.7144 acc_train: 0.9576 loss_val: 1.1541 acc_val: 0.8605\n",
            "Epoch: 0034 loss_train: 0.6196 acc_train: 0.9594 loss_val: 1.0990 acc_val: 0.8826\n",
            "Epoch: 0035 loss_train: 0.6583 acc_train: 0.9623 loss_val: 1.1192 acc_val: 0.8770\n",
            "Epoch: 0036 loss_train: 0.5769 acc_train: 0.9657 loss_val: 0.8821 acc_val: 0.8778\n",
            "Epoch: 0037 loss_train: 0.3560 acc_train: 0.9807 loss_val: 0.9838 acc_val: 0.8816\n",
            "Epoch: 0038 loss_train: 0.2502 acc_train: 0.9858 loss_val: 1.0691 acc_val: 0.8785\n",
            "Epoch: 0039 loss_train: 0.1834 acc_train: 0.9899 loss_val: 1.1508 acc_val: 0.8803\n",
            "Epoch: 0040 loss_train: 0.1176 acc_train: 0.9932 loss_val: 1.1723 acc_val: 0.8770\n",
            "Epoch: 0041 loss_train: 0.0921 acc_train: 0.9954 loss_val: 1.2731 acc_val: 0.8791\n",
            "Epoch: 0042 loss_train: 0.0806 acc_train: 0.9963 loss_val: 1.3150 acc_val: 0.8824\n",
            "Epoch: 0043 loss_train: 0.0630 acc_train: 0.9975 loss_val: 1.3461 acc_val: 0.8851\n",
            "Epoch: 0044 loss_train: 0.0423 acc_train: 0.9982 loss_val: 1.4321 acc_val: 0.8834\n",
            "Epoch: 0045 loss_train: 0.0405 acc_train: 0.9981 loss_val: 1.4822 acc_val: 0.8816\n",
            "Epoch: 0046 loss_train: 0.0403 acc_train: 0.9978 loss_val: 1.4735 acc_val: 0.8803\n",
            "Epoch: 0047 loss_train: 0.0452 acc_train: 0.9975 loss_val: 1.5169 acc_val: 0.8818\n",
            "Epoch: 0048 loss_train: 0.0573 acc_train: 0.9966 loss_val: 1.5403 acc_val: 0.8811\n",
            "Epoch: 0049 loss_train: 0.0507 acc_train: 0.9970 loss_val: 1.6027 acc_val: 0.8780\n",
            "Epoch: 0050 loss_train: 0.0522 acc_train: 0.9970 loss_val: 1.5824 acc_val: 0.8791\n",
            "Epoch: 0051 loss_train: 0.0622 acc_train: 0.9969 loss_val: 1.6464 acc_val: 0.8753\n",
            "Epoch: 0052 loss_train: 0.0543 acc_train: 0.9969 loss_val: 1.6753 acc_val: 0.8742\n",
            "Epoch: 0053 loss_train: 0.0756 acc_train: 0.9961 loss_val: 1.7133 acc_val: 0.8735\n",
            "Epoch: 0054 loss_train: 0.0723 acc_train: 0.9958 loss_val: 1.7299 acc_val: 0.8770\n",
            "Epoch: 0055 loss_train: 0.1142 acc_train: 0.9930 loss_val: 2.0479 acc_val: 0.8514\n",
            "Epoch: 0056 loss_train: 5.6391 acc_train: 0.7626 loss_val: 2.3017 acc_val: 0.5294\n",
            "Epoch: 0057 loss_train: 4.7769 acc_train: 0.6653 loss_val: 1.0550 acc_val: 0.7918\n",
            "Epoch: 0058 loss_train: 2.7487 acc_train: 0.8244 loss_val: 0.8637 acc_val: 0.8347\n",
            "Epoch: 0059 loss_train: 3.2688 acc_train: 0.7867 loss_val: 0.9494 acc_val: 0.8012\n",
            "Epoch: 0060 loss_train: 2.5691 acc_train: 0.8255 loss_val: 0.9026 acc_val: 0.8131\n",
            "Epoch: 0061 loss_train: 2.3066 acc_train: 0.8536 loss_val: 0.7375 acc_val: 0.8598\n",
            "Epoch: 0062 loss_train: 2.0242 acc_train: 0.8661 loss_val: 0.7294 acc_val: 0.8646\n",
            "Epoch: 0063 loss_train: 1.9044 acc_train: 0.8779 loss_val: 0.6774 acc_val: 0.8687\n",
            "Epoch: 0064 loss_train: 1.7796 acc_train: 0.8870 loss_val: 0.6624 acc_val: 0.8709\n",
            "Epoch: 0065 loss_train: 1.6959 acc_train: 0.8922 loss_val: 0.6309 acc_val: 0.8791\n",
            "Epoch: 0066 loss_train: 1.6078 acc_train: 0.8992 loss_val: 0.6236 acc_val: 0.8806\n",
            "Epoch: 0067 loss_train: 1.5173 acc_train: 0.9060 loss_val: 0.6224 acc_val: 0.8806\n",
            "Epoch: 0068 loss_train: 1.4677 acc_train: 0.9092 loss_val: 0.6067 acc_val: 0.8851\n",
            "Epoch: 0069 loss_train: 1.3689 acc_train: 0.9154 loss_val: 0.6076 acc_val: 0.8867\n",
            "Epoch: 0070 loss_train: 1.3036 acc_train: 0.9176 loss_val: 0.6205 acc_val: 0.8851\n",
            "Epoch: 0071 loss_train: 1.2713 acc_train: 0.9203 loss_val: 0.6351 acc_val: 0.8905\n",
            "Epoch: 0072 loss_train: 1.2381 acc_train: 0.9236 loss_val: 0.6851 acc_val: 0.8750\n",
            "Epoch: 0073 loss_train: 1.2760 acc_train: 0.9182 loss_val: 0.6240 acc_val: 0.8872\n",
            "Epoch: 0074 loss_train: 1.2092 acc_train: 0.9250 loss_val: 0.6453 acc_val: 0.8869\n",
            "Epoch: 0075 loss_train: 1.1753 acc_train: 0.9275 loss_val: 0.6733 acc_val: 0.8796\n",
            "Epoch: 0076 loss_train: 1.0985 acc_train: 0.9304 loss_val: 0.6581 acc_val: 0.8889\n",
            "Epoch: 0077 loss_train: 1.0217 acc_train: 0.9390 loss_val: 0.6515 acc_val: 0.8846\n",
            "Epoch: 0078 loss_train: 0.9535 acc_train: 0.9441 loss_val: 0.6804 acc_val: 0.8915\n",
            "Epoch: 0079 loss_train: 0.9203 acc_train: 0.9451 loss_val: 0.7042 acc_val: 0.8895\n",
            "Epoch: 0080 loss_train: 0.8812 acc_train: 0.9471 loss_val: 0.7121 acc_val: 0.8884\n",
            "Epoch: 0081 loss_train: 0.8589 acc_train: 0.9501 loss_val: 0.7313 acc_val: 0.8859\n",
            "Epoch: 0082 loss_train: 0.7780 acc_train: 0.9577 loss_val: 0.7680 acc_val: 0.8889\n",
            "Epoch: 0083 loss_train: 0.7578 acc_train: 0.9568 loss_val: 0.7519 acc_val: 0.8864\n",
            "Epoch: 0084 loss_train: 0.7270 acc_train: 0.9597 loss_val: 0.8329 acc_val: 0.8796\n",
            "Epoch: 0085 loss_train: 0.9022 acc_train: 0.9493 loss_val: 0.8528 acc_val: 0.8697\n",
            "Epoch: 0086 loss_train: 1.0906 acc_train: 0.9307 loss_val: 0.6755 acc_val: 0.8877\n",
            "Epoch: 0087 loss_train: 0.9863 acc_train: 0.9408 loss_val: 0.6651 acc_val: 0.8912\n",
            "Epoch: 0088 loss_train: 0.8798 acc_train: 0.9475 loss_val: 0.6891 acc_val: 0.8859\n",
            "Epoch: 0089 loss_train: 0.7700 acc_train: 0.9553 loss_val: 0.6914 acc_val: 0.8887\n",
            "Epoch: 0090 loss_train: 0.7212 acc_train: 0.9589 loss_val: 0.7566 acc_val: 0.8920\n",
            "Epoch: 0091 loss_train: 0.6481 acc_train: 0.9633 loss_val: 0.7884 acc_val: 0.8846\n",
            "Epoch: 0092 loss_train: 0.6038 acc_train: 0.9670 loss_val: 0.8251 acc_val: 0.8839\n",
            "Epoch: 0093 loss_train: 0.5660 acc_train: 0.9691 loss_val: 0.8529 acc_val: 0.8851\n",
            "Epoch: 0094 loss_train: 0.5659 acc_train: 0.9702 loss_val: 0.8750 acc_val: 0.8829\n",
            "Epoch: 0095 loss_train: 0.5375 acc_train: 0.9702 loss_val: 0.8799 acc_val: 0.8859\n",
            "Epoch: 0096 loss_train: 0.5349 acc_train: 0.9714 loss_val: 0.8961 acc_val: 0.8874\n",
            "Epoch: 0097 loss_train: 0.4617 acc_train: 0.9767 loss_val: 0.9232 acc_val: 0.8854\n",
            "Epoch: 0098 loss_train: 0.4605 acc_train: 0.9757 loss_val: 0.9660 acc_val: 0.8791\n",
            "Epoch: 0099 loss_train: 0.4736 acc_train: 0.9759 loss_val: 0.9793 acc_val: 0.8829\n",
            "Epoch: 0100 loss_train: 0.4557 acc_train: 0.9761 loss_val: 0.9576 acc_val: 0.8826\n",
            "Epoch: 0101 loss_train: 0.4262 acc_train: 0.9791 loss_val: 0.9954 acc_val: 0.8841\n",
            "Epoch: 0102 loss_train: 0.4439 acc_train: 0.9765 loss_val: 1.0609 acc_val: 0.8798\n",
            "Epoch: 0103 loss_train: 0.5062 acc_train: 0.9711 loss_val: 0.9898 acc_val: 0.8834\n",
            "Epoch: 0104 loss_train: 0.4448 acc_train: 0.9768 loss_val: 0.9441 acc_val: 0.8887\n",
            "Epoch: 0105 loss_train: 0.4457 acc_train: 0.9773 loss_val: 0.9701 acc_val: 0.8801\n",
            "Epoch: 0106 loss_train: 0.4389 acc_train: 0.9767 loss_val: 1.0631 acc_val: 0.8763\n",
            "Epoch: 0107 loss_train: 0.4121 acc_train: 0.9794 loss_val: 0.9949 acc_val: 0.8793\n",
            "Epoch: 0108 loss_train: 0.3538 acc_train: 0.9837 loss_val: 1.0254 acc_val: 0.8824\n",
            "Epoch: 0109 loss_train: 0.3436 acc_train: 0.9836 loss_val: 1.0270 acc_val: 0.8839\n",
            "Epoch: 0110 loss_train: 0.3398 acc_train: 0.9838 loss_val: 1.0582 acc_val: 0.8829\n",
            "Epoch: 0111 loss_train: 0.3228 acc_train: 0.9840 loss_val: 1.0969 acc_val: 0.8798\n",
            "Epoch: 0112 loss_train: 0.3675 acc_train: 0.9806 loss_val: 1.0711 acc_val: 0.8829\n",
            "Epoch: 0113 loss_train: 0.3062 acc_train: 0.9853 loss_val: 1.0910 acc_val: 0.8816\n",
            "Epoch: 0114 loss_train: 0.2863 acc_train: 0.9860 loss_val: 1.1241 acc_val: 0.8775\n",
            "Epoch: 0115 loss_train: 0.2794 acc_train: 0.9877 loss_val: 1.1985 acc_val: 0.8798\n",
            "Epoch: 0116 loss_train: 0.2628 acc_train: 0.9877 loss_val: 1.1814 acc_val: 0.8780\n",
            "Epoch: 0117 loss_train: 0.2706 acc_train: 0.9870 loss_val: 1.2116 acc_val: 0.8811\n",
            "Epoch: 0118 loss_train: 0.2647 acc_train: 0.9873 loss_val: 1.1897 acc_val: 0.8811\n",
            "Epoch: 0119 loss_train: 0.2516 acc_train: 0.9891 loss_val: 1.1783 acc_val: 0.8824\n",
            "Epoch: 0120 loss_train: 0.2204 acc_train: 0.9901 loss_val: 1.2201 acc_val: 0.8798\n",
            "Epoch: 0121 loss_train: 0.2276 acc_train: 0.9901 loss_val: 1.2666 acc_val: 0.8793\n",
            "Epoch: 0122 loss_train: 0.2196 acc_train: 0.9899 loss_val: 1.2593 acc_val: 0.8801\n",
            "Epoch: 0123 loss_train: 0.1977 acc_train: 0.9910 loss_val: 1.2743 acc_val: 0.8818\n",
            "Epoch: 0124 loss_train: 0.1905 acc_train: 0.9919 loss_val: 1.3226 acc_val: 0.8773\n",
            "Epoch: 0125 loss_train: 0.1880 acc_train: 0.9926 loss_val: 1.3028 acc_val: 0.8801\n",
            "Epoch: 0126 loss_train: 0.1949 acc_train: 0.9920 loss_val: 1.3192 acc_val: 0.8780\n",
            "Epoch: 0127 loss_train: 0.2209 acc_train: 0.9902 loss_val: 1.3070 acc_val: 0.8758\n",
            "Epoch: 0128 loss_train: 0.2622 acc_train: 0.9866 loss_val: 1.2770 acc_val: 0.8829\n",
            "Epoch: 0129 loss_train: 0.2576 acc_train: 0.9884 loss_val: 1.2361 acc_val: 0.8785\n",
            "Epoch: 0130 loss_train: 0.2407 acc_train: 0.9892 loss_val: 1.3049 acc_val: 0.8765\n",
            "Epoch: 0131 loss_train: 0.2296 acc_train: 0.9901 loss_val: 1.2518 acc_val: 0.8791\n",
            "Epoch: 0132 loss_train: 0.2209 acc_train: 0.9899 loss_val: 1.2675 acc_val: 0.8803\n",
            "Epoch: 0133 loss_train: 0.2049 acc_train: 0.9909 loss_val: 1.2856 acc_val: 0.8796\n",
            "Epoch: 0134 loss_train: 0.1963 acc_train: 0.9913 loss_val: 1.2263 acc_val: 0.8803\n",
            "Epoch: 0135 loss_train: 0.2195 acc_train: 0.9897 loss_val: 1.2534 acc_val: 0.8826\n",
            "Epoch: 0136 loss_train: 0.1919 acc_train: 0.9917 loss_val: 1.3379 acc_val: 0.8778\n",
            "Epoch: 0137 loss_train: 0.1735 acc_train: 0.9931 loss_val: 1.2871 acc_val: 0.8829\n",
            "Epoch: 0138 loss_train: 0.1707 acc_train: 0.9933 loss_val: 1.3179 acc_val: 0.8818\n",
            "Epoch: 0139 loss_train: 0.1558 acc_train: 0.9936 loss_val: 1.3507 acc_val: 0.8831\n",
            "Epoch: 0140 loss_train: 0.1588 acc_train: 0.9929 loss_val: 1.3458 acc_val: 0.8806\n",
            "Optimization Finished!\n",
            "Train cost: 109.9899s\n",
            "Loading 90th epoch\n",
            "Test set results: loss= 0.7947 accuracy= 0.8813\n",
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=540, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=3, bias=True)\n",
            ")\n",
            "total params: 2513925\n",
            "Epoch: 0001 loss_train: 6.4184 acc_train: 0.4478 loss_val: 1.9449 acc_val: 0.6491\n",
            "Epoch: 0002 loss_train: 5.4380 acc_train: 0.6518 loss_val: 1.5493 acc_val: 0.6886\n",
            "Epoch: 0003 loss_train: 4.3173 acc_train: 0.7174 loss_val: 1.2134 acc_val: 0.7670\n",
            "Epoch: 0004 loss_train: 3.4826 acc_train: 0.7818 loss_val: 1.0695 acc_val: 0.7913\n",
            "Epoch: 0005 loss_train: 3.1238 acc_train: 0.7947 loss_val: 1.0063 acc_val: 0.8058\n",
            "Epoch: 0006 loss_train: 2.8865 acc_train: 0.8126 loss_val: 0.9225 acc_val: 0.8200\n",
            "Epoch: 0007 loss_train: 2.6415 acc_train: 0.8298 loss_val: 0.8489 acc_val: 0.8344\n",
            "Epoch: 0008 loss_train: 2.4055 acc_train: 0.8473 loss_val: 0.7916 acc_val: 0.8481\n",
            "Epoch: 0009 loss_train: 2.1667 acc_train: 0.8598 loss_val: 0.7433 acc_val: 0.8588\n",
            "Epoch: 0010 loss_train: 1.9909 acc_train: 0.8705 loss_val: 0.6985 acc_val: 0.8651\n",
            "Epoch: 0011 loss_train: 1.8217 acc_train: 0.8826 loss_val: 0.6732 acc_val: 0.8669\n",
            "Epoch: 0012 loss_train: 1.6894 acc_train: 0.8932 loss_val: 0.6729 acc_val: 0.8737\n",
            "Epoch: 0013 loss_train: 1.5628 acc_train: 0.9018 loss_val: 0.6539 acc_val: 0.8778\n",
            "Epoch: 0014 loss_train: 1.4972 acc_train: 0.9066 loss_val: 0.6430 acc_val: 0.8841\n",
            "Epoch: 0015 loss_train: 1.3876 acc_train: 0.9148 loss_val: 0.6673 acc_val: 0.8803\n",
            "Epoch: 0016 loss_train: 1.3409 acc_train: 0.9162 loss_val: 0.6515 acc_val: 0.8846\n",
            "Epoch: 0017 loss_train: 1.2430 acc_train: 0.9230 loss_val: 0.6615 acc_val: 0.8834\n",
            "Epoch: 0018 loss_train: 1.2977 acc_train: 0.9166 loss_val: 0.7301 acc_val: 0.8682\n",
            "Epoch: 0019 loss_train: 1.1858 acc_train: 0.9232 loss_val: 0.6365 acc_val: 0.8851\n",
            "Epoch: 0020 loss_train: 1.0717 acc_train: 0.9328 loss_val: 0.6384 acc_val: 0.8897\n",
            "Epoch: 0021 loss_train: 0.9689 acc_train: 0.9414 loss_val: 0.7157 acc_val: 0.8854\n",
            "Epoch: 0022 loss_train: 0.8868 acc_train: 0.9474 loss_val: 0.6956 acc_val: 0.8907\n",
            "Epoch: 0023 loss_train: 0.7599 acc_train: 0.9547 loss_val: 0.7263 acc_val: 0.8869\n",
            "Epoch: 0024 loss_train: 0.7573 acc_train: 0.9538 loss_val: 0.7423 acc_val: 0.8902\n",
            "Epoch: 0025 loss_train: 0.7150 acc_train: 0.9579 loss_val: 0.7498 acc_val: 0.8849\n",
            "Epoch: 0026 loss_train: 0.6298 acc_train: 0.9620 loss_val: 0.8237 acc_val: 0.8780\n",
            "Epoch: 0027 loss_train: 0.6248 acc_train: 0.9622 loss_val: 0.8464 acc_val: 0.8829\n",
            "Epoch: 0028 loss_train: 0.5342 acc_train: 0.9676 loss_val: 0.8525 acc_val: 0.8849\n",
            "Epoch: 0029 loss_train: 0.4029 acc_train: 0.9776 loss_val: 0.9253 acc_val: 0.8859\n",
            "Epoch: 0030 loss_train: 0.4002 acc_train: 0.9747 loss_val: 1.6416 acc_val: 0.8410\n",
            "Epoch: 0031 loss_train: 2.5552 acc_train: 0.8591 loss_val: 0.8578 acc_val: 0.8484\n",
            "Epoch: 0032 loss_train: 1.5741 acc_train: 0.8996 loss_val: 0.8327 acc_val: 0.8633\n",
            "Epoch: 0033 loss_train: 1.2917 acc_train: 0.9189 loss_val: 0.6724 acc_val: 0.8803\n",
            "Epoch: 0034 loss_train: 1.0914 acc_train: 0.9332 loss_val: 0.7057 acc_val: 0.8864\n",
            "Epoch: 0035 loss_train: 0.8889 acc_train: 0.9441 loss_val: 0.7598 acc_val: 0.8839\n",
            "Epoch: 0036 loss_train: 0.7018 acc_train: 0.9592 loss_val: 0.8386 acc_val: 0.8803\n",
            "Epoch: 0037 loss_train: 0.5467 acc_train: 0.9686 loss_val: 0.9056 acc_val: 0.8839\n",
            "Epoch: 0038 loss_train: 0.5117 acc_train: 0.9702 loss_val: 1.0681 acc_val: 0.8636\n",
            "Epoch: 0039 loss_train: 0.4672 acc_train: 0.9720 loss_val: 0.9980 acc_val: 0.8798\n",
            "Epoch: 0040 loss_train: 0.3986 acc_train: 0.9773 loss_val: 1.0536 acc_val: 0.8839\n",
            "Epoch: 0041 loss_train: 0.3277 acc_train: 0.9813 loss_val: 1.0665 acc_val: 0.8770\n",
            "Epoch: 0042 loss_train: 0.3015 acc_train: 0.9844 loss_val: 1.0914 acc_val: 0.8755\n",
            "Epoch: 0043 loss_train: 0.2517 acc_train: 0.9873 loss_val: 1.1336 acc_val: 0.8808\n",
            "Epoch: 0044 loss_train: 0.2052 acc_train: 0.9888 loss_val: 1.1690 acc_val: 0.8770\n",
            "Epoch: 0045 loss_train: 0.1633 acc_train: 0.9921 loss_val: 1.2579 acc_val: 0.8773\n",
            "Epoch: 0046 loss_train: 0.1215 acc_train: 0.9943 loss_val: 1.2818 acc_val: 0.8775\n",
            "Epoch: 0047 loss_train: 0.0933 acc_train: 0.9962 loss_val: 1.3484 acc_val: 0.8836\n",
            "Epoch: 0048 loss_train: 0.0707 acc_train: 0.9965 loss_val: 1.3985 acc_val: 0.8783\n",
            "Epoch: 0049 loss_train: 0.0633 acc_train: 0.9969 loss_val: 1.5029 acc_val: 0.8785\n",
            "Epoch: 0050 loss_train: 0.0777 acc_train: 0.9958 loss_val: 1.5622 acc_val: 0.8780\n",
            "Epoch: 0051 loss_train: 0.0744 acc_train: 0.9961 loss_val: 1.5897 acc_val: 0.8732\n",
            "Epoch: 0052 loss_train: 0.0762 acc_train: 0.9960 loss_val: 1.6421 acc_val: 0.8758\n",
            "Epoch: 0053 loss_train: 0.0792 acc_train: 0.9955 loss_val: 1.6119 acc_val: 0.8750\n",
            "Epoch: 0054 loss_train: 0.0767 acc_train: 0.9955 loss_val: 1.6954 acc_val: 0.8704\n",
            "Epoch: 0055 loss_train: 0.1043 acc_train: 0.9944 loss_val: 1.6889 acc_val: 0.8765\n",
            "Epoch: 0056 loss_train: 0.1254 acc_train: 0.9928 loss_val: 1.6673 acc_val: 0.8791\n",
            "Epoch: 0057 loss_train: 0.1194 acc_train: 0.9928 loss_val: 1.6949 acc_val: 0.8636\n",
            "Epoch: 0058 loss_train: 1.7157 acc_train: 0.9376 loss_val: 3.7503 acc_val: 0.7632\n",
            "Epoch: 0059 loss_train: 5.9336 acc_train: 0.7449 loss_val: 1.0809 acc_val: 0.8190\n",
            "Epoch: 0060 loss_train: 2.5596 acc_train: 0.8641 loss_val: 0.7871 acc_val: 0.8684\n",
            "Epoch: 0061 loss_train: 1.9971 acc_train: 0.8901 loss_val: 0.7545 acc_val: 0.8659\n",
            "Epoch: 0062 loss_train: 1.7324 acc_train: 0.8939 loss_val: 0.6624 acc_val: 0.8765\n",
            "Epoch: 0063 loss_train: 1.5000 acc_train: 0.9096 loss_val: 0.6554 acc_val: 0.8785\n",
            "Epoch: 0064 loss_train: 1.3613 acc_train: 0.9178 loss_val: 0.6563 acc_val: 0.8846\n",
            "Epoch: 0065 loss_train: 1.2657 acc_train: 0.9231 loss_val: 0.6625 acc_val: 0.8801\n",
            "Epoch: 0066 loss_train: 1.1636 acc_train: 0.9324 loss_val: 0.6728 acc_val: 0.8846\n",
            "Epoch: 0067 loss_train: 1.1233 acc_train: 0.9317 loss_val: 0.6743 acc_val: 0.8849\n",
            "Epoch: 0068 loss_train: 1.0631 acc_train: 0.9328 loss_val: 0.6459 acc_val: 0.8874\n",
            "Epoch: 0069 loss_train: 0.9202 acc_train: 0.9456 loss_val: 0.7023 acc_val: 0.8869\n",
            "Epoch: 0070 loss_train: 0.8484 acc_train: 0.9515 loss_val: 0.7539 acc_val: 0.8796\n",
            "Epoch: 0071 loss_train: 0.7620 acc_train: 0.9558 loss_val: 0.7460 acc_val: 0.8867\n",
            "Epoch: 0072 loss_train: 0.6698 acc_train: 0.9628 loss_val: 0.8049 acc_val: 0.8846\n",
            "Optimization Finished!\n",
            "Train cost: 61.3335s\n",
            "Loading 22th epoch\n",
            "Test set results: loss= 0.7263 accuracy= 0.8884\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attacks on NAGphormer"
      ],
      "metadata": {
        "id": "qOLubwmNZUc6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cora"
      ],
      "metadata": {
        "id": "S_8UqOlTZ95Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --dataset cora --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 5  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack none --attperc 20 --TrainTest train --weight_decay=1e-05\n",
        "\n",
        "!python train.py --dataset cora --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 5  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack dice --attperc 10 --TrainTest train --weight_decay=1e-05\n",
        "\n",
        "!python train.py --dataset cora --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 5  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack dice --attperc 40 --TrainTest train --weight_decay=1e-05\n",
        "\n",
        "!python train.py --dataset cora --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 5  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack dice --attperc 60 --TrainTest train --weight_decay=1e-05\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOz5JvswWRXR",
        "outputId": "9fb4d119-424f-4972-b69e-f49d67592d52"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=1448, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=7, bias=True)\n",
            ")\n",
            "total params: 2979849\n",
            "Epoch: 0001 loss_train: 1.9727 acc_train: 0.1156 loss_val: 1.9686 acc_val: 0.1310\n",
            "Epoch: 0002 loss_train: 1.9669 acc_train: 0.1279 loss_val: 1.9561 acc_val: 0.1384\n",
            "Epoch: 0003 loss_train: 1.9547 acc_train: 0.1396 loss_val: 1.9375 acc_val: 0.1587\n",
            "Epoch: 0004 loss_train: 1.9373 acc_train: 0.1525 loss_val: 1.9130 acc_val: 0.2122\n",
            "Epoch: 0005 loss_train: 1.9160 acc_train: 0.1925 loss_val: 1.8830 acc_val: 0.3118\n",
            "Epoch: 0006 loss_train: 1.8886 acc_train: 0.2768 loss_val: 1.8478 acc_val: 0.4428\n",
            "Epoch: 0007 loss_train: 1.8561 acc_train: 0.3930 loss_val: 1.8078 acc_val: 0.4815\n",
            "Epoch: 0008 loss_train: 1.8199 acc_train: 0.4736 loss_val: 1.7640 acc_val: 0.5055\n",
            "Epoch: 0009 loss_train: 1.7805 acc_train: 0.4889 loss_val: 1.7170 acc_val: 0.5055\n",
            "Epoch: 0010 loss_train: 1.7355 acc_train: 0.5092 loss_val: 1.6678 acc_val: 0.5166\n",
            "Epoch: 0011 loss_train: 1.6880 acc_train: 0.5203 loss_val: 1.6169 acc_val: 0.5277\n",
            "Epoch: 0012 loss_train: 1.6405 acc_train: 0.5295 loss_val: 1.5647 acc_val: 0.5461\n",
            "Epoch: 0013 loss_train: 1.5898 acc_train: 0.5387 loss_val: 1.5112 acc_val: 0.5517\n",
            "Epoch: 0014 loss_train: 1.5383 acc_train: 0.5541 loss_val: 1.4567 acc_val: 0.5627\n",
            "Epoch: 0015 loss_train: 1.4837 acc_train: 0.5658 loss_val: 1.4014 acc_val: 0.5701\n",
            "Epoch: 0016 loss_train: 1.4274 acc_train: 0.5824 loss_val: 1.3452 acc_val: 0.5849\n",
            "Epoch: 0017 loss_train: 1.3709 acc_train: 0.5996 loss_val: 1.2880 acc_val: 0.6070\n",
            "Epoch: 0018 loss_train: 1.3128 acc_train: 0.6193 loss_val: 1.2305 acc_val: 0.6494\n",
            "Epoch: 0019 loss_train: 1.2546 acc_train: 0.6513 loss_val: 1.1740 acc_val: 0.6734\n",
            "Epoch: 0020 loss_train: 1.1946 acc_train: 0.6845 loss_val: 1.1196 acc_val: 0.6882\n",
            "Epoch: 0021 loss_train: 1.1373 acc_train: 0.7103 loss_val: 1.0686 acc_val: 0.6993\n",
            "Epoch: 0022 loss_train: 1.0810 acc_train: 0.7269 loss_val: 1.0214 acc_val: 0.6974\n",
            "Epoch: 0023 loss_train: 1.0270 acc_train: 0.7386 loss_val: 0.9779 acc_val: 0.7103\n",
            "Epoch: 0024 loss_train: 0.9762 acc_train: 0.7491 loss_val: 0.9369 acc_val: 0.7214\n",
            "Epoch: 0025 loss_train: 0.9308 acc_train: 0.7558 loss_val: 0.8978 acc_val: 0.7288\n",
            "Epoch: 0026 loss_train: 0.8845 acc_train: 0.7706 loss_val: 0.8600 acc_val: 0.7306\n",
            "Epoch: 0027 loss_train: 0.8421 acc_train: 0.7811 loss_val: 0.8239 acc_val: 0.7472\n",
            "Epoch: 0028 loss_train: 0.8022 acc_train: 0.8007 loss_val: 0.7899 acc_val: 0.7509\n",
            "Epoch: 0029 loss_train: 0.7630 acc_train: 0.8093 loss_val: 0.7580 acc_val: 0.7638\n",
            "Epoch: 0030 loss_train: 0.7271 acc_train: 0.8167 loss_val: 0.7286 acc_val: 0.7768\n",
            "Epoch: 0031 loss_train: 0.6939 acc_train: 0.8266 loss_val: 0.7010 acc_val: 0.7804\n",
            "Epoch: 0032 loss_train: 0.6632 acc_train: 0.8284 loss_val: 0.6747 acc_val: 0.7823\n",
            "Epoch: 0033 loss_train: 0.6327 acc_train: 0.8346 loss_val: 0.6496 acc_val: 0.7823\n",
            "Epoch: 0034 loss_train: 0.6044 acc_train: 0.8432 loss_val: 0.6263 acc_val: 0.7878\n",
            "Epoch: 0035 loss_train: 0.5759 acc_train: 0.8481 loss_val: 0.6049 acc_val: 0.8044\n",
            "Epoch: 0036 loss_train: 0.5496 acc_train: 0.8524 loss_val: 0.5844 acc_val: 0.8081\n",
            "Epoch: 0037 loss_train: 0.5268 acc_train: 0.8598 loss_val: 0.5635 acc_val: 0.8118\n",
            "Epoch: 0038 loss_train: 0.5036 acc_train: 0.8598 loss_val: 0.5414 acc_val: 0.8192\n",
            "Epoch: 0039 loss_train: 0.4787 acc_train: 0.8647 loss_val: 0.5187 acc_val: 0.8266\n",
            "Epoch: 0040 loss_train: 0.4569 acc_train: 0.8684 loss_val: 0.4966 acc_val: 0.8266\n",
            "Epoch: 0041 loss_train: 0.4365 acc_train: 0.8739 loss_val: 0.4760 acc_val: 0.8303\n",
            "Epoch: 0042 loss_train: 0.4171 acc_train: 0.8733 loss_val: 0.4578 acc_val: 0.8413\n",
            "Epoch: 0043 loss_train: 0.3960 acc_train: 0.8819 loss_val: 0.4426 acc_val: 0.8561\n",
            "Epoch: 0044 loss_train: 0.3774 acc_train: 0.8887 loss_val: 0.4296 acc_val: 0.8635\n",
            "Epoch: 0045 loss_train: 0.3595 acc_train: 0.8918 loss_val: 0.4170 acc_val: 0.8690\n",
            "Epoch: 0046 loss_train: 0.3405 acc_train: 0.8998 loss_val: 0.4041 acc_val: 0.8727\n",
            "Epoch: 0047 loss_train: 0.3229 acc_train: 0.9090 loss_val: 0.3923 acc_val: 0.8745\n",
            "Epoch: 0048 loss_train: 0.3065 acc_train: 0.9157 loss_val: 0.3836 acc_val: 0.8838\n",
            "Epoch: 0049 loss_train: 0.2879 acc_train: 0.9194 loss_val: 0.3782 acc_val: 0.8875\n",
            "Epoch: 0050 loss_train: 0.2725 acc_train: 0.9274 loss_val: 0.3758 acc_val: 0.8893\n",
            "Epoch: 0051 loss_train: 0.2565 acc_train: 0.9293 loss_val: 0.3755 acc_val: 0.8838\n",
            "Epoch: 0052 loss_train: 0.2410 acc_train: 0.9342 loss_val: 0.3742 acc_val: 0.8819\n",
            "Epoch: 0053 loss_train: 0.2288 acc_train: 0.9403 loss_val: 0.3732 acc_val: 0.8838\n",
            "Epoch: 0054 loss_train: 0.2129 acc_train: 0.9434 loss_val: 0.3749 acc_val: 0.8819\n",
            "Epoch: 0055 loss_train: 0.1947 acc_train: 0.9508 loss_val: 0.3805 acc_val: 0.8782\n",
            "Epoch: 0056 loss_train: 0.1813 acc_train: 0.9533 loss_val: 0.3863 acc_val: 0.8801\n",
            "Epoch: 0057 loss_train: 0.1677 acc_train: 0.9576 loss_val: 0.3874 acc_val: 0.8782\n",
            "Epoch: 0058 loss_train: 0.1540 acc_train: 0.9619 loss_val: 0.3872 acc_val: 0.8782\n",
            "Epoch: 0059 loss_train: 0.1432 acc_train: 0.9656 loss_val: 0.3902 acc_val: 0.8782\n",
            "Epoch: 0060 loss_train: 0.1315 acc_train: 0.9686 loss_val: 0.3960 acc_val: 0.8801\n",
            "Epoch: 0061 loss_train: 0.1190 acc_train: 0.9729 loss_val: 0.4023 acc_val: 0.8782\n",
            "Epoch: 0062 loss_train: 0.1102 acc_train: 0.9760 loss_val: 0.4080 acc_val: 0.8764\n",
            "Epoch: 0063 loss_train: 0.1000 acc_train: 0.9822 loss_val: 0.4138 acc_val: 0.8764\n",
            "Epoch: 0064 loss_train: 0.0907 acc_train: 0.9859 loss_val: 0.4196 acc_val: 0.8764\n",
            "Epoch: 0065 loss_train: 0.0822 acc_train: 0.9865 loss_val: 0.4272 acc_val: 0.8801\n",
            "Epoch: 0066 loss_train: 0.0764 acc_train: 0.9852 loss_val: 0.4338 acc_val: 0.8745\n",
            "Epoch: 0067 loss_train: 0.0687 acc_train: 0.9871 loss_val: 0.4419 acc_val: 0.8727\n",
            "Epoch: 0068 loss_train: 0.0618 acc_train: 0.9895 loss_val: 0.4517 acc_val: 0.8708\n",
            "Epoch: 0069 loss_train: 0.0564 acc_train: 0.9914 loss_val: 0.4612 acc_val: 0.8727\n",
            "Epoch: 0070 loss_train: 0.0516 acc_train: 0.9908 loss_val: 0.4702 acc_val: 0.8708\n",
            "Epoch: 0071 loss_train: 0.0470 acc_train: 0.9920 loss_val: 0.4785 acc_val: 0.8690\n",
            "Epoch: 0072 loss_train: 0.0425 acc_train: 0.9926 loss_val: 0.4848 acc_val: 0.8690\n",
            "Epoch: 0073 loss_train: 0.0387 acc_train: 0.9945 loss_val: 0.4902 acc_val: 0.8690\n",
            "Epoch: 0074 loss_train: 0.0355 acc_train: 0.9938 loss_val: 0.4948 acc_val: 0.8672\n",
            "Epoch: 0075 loss_train: 0.0312 acc_train: 0.9945 loss_val: 0.4993 acc_val: 0.8672\n",
            "Epoch: 0076 loss_train: 0.0288 acc_train: 0.9963 loss_val: 0.5046 acc_val: 0.8690\n",
            "Epoch: 0077 loss_train: 0.0262 acc_train: 0.9951 loss_val: 0.5107 acc_val: 0.8690\n",
            "Epoch: 0078 loss_train: 0.0229 acc_train: 0.9988 loss_val: 0.5159 acc_val: 0.8708\n",
            "Epoch: 0079 loss_train: 0.0213 acc_train: 0.9982 loss_val: 0.5216 acc_val: 0.8708\n",
            "Epoch: 0080 loss_train: 0.0198 acc_train: 0.9975 loss_val: 0.5286 acc_val: 0.8708\n",
            "Epoch: 0081 loss_train: 0.0173 acc_train: 0.9982 loss_val: 0.5363 acc_val: 0.8690\n",
            "Epoch: 0082 loss_train: 0.0152 acc_train: 0.9994 loss_val: 0.5455 acc_val: 0.8708\n",
            "Epoch: 0083 loss_train: 0.0139 acc_train: 0.9988 loss_val: 0.5557 acc_val: 0.8727\n",
            "Epoch: 0084 loss_train: 0.0124 acc_train: 0.9994 loss_val: 0.5655 acc_val: 0.8708\n",
            "Epoch: 0085 loss_train: 0.0114 acc_train: 0.9994 loss_val: 0.5740 acc_val: 0.8690\n",
            "Epoch: 0086 loss_train: 0.0101 acc_train: 1.0000 loss_val: 0.5804 acc_val: 0.8690\n",
            "Epoch: 0087 loss_train: 0.0090 acc_train: 1.0000 loss_val: 0.5846 acc_val: 0.8708\n",
            "Epoch: 0088 loss_train: 0.0084 acc_train: 1.0000 loss_val: 0.5882 acc_val: 0.8690\n",
            "Epoch: 0089 loss_train: 0.0082 acc_train: 0.9994 loss_val: 0.5925 acc_val: 0.8708\n",
            "Epoch: 0090 loss_train: 0.0073 acc_train: 1.0000 loss_val: 0.5984 acc_val: 0.8708\n",
            "Epoch: 0091 loss_train: 0.0067 acc_train: 1.0000 loss_val: 0.6056 acc_val: 0.8690\n",
            "Epoch: 0092 loss_train: 0.0060 acc_train: 1.0000 loss_val: 0.6134 acc_val: 0.8690\n",
            "Epoch: 0093 loss_train: 0.0053 acc_train: 1.0000 loss_val: 0.6213 acc_val: 0.8708\n",
            "Epoch: 0094 loss_train: 0.0052 acc_train: 1.0000 loss_val: 0.6275 acc_val: 0.8690\n",
            "Epoch: 0095 loss_train: 0.0047 acc_train: 1.0000 loss_val: 0.6331 acc_val: 0.8672\n",
            "Epoch: 0096 loss_train: 0.0043 acc_train: 1.0000 loss_val: 0.6373 acc_val: 0.8672\n",
            "Epoch: 0097 loss_train: 0.0041 acc_train: 1.0000 loss_val: 0.6404 acc_val: 0.8690\n",
            "Epoch: 0098 loss_train: 0.0040 acc_train: 1.0000 loss_val: 0.6428 acc_val: 0.8672\n",
            "Epoch: 0099 loss_train: 0.0035 acc_train: 1.0000 loss_val: 0.6451 acc_val: 0.8672\n",
            "Epoch: 0100 loss_train: 0.0035 acc_train: 1.0000 loss_val: 0.6471 acc_val: 0.8635\n",
            "Epoch: 0101 loss_train: 0.0032 acc_train: 1.0000 loss_val: 0.6496 acc_val: 0.8635\n",
            "Epoch: 0102 loss_train: 0.0031 acc_train: 1.0000 loss_val: 0.6526 acc_val: 0.8635\n",
            "Epoch: 0103 loss_train: 0.0029 acc_train: 1.0000 loss_val: 0.6555 acc_val: 0.8635\n",
            "Optimization Finished!\n",
            "Train cost: 13.5636s\n",
            "Loading 50th epoch\n",
            "Test set results: loss= 0.3476 accuracy= 0.9019\n",
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=1448, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=7, bias=True)\n",
            ")\n",
            "total params: 2979849\n",
            "Epoch: 0001 loss_train: 1.9696 acc_train: 0.1089 loss_val: 1.9666 acc_val: 0.0978\n",
            "Epoch: 0002 loss_train: 1.9674 acc_train: 0.1113 loss_val: 1.9559 acc_val: 0.0941\n",
            "Epoch: 0003 loss_train: 1.9583 acc_train: 0.1052 loss_val: 1.9406 acc_val: 0.0978\n",
            "Epoch: 0004 loss_train: 1.9446 acc_train: 0.1181 loss_val: 1.9216 acc_val: 0.1624\n",
            "Epoch: 0005 loss_train: 1.9270 acc_train: 0.1531 loss_val: 1.9005 acc_val: 0.2934\n",
            "Epoch: 0006 loss_train: 1.9087 acc_train: 0.2675 loss_val: 1.8791 acc_val: 0.3026\n",
            "Epoch: 0007 loss_train: 1.8889 acc_train: 0.3020 loss_val: 1.8592 acc_val: 0.3026\n",
            "Epoch: 0008 loss_train: 1.8706 acc_train: 0.3020 loss_val: 1.8424 acc_val: 0.3026\n",
            "Epoch: 0009 loss_train: 1.8543 acc_train: 0.3020 loss_val: 1.8294 acc_val: 0.3026\n",
            "Epoch: 0010 loss_train: 1.8396 acc_train: 0.3020 loss_val: 1.8207 acc_val: 0.3026\n",
            "Epoch: 0011 loss_train: 1.8296 acc_train: 0.3020 loss_val: 1.8154 acc_val: 0.3026\n",
            "Epoch: 0012 loss_train: 1.8234 acc_train: 0.3020 loss_val: 1.8123 acc_val: 0.3026\n",
            "Epoch: 0013 loss_train: 1.8195 acc_train: 0.3020 loss_val: 1.8094 acc_val: 0.3026\n",
            "Epoch: 0014 loss_train: 1.8152 acc_train: 0.3020 loss_val: 1.8054 acc_val: 0.3026\n",
            "Epoch: 0015 loss_train: 1.8104 acc_train: 0.3020 loss_val: 1.7997 acc_val: 0.3026\n",
            "Epoch: 0016 loss_train: 1.8053 acc_train: 0.3020 loss_val: 1.7923 acc_val: 0.3026\n",
            "Epoch: 0017 loss_train: 1.7985 acc_train: 0.3020 loss_val: 1.7842 acc_val: 0.3026\n",
            "Epoch: 0018 loss_train: 1.7929 acc_train: 0.3020 loss_val: 1.7759 acc_val: 0.3026\n",
            "Epoch: 0019 loss_train: 1.7855 acc_train: 0.3020 loss_val: 1.7670 acc_val: 0.3026\n",
            "Epoch: 0020 loss_train: 1.7805 acc_train: 0.3026 loss_val: 1.7565 acc_val: 0.3044\n",
            "Epoch: 0021 loss_train: 1.7722 acc_train: 0.3063 loss_val: 1.7428 acc_val: 0.3155\n",
            "Epoch: 0022 loss_train: 1.7603 acc_train: 0.3130 loss_val: 1.7235 acc_val: 0.3192\n",
            "Epoch: 0023 loss_train: 1.7437 acc_train: 0.3167 loss_val: 1.6983 acc_val: 0.3192\n",
            "Epoch: 0024 loss_train: 1.7244 acc_train: 0.3161 loss_val: 1.6678 acc_val: 0.3192\n",
            "Epoch: 0025 loss_train: 1.6992 acc_train: 0.3180 loss_val: 1.6288 acc_val: 0.3303\n",
            "Epoch: 0026 loss_train: 1.6666 acc_train: 0.3321 loss_val: 1.5743 acc_val: 0.3801\n",
            "Epoch: 0027 loss_train: 1.6239 acc_train: 0.3678 loss_val: 1.5046 acc_val: 0.4649\n",
            "Epoch: 0028 loss_train: 1.5685 acc_train: 0.4459 loss_val: 1.4297 acc_val: 0.5554\n",
            "Epoch: 0029 loss_train: 1.5119 acc_train: 0.5135 loss_val: 1.3441 acc_val: 0.5701\n",
            "Epoch: 0030 loss_train: 1.4440 acc_train: 0.5332 loss_val: 1.2657 acc_val: 0.5849\n",
            "Epoch: 0031 loss_train: 1.3793 acc_train: 0.5418 loss_val: 1.1824 acc_val: 0.6439\n",
            "Epoch: 0032 loss_train: 1.3154 acc_train: 0.6052 loss_val: 1.1162 acc_val: 0.6937\n",
            "Epoch: 0033 loss_train: 1.2614 acc_train: 0.6513 loss_val: 1.0581 acc_val: 0.6956\n",
            "Epoch: 0034 loss_train: 1.2083 acc_train: 0.6648 loss_val: 1.0089 acc_val: 0.7066\n",
            "Epoch: 0035 loss_train: 1.1550 acc_train: 0.6697 loss_val: 0.9518 acc_val: 0.7509\n",
            "Epoch: 0036 loss_train: 1.1059 acc_train: 0.7005 loss_val: 0.8974 acc_val: 0.7675\n",
            "Epoch: 0037 loss_train: 1.0518 acc_train: 0.7134 loss_val: 0.8466 acc_val: 0.7694\n",
            "Epoch: 0038 loss_train: 0.9998 acc_train: 0.7220 loss_val: 0.7953 acc_val: 0.7915\n",
            "Epoch: 0039 loss_train: 0.9459 acc_train: 0.7399 loss_val: 0.7499 acc_val: 0.8081\n",
            "Epoch: 0040 loss_train: 0.8996 acc_train: 0.7583 loss_val: 0.7084 acc_val: 0.7989\n",
            "Epoch: 0041 loss_train: 0.8496 acc_train: 0.7558 loss_val: 0.6682 acc_val: 0.8137\n",
            "Epoch: 0042 loss_train: 0.7999 acc_train: 0.7724 loss_val: 0.6334 acc_val: 0.8192\n",
            "Epoch: 0043 loss_train: 0.7569 acc_train: 0.7878 loss_val: 0.6013 acc_val: 0.8266\n",
            "Epoch: 0044 loss_train: 0.7147 acc_train: 0.7940 loss_val: 0.5711 acc_val: 0.8358\n",
            "Epoch: 0045 loss_train: 0.6784 acc_train: 0.8044 loss_val: 0.5457 acc_val: 0.8303\n",
            "Epoch: 0046 loss_train: 0.6333 acc_train: 0.8173 loss_val: 0.5234 acc_val: 0.8321\n",
            "Epoch: 0047 loss_train: 0.5971 acc_train: 0.8266 loss_val: 0.5023 acc_val: 0.8376\n",
            "Epoch: 0048 loss_train: 0.5597 acc_train: 0.8339 loss_val: 0.4868 acc_val: 0.8469\n",
            "Epoch: 0049 loss_train: 0.5277 acc_train: 0.8462 loss_val: 0.4751 acc_val: 0.8450\n",
            "Epoch: 0050 loss_train: 0.4953 acc_train: 0.8542 loss_val: 0.4610 acc_val: 0.8487\n",
            "Epoch: 0051 loss_train: 0.4664 acc_train: 0.8616 loss_val: 0.4533 acc_val: 0.8524\n",
            "Epoch: 0052 loss_train: 0.4391 acc_train: 0.8665 loss_val: 0.4444 acc_val: 0.8561\n",
            "Epoch: 0053 loss_train: 0.4110 acc_train: 0.8733 loss_val: 0.4421 acc_val: 0.8542\n",
            "Epoch: 0054 loss_train: 0.3852 acc_train: 0.8844 loss_val: 0.4376 acc_val: 0.8598\n",
            "Epoch: 0055 loss_train: 0.3604 acc_train: 0.8936 loss_val: 0.4344 acc_val: 0.8616\n",
            "Epoch: 0056 loss_train: 0.3358 acc_train: 0.8998 loss_val: 0.4313 acc_val: 0.8598\n",
            "Epoch: 0057 loss_train: 0.3145 acc_train: 0.9065 loss_val: 0.4325 acc_val: 0.8579\n",
            "Epoch: 0058 loss_train: 0.2965 acc_train: 0.9102 loss_val: 0.4356 acc_val: 0.8561\n",
            "Epoch: 0059 loss_train: 0.2759 acc_train: 0.9170 loss_val: 0.4380 acc_val: 0.8598\n",
            "Epoch: 0060 loss_train: 0.2562 acc_train: 0.9244 loss_val: 0.4402 acc_val: 0.8579\n",
            "Epoch: 0061 loss_train: 0.2399 acc_train: 0.9287 loss_val: 0.4436 acc_val: 0.8542\n",
            "Epoch: 0062 loss_train: 0.2246 acc_train: 0.9305 loss_val: 0.4511 acc_val: 0.8561\n",
            "Epoch: 0063 loss_train: 0.2076 acc_train: 0.9354 loss_val: 0.4592 acc_val: 0.8598\n",
            "Epoch: 0064 loss_train: 0.1943 acc_train: 0.9416 loss_val: 0.4669 acc_val: 0.8635\n",
            "Epoch: 0065 loss_train: 0.1830 acc_train: 0.9434 loss_val: 0.4786 acc_val: 0.8561\n",
            "Epoch: 0066 loss_train: 0.1703 acc_train: 0.9508 loss_val: 0.4891 acc_val: 0.8524\n",
            "Epoch: 0067 loss_train: 0.1579 acc_train: 0.9569 loss_val: 0.5013 acc_val: 0.8469\n",
            "Epoch: 0068 loss_train: 0.1456 acc_train: 0.9606 loss_val: 0.5142 acc_val: 0.8487\n",
            "Epoch: 0069 loss_train: 0.1367 acc_train: 0.9625 loss_val: 0.5266 acc_val: 0.8469\n",
            "Epoch: 0070 loss_train: 0.1259 acc_train: 0.9686 loss_val: 0.5417 acc_val: 0.8450\n",
            "Epoch: 0071 loss_train: 0.1181 acc_train: 0.9729 loss_val: 0.5587 acc_val: 0.8469\n",
            "Epoch: 0072 loss_train: 0.1073 acc_train: 0.9772 loss_val: 0.5768 acc_val: 0.8469\n",
            "Epoch: 0073 loss_train: 0.0995 acc_train: 0.9828 loss_val: 0.5897 acc_val: 0.8395\n",
            "Epoch: 0074 loss_train: 0.0888 acc_train: 0.9852 loss_val: 0.6052 acc_val: 0.8395\n",
            "Epoch: 0075 loss_train: 0.0822 acc_train: 0.9859 loss_val: 0.6191 acc_val: 0.8413\n",
            "Epoch: 0076 loss_train: 0.0747 acc_train: 0.9889 loss_val: 0.6367 acc_val: 0.8395\n",
            "Epoch: 0077 loss_train: 0.0672 acc_train: 0.9908 loss_val: 0.6565 acc_val: 0.8413\n",
            "Epoch: 0078 loss_train: 0.0620 acc_train: 0.9908 loss_val: 0.6687 acc_val: 0.8395\n",
            "Epoch: 0079 loss_train: 0.0538 acc_train: 0.9957 loss_val: 0.6778 acc_val: 0.8358\n",
            "Epoch: 0080 loss_train: 0.0488 acc_train: 0.9957 loss_val: 0.6871 acc_val: 0.8358\n",
            "Epoch: 0081 loss_train: 0.0433 acc_train: 0.9969 loss_val: 0.6998 acc_val: 0.8358\n",
            "Epoch: 0082 loss_train: 0.0387 acc_train: 0.9969 loss_val: 0.7137 acc_val: 0.8358\n",
            "Epoch: 0083 loss_train: 0.0345 acc_train: 0.9969 loss_val: 0.7267 acc_val: 0.8376\n",
            "Epoch: 0084 loss_train: 0.0313 acc_train: 0.9975 loss_val: 0.7368 acc_val: 0.8339\n",
            "Epoch: 0085 loss_train: 0.0267 acc_train: 0.9994 loss_val: 0.7441 acc_val: 0.8321\n",
            "Epoch: 0086 loss_train: 0.0237 acc_train: 0.9988 loss_val: 0.7503 acc_val: 0.8284\n",
            "Epoch: 0087 loss_train: 0.0210 acc_train: 0.9994 loss_val: 0.7579 acc_val: 0.8303\n",
            "Epoch: 0088 loss_train: 0.0190 acc_train: 0.9994 loss_val: 0.7666 acc_val: 0.8266\n",
            "Epoch: 0089 loss_train: 0.0169 acc_train: 0.9994 loss_val: 0.7744 acc_val: 0.8284\n",
            "Epoch: 0090 loss_train: 0.0162 acc_train: 0.9994 loss_val: 0.7842 acc_val: 0.8321\n",
            "Epoch: 0091 loss_train: 0.0131 acc_train: 0.9994 loss_val: 0.7945 acc_val: 0.8303\n",
            "Epoch: 0092 loss_train: 0.0117 acc_train: 0.9994 loss_val: 0.8034 acc_val: 0.8247\n",
            "Epoch: 0093 loss_train: 0.0104 acc_train: 0.9994 loss_val: 0.8086 acc_val: 0.8247\n",
            "Epoch: 0094 loss_train: 0.0088 acc_train: 0.9994 loss_val: 0.8122 acc_val: 0.8266\n",
            "Epoch: 0095 loss_train: 0.0081 acc_train: 0.9994 loss_val: 0.8166 acc_val: 0.8247\n",
            "Epoch: 0096 loss_train: 0.0072 acc_train: 0.9994 loss_val: 0.8201 acc_val: 0.8229\n",
            "Epoch: 0097 loss_train: 0.0069 acc_train: 0.9994 loss_val: 0.8240 acc_val: 0.8284\n",
            "Epoch: 0098 loss_train: 0.0057 acc_train: 1.0000 loss_val: 0.8280 acc_val: 0.8303\n",
            "Epoch: 0099 loss_train: 0.0051 acc_train: 1.0000 loss_val: 0.8331 acc_val: 0.8266\n",
            "Epoch: 0100 loss_train: 0.0046 acc_train: 1.0000 loss_val: 0.8393 acc_val: 0.8266\n",
            "Epoch: 0101 loss_train: 0.0042 acc_train: 1.0000 loss_val: 0.8455 acc_val: 0.8229\n",
            "Epoch: 0102 loss_train: 0.0037 acc_train: 1.0000 loss_val: 0.8533 acc_val: 0.8284\n",
            "Epoch: 0103 loss_train: 0.0034 acc_train: 1.0000 loss_val: 0.8597 acc_val: 0.8247\n",
            "Epoch: 0104 loss_train: 0.0030 acc_train: 1.0000 loss_val: 0.8637 acc_val: 0.8247\n",
            "Epoch: 0105 loss_train: 0.0029 acc_train: 1.0000 loss_val: 0.8640 acc_val: 0.8229\n",
            "Epoch: 0106 loss_train: 0.0027 acc_train: 1.0000 loss_val: 0.8658 acc_val: 0.8266\n",
            "Epoch: 0107 loss_train: 0.0022 acc_train: 1.0000 loss_val: 0.8690 acc_val: 0.8284\n",
            "Epoch: 0108 loss_train: 0.0023 acc_train: 1.0000 loss_val: 0.8737 acc_val: 0.8266\n",
            "Epoch: 0109 loss_train: 0.0024 acc_train: 1.0000 loss_val: 0.8794 acc_val: 0.8284\n",
            "Epoch: 0110 loss_train: 0.0029 acc_train: 0.9994 loss_val: 0.8948 acc_val: 0.8284\n",
            "Epoch: 0111 loss_train: 0.0019 acc_train: 1.0000 loss_val: 0.9121 acc_val: 0.8266\n",
            "Epoch: 0112 loss_train: 0.0019 acc_train: 1.0000 loss_val: 0.9278 acc_val: 0.8247\n",
            "Epoch: 0113 loss_train: 0.0018 acc_train: 1.0000 loss_val: 0.9366 acc_val: 0.8247\n",
            "Epoch: 0114 loss_train: 0.0026 acc_train: 0.9994 loss_val: 0.9374 acc_val: 0.8210\n",
            "Optimization Finished!\n",
            "Train cost: 15.0210s\n",
            "Loading 64th epoch\n",
            "Test set results: loss= 0.4508 accuracy= 0.8667\n",
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=1448, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=7, bias=True)\n",
            ")\n",
            "total params: 2979849\n",
            "Epoch: 0001 loss_train: 1.9685 acc_train: 0.1193 loss_val: 1.9664 acc_val: 0.1162\n",
            "Epoch: 0002 loss_train: 1.9660 acc_train: 0.1119 loss_val: 1.9558 acc_val: 0.1199\n",
            "Epoch: 0003 loss_train: 1.9578 acc_train: 0.1169 loss_val: 1.9409 acc_val: 0.1310\n",
            "Epoch: 0004 loss_train: 1.9441 acc_train: 0.1138 loss_val: 1.9224 acc_val: 0.1273\n",
            "Epoch: 0005 loss_train: 1.9276 acc_train: 0.1365 loss_val: 1.9022 acc_val: 0.3026\n",
            "Epoch: 0006 loss_train: 1.9099 acc_train: 0.2780 loss_val: 1.8822 acc_val: 0.3026\n",
            "Epoch: 0007 loss_train: 1.8914 acc_train: 0.3020 loss_val: 1.8641 acc_val: 0.3026\n",
            "Epoch: 0008 loss_train: 1.8743 acc_train: 0.3020 loss_val: 1.8486 acc_val: 0.3026\n",
            "Epoch: 0009 loss_train: 1.8594 acc_train: 0.3020 loss_val: 1.8373 acc_val: 0.3026\n",
            "Epoch: 0010 loss_train: 1.8467 acc_train: 0.3020 loss_val: 1.8294 acc_val: 0.3026\n",
            "Epoch: 0011 loss_train: 1.8385 acc_train: 0.3020 loss_val: 1.8248 acc_val: 0.3026\n",
            "Epoch: 0012 loss_train: 1.8330 acc_train: 0.3020 loss_val: 1.8229 acc_val: 0.3026\n",
            "Epoch: 0013 loss_train: 1.8310 acc_train: 0.3020 loss_val: 1.8216 acc_val: 0.3026\n",
            "Epoch: 0014 loss_train: 1.8283 acc_train: 0.3020 loss_val: 1.8199 acc_val: 0.3026\n",
            "Epoch: 0015 loss_train: 1.8265 acc_train: 0.3020 loss_val: 1.8171 acc_val: 0.3026\n",
            "Epoch: 0016 loss_train: 1.8248 acc_train: 0.3020 loss_val: 1.8131 acc_val: 0.3026\n",
            "Epoch: 0017 loss_train: 1.8214 acc_train: 0.3020 loss_val: 1.8084 acc_val: 0.3026\n",
            "Epoch: 0018 loss_train: 1.8194 acc_train: 0.3020 loss_val: 1.8033 acc_val: 0.3026\n",
            "Epoch: 0019 loss_train: 1.8157 acc_train: 0.3020 loss_val: 1.7979 acc_val: 0.3026\n",
            "Epoch: 0020 loss_train: 1.8148 acc_train: 0.3020 loss_val: 1.7918 acc_val: 0.3026\n",
            "Epoch: 0021 loss_train: 1.8110 acc_train: 0.3020 loss_val: 1.7845 acc_val: 0.3026\n",
            "Epoch: 0022 loss_train: 1.8061 acc_train: 0.3020 loss_val: 1.7754 acc_val: 0.3026\n",
            "Epoch: 0023 loss_train: 1.8001 acc_train: 0.3020 loss_val: 1.7643 acc_val: 0.3026\n",
            "Epoch: 0024 loss_train: 1.7936 acc_train: 0.3020 loss_val: 1.7518 acc_val: 0.3026\n",
            "Epoch: 0025 loss_train: 1.7839 acc_train: 0.3020 loss_val: 1.7383 acc_val: 0.3026\n",
            "Epoch: 0026 loss_train: 1.7745 acc_train: 0.3020 loss_val: 1.7224 acc_val: 0.3026\n",
            "Epoch: 0027 loss_train: 1.7636 acc_train: 0.3020 loss_val: 1.7005 acc_val: 0.3026\n",
            "Epoch: 0028 loss_train: 1.7488 acc_train: 0.3020 loss_val: 1.6703 acc_val: 0.3026\n",
            "Epoch: 0029 loss_train: 1.7315 acc_train: 0.3020 loss_val: 1.6330 acc_val: 0.3026\n",
            "Epoch: 0030 loss_train: 1.7087 acc_train: 0.3026 loss_val: 1.5901 acc_val: 0.3358\n",
            "Epoch: 0031 loss_train: 1.6847 acc_train: 0.3376 loss_val: 1.5381 acc_val: 0.4631\n",
            "Epoch: 0032 loss_train: 1.6544 acc_train: 0.3954 loss_val: 1.4799 acc_val: 0.4815\n",
            "Epoch: 0033 loss_train: 1.6186 acc_train: 0.4176 loss_val: 1.4184 acc_val: 0.5000\n",
            "Epoch: 0034 loss_train: 1.5794 acc_train: 0.4182 loss_val: 1.3405 acc_val: 0.5904\n",
            "Epoch: 0035 loss_train: 1.5365 acc_train: 0.4619 loss_val: 1.2625 acc_val: 0.6181\n",
            "Epoch: 0036 loss_train: 1.4989 acc_train: 0.4668 loss_val: 1.1899 acc_val: 0.6587\n",
            "Epoch: 0037 loss_train: 1.4559 acc_train: 0.5000 loss_val: 1.1226 acc_val: 0.7122\n",
            "Epoch: 0038 loss_train: 1.4132 acc_train: 0.5394 loss_val: 1.0479 acc_val: 0.7565\n",
            "Epoch: 0039 loss_train: 1.3690 acc_train: 0.5523 loss_val: 0.9806 acc_val: 0.7749\n",
            "Epoch: 0040 loss_train: 1.3275 acc_train: 0.5633 loss_val: 0.9274 acc_val: 0.8100\n",
            "Epoch: 0041 loss_train: 1.2854 acc_train: 0.6150 loss_val: 0.8561 acc_val: 0.8395\n",
            "Epoch: 0042 loss_train: 1.2411 acc_train: 0.6089 loss_val: 0.7970 acc_val: 0.8782\n",
            "Epoch: 0043 loss_train: 1.1975 acc_train: 0.6482 loss_val: 0.7302 acc_val: 0.8911\n",
            "Epoch: 0044 loss_train: 1.1505 acc_train: 0.6556 loss_val: 0.6599 acc_val: 0.8893\n",
            "Epoch: 0045 loss_train: 1.1020 acc_train: 0.6568 loss_val: 0.6279 acc_val: 0.8893\n",
            "Epoch: 0046 loss_train: 1.0643 acc_train: 0.6820 loss_val: 0.5890 acc_val: 0.8930\n",
            "Epoch: 0047 loss_train: 1.0593 acc_train: 0.6427 loss_val: 0.5063 acc_val: 0.9059\n",
            "Epoch: 0048 loss_train: 0.9747 acc_train: 0.6931 loss_val: 0.5784 acc_val: 0.8579\n",
            "Epoch: 0049 loss_train: 0.9953 acc_train: 0.6827 loss_val: 0.4696 acc_val: 0.9059\n",
            "Epoch: 0050 loss_train: 0.9324 acc_train: 0.7239 loss_val: 0.5061 acc_val: 0.8764\n",
            "Epoch: 0051 loss_train: 0.9544 acc_train: 0.6507 loss_val: 0.3876 acc_val: 0.9225\n",
            "Epoch: 0052 loss_train: 0.8362 acc_train: 0.7485 loss_val: 0.3963 acc_val: 0.9317\n",
            "Epoch: 0053 loss_train: 0.8151 acc_train: 0.7552 loss_val: 0.3932 acc_val: 0.9354\n",
            "Epoch: 0054 loss_train: 0.7994 acc_train: 0.7417 loss_val: 0.3119 acc_val: 0.9520\n",
            "Epoch: 0055 loss_train: 0.7132 acc_train: 0.7915 loss_val: 0.3151 acc_val: 0.9373\n",
            "Epoch: 0056 loss_train: 0.7026 acc_train: 0.7958 loss_val: 0.3032 acc_val: 0.9373\n",
            "Epoch: 0057 loss_train: 0.6773 acc_train: 0.8007 loss_val: 0.2662 acc_val: 0.9446\n",
            "Epoch: 0058 loss_train: 0.6202 acc_train: 0.8247 loss_val: 0.2330 acc_val: 0.9631\n",
            "Epoch: 0059 loss_train: 0.5687 acc_train: 0.8481 loss_val: 0.2347 acc_val: 0.9705\n",
            "Epoch: 0060 loss_train: 0.5539 acc_train: 0.8462 loss_val: 0.2110 acc_val: 0.9742\n",
            "Epoch: 0061 loss_train: 0.5077 acc_train: 0.8690 loss_val: 0.1814 acc_val: 0.9815\n",
            "Epoch: 0062 loss_train: 0.4586 acc_train: 0.8795 loss_val: 0.1742 acc_val: 0.9815\n",
            "Epoch: 0063 loss_train: 0.4347 acc_train: 0.8795 loss_val: 0.1610 acc_val: 0.9705\n",
            "Epoch: 0064 loss_train: 0.3989 acc_train: 0.8930 loss_val: 0.1514 acc_val: 0.9723\n",
            "Epoch: 0065 loss_train: 0.3649 acc_train: 0.9034 loss_val: 0.1384 acc_val: 0.9815\n",
            "Epoch: 0066 loss_train: 0.3299 acc_train: 0.9157 loss_val: 0.1242 acc_val: 0.9889\n",
            "Epoch: 0067 loss_train: 0.2976 acc_train: 0.9268 loss_val: 0.1211 acc_val: 0.9908\n",
            "Epoch: 0068 loss_train: 0.2679 acc_train: 0.9397 loss_val: 0.1099 acc_val: 0.9797\n",
            "Epoch: 0069 loss_train: 0.2383 acc_train: 0.9403 loss_val: 0.0972 acc_val: 0.9797\n",
            "Epoch: 0070 loss_train: 0.2056 acc_train: 0.9533 loss_val: 0.0963 acc_val: 0.9797\n",
            "Epoch: 0071 loss_train: 0.1770 acc_train: 0.9631 loss_val: 0.0976 acc_val: 0.9760\n",
            "Epoch: 0072 loss_train: 0.1516 acc_train: 0.9692 loss_val: 0.0919 acc_val: 0.9797\n",
            "Epoch: 0073 loss_train: 0.1237 acc_train: 0.9803 loss_val: 0.0967 acc_val: 0.9723\n",
            "Epoch: 0074 loss_train: 0.0997 acc_train: 0.9852 loss_val: 0.1013 acc_val: 0.9742\n",
            "Epoch: 0075 loss_train: 0.0834 acc_train: 0.9914 loss_val: 0.1138 acc_val: 0.9613\n",
            "Epoch: 0076 loss_train: 0.0728 acc_train: 0.9938 loss_val: 0.1215 acc_val: 0.9594\n",
            "Epoch: 0077 loss_train: 0.0604 acc_train: 0.9902 loss_val: 0.1026 acc_val: 0.9613\n",
            "Epoch: 0078 loss_train: 0.0438 acc_train: 0.9969 loss_val: 0.1103 acc_val: 0.9576\n",
            "Epoch: 0079 loss_train: 0.0411 acc_train: 0.9975 loss_val: 0.1182 acc_val: 0.9557\n",
            "Epoch: 0080 loss_train: 0.0341 acc_train: 0.9951 loss_val: 0.1192 acc_val: 0.9557\n",
            "Epoch: 0081 loss_train: 0.0263 acc_train: 0.9975 loss_val: 0.1049 acc_val: 0.9631\n",
            "Epoch: 0082 loss_train: 0.0214 acc_train: 1.0000 loss_val: 0.0964 acc_val: 0.9668\n",
            "Epoch: 0083 loss_train: 0.0161 acc_train: 1.0000 loss_val: 0.1031 acc_val: 0.9668\n",
            "Epoch: 0084 loss_train: 0.0139 acc_train: 0.9988 loss_val: 0.0981 acc_val: 0.9723\n",
            "Epoch: 0085 loss_train: 0.0117 acc_train: 1.0000 loss_val: 0.0889 acc_val: 0.9668\n",
            "Epoch: 0086 loss_train: 0.0092 acc_train: 0.9994 loss_val: 0.0895 acc_val: 0.9649\n",
            "Epoch: 0087 loss_train: 0.0087 acc_train: 1.0000 loss_val: 0.0818 acc_val: 0.9705\n",
            "Epoch: 0088 loss_train: 0.0069 acc_train: 1.0000 loss_val: 0.0749 acc_val: 0.9705\n",
            "Epoch: 0089 loss_train: 0.0055 acc_train: 1.0000 loss_val: 0.0790 acc_val: 0.9797\n",
            "Epoch: 0090 loss_train: 0.0055 acc_train: 1.0000 loss_val: 0.0803 acc_val: 0.9797\n",
            "Epoch: 0091 loss_train: 0.0048 acc_train: 1.0000 loss_val: 0.0743 acc_val: 0.9760\n",
            "Epoch: 0092 loss_train: 0.0042 acc_train: 1.0000 loss_val: 0.0672 acc_val: 0.9760\n",
            "Epoch: 0093 loss_train: 0.0036 acc_train: 1.0000 loss_val: 0.0660 acc_val: 0.9779\n",
            "Epoch: 0094 loss_train: 0.0036 acc_train: 1.0000 loss_val: 0.0667 acc_val: 0.9797\n",
            "Epoch: 0095 loss_train: 0.0033 acc_train: 1.0000 loss_val: 0.0669 acc_val: 0.9760\n",
            "Epoch: 0096 loss_train: 0.0028 acc_train: 1.0000 loss_val: 0.0693 acc_val: 0.9742\n",
            "Epoch: 0097 loss_train: 0.0027 acc_train: 1.0000 loss_val: 0.0738 acc_val: 0.9797\n",
            "Epoch: 0098 loss_train: 0.0022 acc_train: 1.0000 loss_val: 0.0784 acc_val: 0.9779\n",
            "Epoch: 0099 loss_train: 0.0021 acc_train: 1.0000 loss_val: 0.0784 acc_val: 0.9797\n",
            "Epoch: 0100 loss_train: 0.0021 acc_train: 1.0000 loss_val: 0.0738 acc_val: 0.9815\n",
            "Epoch: 0101 loss_train: 0.0018 acc_train: 1.0000 loss_val: 0.0681 acc_val: 0.9815\n",
            "Epoch: 0102 loss_train: 0.0016 acc_train: 1.0000 loss_val: 0.0628 acc_val: 0.9797\n",
            "Epoch: 0103 loss_train: 0.0015 acc_train: 1.0000 loss_val: 0.0594 acc_val: 0.9797\n",
            "Epoch: 0104 loss_train: 0.0014 acc_train: 1.0000 loss_val: 0.0574 acc_val: 0.9815\n",
            "Epoch: 0105 loss_train: 0.0015 acc_train: 1.0000 loss_val: 0.0566 acc_val: 0.9852\n",
            "Epoch: 0106 loss_train: 0.0014 acc_train: 1.0000 loss_val: 0.0566 acc_val: 0.9834\n",
            "Epoch: 0107 loss_train: 0.0012 acc_train: 1.0000 loss_val: 0.0574 acc_val: 0.9834\n",
            "Epoch: 0108 loss_train: 0.0011 acc_train: 1.0000 loss_val: 0.0593 acc_val: 0.9834\n",
            "Epoch: 0109 loss_train: 0.0011 acc_train: 1.0000 loss_val: 0.0620 acc_val: 0.9834\n",
            "Epoch: 0110 loss_train: 0.0011 acc_train: 1.0000 loss_val: 0.0648 acc_val: 0.9815\n",
            "Epoch: 0111 loss_train: 0.0010 acc_train: 1.0000 loss_val: 0.0669 acc_val: 0.9815\n",
            "Epoch: 0112 loss_train: 0.0010 acc_train: 1.0000 loss_val: 0.0682 acc_val: 0.9834\n",
            "Epoch: 0113 loss_train: 0.0010 acc_train: 1.0000 loss_val: 0.0682 acc_val: 0.9834\n",
            "Epoch: 0114 loss_train: 0.0009 acc_train: 1.0000 loss_val: 0.0673 acc_val: 0.9834\n",
            "Epoch: 0115 loss_train: 0.0009 acc_train: 1.0000 loss_val: 0.0656 acc_val: 0.9815\n",
            "Epoch: 0116 loss_train: 0.0009 acc_train: 1.0000 loss_val: 0.0631 acc_val: 0.9815\n",
            "Epoch: 0117 loss_train: 0.0009 acc_train: 1.0000 loss_val: 0.0603 acc_val: 0.9834\n",
            "Epoch: 0118 loss_train: 0.0008 acc_train: 1.0000 loss_val: 0.0582 acc_val: 0.9834\n",
            "Epoch: 0119 loss_train: 0.0008 acc_train: 1.0000 loss_val: 0.0567 acc_val: 0.9834\n",
            "Epoch: 0120 loss_train: 0.0008 acc_train: 1.0000 loss_val: 0.0561 acc_val: 0.9834\n",
            "Epoch: 0121 loss_train: 0.0008 acc_train: 1.0000 loss_val: 0.0560 acc_val: 0.9834\n",
            "Epoch: 0122 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0562 acc_val: 0.9834\n",
            "Epoch: 0123 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0567 acc_val: 0.9834\n",
            "Epoch: 0124 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0575 acc_val: 0.9834\n",
            "Epoch: 0125 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0584 acc_val: 0.9834\n",
            "Epoch: 0126 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0589 acc_val: 0.9834\n",
            "Epoch: 0127 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0592 acc_val: 0.9834\n",
            "Epoch: 0128 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0596 acc_val: 0.9834\n",
            "Epoch: 0129 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0600 acc_val: 0.9834\n",
            "Epoch: 0130 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0601 acc_val: 0.9834\n",
            "Epoch: 0131 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0599 acc_val: 0.9852\n",
            "Epoch: 0132 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0590 acc_val: 0.9834\n",
            "Epoch: 0133 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0584 acc_val: 0.9834\n",
            "Epoch: 0134 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0572 acc_val: 0.9834\n",
            "Epoch: 0135 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0560 acc_val: 0.9834\n",
            "Epoch: 0136 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0547 acc_val: 0.9834\n",
            "Epoch: 0137 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0537 acc_val: 0.9834\n",
            "Epoch: 0138 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0532 acc_val: 0.9834\n",
            "Epoch: 0139 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0529 acc_val: 0.9834\n",
            "Epoch: 0140 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0532 acc_val: 0.9834\n",
            "Epoch: 0141 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0535 acc_val: 0.9834\n",
            "Epoch: 0142 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0532 acc_val: 0.9834\n",
            "Epoch: 0143 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0530 acc_val: 0.9834\n",
            "Epoch: 0144 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0528 acc_val: 0.9834\n",
            "Epoch: 0145 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0529 acc_val: 0.9834\n",
            "Epoch: 0146 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0530 acc_val: 0.9834\n",
            "Epoch: 0147 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0532 acc_val: 0.9834\n",
            "Epoch: 0148 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0537 acc_val: 0.9834\n",
            "Epoch: 0149 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0539 acc_val: 0.9834\n",
            "Epoch: 0150 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0543 acc_val: 0.9834\n",
            "Epoch: 0151 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0549 acc_val: 0.9852\n",
            "Epoch: 0152 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0551 acc_val: 0.9852\n",
            "Epoch: 0153 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0551 acc_val: 0.9852\n",
            "Epoch: 0154 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0549 acc_val: 0.9852\n",
            "Epoch: 0155 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0546 acc_val: 0.9852\n",
            "Epoch: 0156 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0543 acc_val: 0.9834\n",
            "Epoch: 0157 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0542 acc_val: 0.9834\n",
            "Epoch: 0158 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0540 acc_val: 0.9834\n",
            "Epoch: 0159 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0537 acc_val: 0.9834\n",
            "Epoch: 0160 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0534 acc_val: 0.9834\n",
            "Epoch: 0161 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0533 acc_val: 0.9834\n",
            "Epoch: 0162 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0532 acc_val: 0.9834\n",
            "Epoch: 0163 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0535 acc_val: 0.9852\n",
            "Epoch: 0164 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0536 acc_val: 0.9852\n",
            "Epoch: 0165 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0543 acc_val: 0.9852\n",
            "Epoch: 0166 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0550 acc_val: 0.9871\n",
            "Epoch: 0167 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0555 acc_val: 0.9871\n",
            "Epoch: 0168 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0560 acc_val: 0.9871\n",
            "Epoch: 0169 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0556 acc_val: 0.9871\n",
            "Epoch: 0170 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0547 acc_val: 0.9871\n",
            "Epoch: 0171 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0539 acc_val: 0.9852\n",
            "Epoch: 0172 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0525 acc_val: 0.9852\n",
            "Epoch: 0173 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0514 acc_val: 0.9852\n",
            "Epoch: 0174 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0505 acc_val: 0.9852\n",
            "Epoch: 0175 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0499 acc_val: 0.9852\n",
            "Epoch: 0176 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0497 acc_val: 0.9852\n",
            "Epoch: 0177 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0498 acc_val: 0.9852\n",
            "Epoch: 0178 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0507 acc_val: 0.9852\n",
            "Epoch: 0179 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0519 acc_val: 0.9852\n",
            "Epoch: 0180 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0533 acc_val: 0.9871\n",
            "Epoch: 0181 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0549 acc_val: 0.9871\n",
            "Epoch: 0182 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0561 acc_val: 0.9871\n",
            "Epoch: 0183 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0569 acc_val: 0.9871\n",
            "Epoch: 0184 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0572 acc_val: 0.9871\n",
            "Epoch: 0185 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0569 acc_val: 0.9871\n",
            "Epoch: 0186 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0559 acc_val: 0.9871\n",
            "Epoch: 0187 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0547 acc_val: 0.9871\n",
            "Epoch: 0188 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0534 acc_val: 0.9871\n",
            "Epoch: 0189 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0520 acc_val: 0.9871\n",
            "Epoch: 0190 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0505 acc_val: 0.9871\n",
            "Epoch: 0191 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0494 acc_val: 0.9852\n",
            "Epoch: 0192 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0487 acc_val: 0.9852\n",
            "Epoch: 0193 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0485 acc_val: 0.9852\n",
            "Epoch: 0194 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0489 acc_val: 0.9852\n",
            "Epoch: 0195 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0496 acc_val: 0.9852\n",
            "Epoch: 0196 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0503 acc_val: 0.9852\n",
            "Epoch: 0197 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0511 acc_val: 0.9852\n",
            "Epoch: 0198 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0520 acc_val: 0.9871\n",
            "Epoch: 0199 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0530 acc_val: 0.9889\n",
            "Epoch: 0200 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0528 acc_val: 0.9889\n",
            "Optimization Finished!\n",
            "Train cost: 26.1063s\n",
            "Loading 67th epoch\n",
            "Test set results: loss= 0.1070 accuracy= 0.9907\n",
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=1448, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=7, bias=True)\n",
            ")\n",
            "total params: 2979849\n",
            "Epoch: 0001 loss_train: 1.9684 acc_train: 0.1138 loss_val: 1.9664 acc_val: 0.1310\n",
            "Epoch: 0002 loss_train: 1.9659 acc_train: 0.1138 loss_val: 1.9561 acc_val: 0.1310\n",
            "Epoch: 0003 loss_train: 1.9579 acc_train: 0.1175 loss_val: 1.9414 acc_val: 0.1328\n",
            "Epoch: 0004 loss_train: 1.9444 acc_train: 0.1156 loss_val: 1.9233 acc_val: 0.2214\n",
            "Epoch: 0005 loss_train: 1.9278 acc_train: 0.1574 loss_val: 1.9035 acc_val: 0.3026\n",
            "Epoch: 0006 loss_train: 1.9103 acc_train: 0.2891 loss_val: 1.8836 acc_val: 0.3026\n",
            "Epoch: 0007 loss_train: 1.8926 acc_train: 0.3020 loss_val: 1.8660 acc_val: 0.3026\n",
            "Epoch: 0008 loss_train: 1.8756 acc_train: 0.3020 loss_val: 1.8524 acc_val: 0.3026\n",
            "Epoch: 0009 loss_train: 1.8614 acc_train: 0.3020 loss_val: 1.8424 acc_val: 0.3026\n",
            "Epoch: 0010 loss_train: 1.8495 acc_train: 0.3020 loss_val: 1.8359 acc_val: 0.3026\n",
            "Epoch: 0011 loss_train: 1.8412 acc_train: 0.3020 loss_val: 1.8324 acc_val: 0.3026\n",
            "Epoch: 0012 loss_train: 1.8367 acc_train: 0.3020 loss_val: 1.8310 acc_val: 0.3026\n",
            "Epoch: 0013 loss_train: 1.8350 acc_train: 0.3020 loss_val: 1.8306 acc_val: 0.3026\n",
            "Epoch: 0014 loss_train: 1.8331 acc_train: 0.3020 loss_val: 1.8297 acc_val: 0.3026\n",
            "Epoch: 0015 loss_train: 1.8318 acc_train: 0.3020 loss_val: 1.8280 acc_val: 0.3026\n",
            "Epoch: 0016 loss_train: 1.8310 acc_train: 0.3020 loss_val: 1.8252 acc_val: 0.3026\n",
            "Epoch: 0017 loss_train: 1.8285 acc_train: 0.3020 loss_val: 1.8217 acc_val: 0.3026\n",
            "Epoch: 0018 loss_train: 1.8262 acc_train: 0.3020 loss_val: 1.8179 acc_val: 0.3026\n",
            "Epoch: 0019 loss_train: 1.8242 acc_train: 0.3020 loss_val: 1.8138 acc_val: 0.3026\n",
            "Epoch: 0020 loss_train: 1.8229 acc_train: 0.3020 loss_val: 1.8089 acc_val: 0.3026\n",
            "Epoch: 0021 loss_train: 1.8214 acc_train: 0.3020 loss_val: 1.8040 acc_val: 0.3026\n",
            "Epoch: 0022 loss_train: 1.8182 acc_train: 0.3020 loss_val: 1.7982 acc_val: 0.3026\n",
            "Epoch: 0023 loss_train: 1.8151 acc_train: 0.3020 loss_val: 1.7911 acc_val: 0.3026\n",
            "Epoch: 0024 loss_train: 1.8118 acc_train: 0.3020 loss_val: 1.7823 acc_val: 0.3026\n",
            "Epoch: 0025 loss_train: 1.8057 acc_train: 0.3020 loss_val: 1.7721 acc_val: 0.3026\n",
            "Epoch: 0026 loss_train: 1.7993 acc_train: 0.3020 loss_val: 1.7609 acc_val: 0.3026\n",
            "Epoch: 0027 loss_train: 1.7920 acc_train: 0.3020 loss_val: 1.7487 acc_val: 0.3026\n",
            "Epoch: 0028 loss_train: 1.7842 acc_train: 0.3020 loss_val: 1.7356 acc_val: 0.3026\n",
            "Epoch: 0029 loss_train: 1.7772 acc_train: 0.3020 loss_val: 1.7208 acc_val: 0.3026\n",
            "Epoch: 0030 loss_train: 1.7676 acc_train: 0.3020 loss_val: 1.7028 acc_val: 0.3026\n",
            "Epoch: 0031 loss_train: 1.7557 acc_train: 0.3020 loss_val: 1.6809 acc_val: 0.3026\n",
            "Epoch: 0032 loss_train: 1.7446 acc_train: 0.3020 loss_val: 1.6553 acc_val: 0.3026\n",
            "Epoch: 0033 loss_train: 1.7305 acc_train: 0.3020 loss_val: 1.6240 acc_val: 0.3026\n",
            "Epoch: 0034 loss_train: 1.7143 acc_train: 0.3020 loss_val: 1.5861 acc_val: 0.3026\n",
            "Epoch: 0035 loss_train: 1.6939 acc_train: 0.3020 loss_val: 1.5474 acc_val: 0.3026\n",
            "Epoch: 0036 loss_train: 1.6700 acc_train: 0.3026 loss_val: 1.5036 acc_val: 0.3026\n",
            "Epoch: 0037 loss_train: 1.6416 acc_train: 0.3032 loss_val: 1.4490 acc_val: 0.3118\n",
            "Epoch: 0038 loss_train: 1.6104 acc_train: 0.3155 loss_val: 1.3820 acc_val: 0.4262\n",
            "Epoch: 0039 loss_train: 1.5754 acc_train: 0.3942 loss_val: 1.3090 acc_val: 0.6292\n",
            "Epoch: 0040 loss_train: 1.5358 acc_train: 0.4926 loss_val: 1.2379 acc_val: 0.7196\n",
            "Epoch: 0041 loss_train: 1.4946 acc_train: 0.5418 loss_val: 1.1698 acc_val: 0.7694\n",
            "Epoch: 0042 loss_train: 1.4485 acc_train: 0.5732 loss_val: 1.0987 acc_val: 0.8063\n",
            "Epoch: 0043 loss_train: 1.3995 acc_train: 0.5929 loss_val: 1.0201 acc_val: 0.8376\n",
            "Epoch: 0044 loss_train: 1.3389 acc_train: 0.6169 loss_val: 0.9420 acc_val: 0.8469\n",
            "Epoch: 0045 loss_train: 1.2745 acc_train: 0.6427 loss_val: 0.8730 acc_val: 0.8469\n",
            "Epoch: 0046 loss_train: 1.2027 acc_train: 0.6617 loss_val: 0.8026 acc_val: 0.8487\n",
            "Epoch: 0047 loss_train: 1.1319 acc_train: 0.6839 loss_val: 0.7338 acc_val: 0.8506\n",
            "Epoch: 0048 loss_train: 1.0541 acc_train: 0.7196 loss_val: 0.6621 acc_val: 0.8635\n",
            "Epoch: 0049 loss_train: 0.9788 acc_train: 0.7319 loss_val: 0.6248 acc_val: 0.8653\n",
            "Epoch: 0050 loss_train: 0.9160 acc_train: 0.7743 loss_val: 0.5359 acc_val: 0.9133\n",
            "Epoch: 0051 loss_train: 0.8644 acc_train: 0.7355 loss_val: 0.5768 acc_val: 0.8616\n",
            "Epoch: 0052 loss_train: 0.8237 acc_train: 0.7891 loss_val: 0.5048 acc_val: 0.8690\n",
            "Epoch: 0053 loss_train: 0.7446 acc_train: 0.8075 loss_val: 0.4156 acc_val: 0.9410\n",
            "Epoch: 0054 loss_train: 0.6803 acc_train: 0.8413 loss_val: 0.3713 acc_val: 0.9373\n",
            "Epoch: 0055 loss_train: 0.6067 acc_train: 0.8825 loss_val: 0.3557 acc_val: 0.9299\n",
            "Epoch: 0056 loss_train: 0.5830 acc_train: 0.8376 loss_val: 0.3375 acc_val: 0.9391\n",
            "Epoch: 0057 loss_train: 0.5352 acc_train: 0.8973 loss_val: 0.2694 acc_val: 0.9742\n",
            "Epoch: 0058 loss_train: 0.4668 acc_train: 0.9139 loss_val: 0.2276 acc_val: 0.9871\n",
            "Epoch: 0059 loss_train: 0.4069 acc_train: 0.9397 loss_val: 0.2158 acc_val: 0.9871\n",
            "Epoch: 0060 loss_train: 0.3893 acc_train: 0.9348 loss_val: 0.1816 acc_val: 0.9797\n",
            "Epoch: 0061 loss_train: 0.3367 acc_train: 0.9256 loss_val: 0.1625 acc_val: 0.9945\n",
            "Epoch: 0062 loss_train: 0.3005 acc_train: 0.9619 loss_val: 0.1316 acc_val: 0.9945\n",
            "Epoch: 0063 loss_train: 0.2526 acc_train: 0.9680 loss_val: 0.1229 acc_val: 0.9982\n",
            "Epoch: 0064 loss_train: 0.2304 acc_train: 0.9668 loss_val: 0.0976 acc_val: 0.9963\n",
            "Epoch: 0065 loss_train: 0.1862 acc_train: 0.9840 loss_val: 0.0920 acc_val: 0.9963\n",
            "Epoch: 0066 loss_train: 0.1639 acc_train: 0.9760 loss_val: 0.0742 acc_val: 1.0000\n",
            "Epoch: 0067 loss_train: 0.1350 acc_train: 0.9926 loss_val: 0.0661 acc_val: 1.0000\n",
            "Epoch: 0068 loss_train: 0.1149 acc_train: 0.9982 loss_val: 0.0593 acc_val: 0.9963\n",
            "Epoch: 0069 loss_train: 0.0955 acc_train: 0.9945 loss_val: 0.0546 acc_val: 0.9963\n",
            "Epoch: 0070 loss_train: 0.0789 acc_train: 0.9969 loss_val: 0.0436 acc_val: 0.9982\n",
            "Epoch: 0071 loss_train: 0.0615 acc_train: 1.0000 loss_val: 0.0387 acc_val: 1.0000\n",
            "Epoch: 0072 loss_train: 0.0536 acc_train: 0.9994 loss_val: 0.0354 acc_val: 0.9982\n",
            "Epoch: 0073 loss_train: 0.0433 acc_train: 0.9994 loss_val: 0.0309 acc_val: 1.0000\n",
            "Epoch: 0074 loss_train: 0.0322 acc_train: 1.0000 loss_val: 0.0328 acc_val: 0.9982\n",
            "Epoch: 0075 loss_train: 0.0313 acc_train: 1.0000 loss_val: 0.0311 acc_val: 0.9963\n",
            "Epoch: 0076 loss_train: 0.0252 acc_train: 1.0000 loss_val: 0.0276 acc_val: 0.9963\n",
            "Epoch: 0077 loss_train: 0.0202 acc_train: 1.0000 loss_val: 0.0214 acc_val: 0.9982\n",
            "Epoch: 0078 loss_train: 0.0162 acc_train: 1.0000 loss_val: 0.0184 acc_val: 1.0000\n",
            "Epoch: 0079 loss_train: 0.0144 acc_train: 1.0000 loss_val: 0.0175 acc_val: 0.9982\n",
            "Epoch: 0080 loss_train: 0.0126 acc_train: 1.0000 loss_val: 0.0204 acc_val: 0.9963\n",
            "Epoch: 0081 loss_train: 0.0112 acc_train: 1.0000 loss_val: 0.0191 acc_val: 0.9963\n",
            "Epoch: 0082 loss_train: 0.0095 acc_train: 1.0000 loss_val: 0.0144 acc_val: 1.0000\n",
            "Epoch: 0083 loss_train: 0.0073 acc_train: 1.0000 loss_val: 0.0129 acc_val: 1.0000\n",
            "Epoch: 0084 loss_train: 0.0068 acc_train: 1.0000 loss_val: 0.0126 acc_val: 1.0000\n",
            "Epoch: 0085 loss_train: 0.0062 acc_train: 1.0000 loss_val: 0.0138 acc_val: 0.9982\n",
            "Epoch: 0086 loss_train: 0.0054 acc_train: 1.0000 loss_val: 0.0164 acc_val: 0.9963\n",
            "Epoch: 0087 loss_train: 0.0050 acc_train: 1.0000 loss_val: 0.0163 acc_val: 0.9963\n",
            "Epoch: 0088 loss_train: 0.0044 acc_train: 1.0000 loss_val: 0.0131 acc_val: 0.9982\n",
            "Epoch: 0089 loss_train: 0.0038 acc_train: 1.0000 loss_val: 0.0097 acc_val: 0.9982\n",
            "Epoch: 0090 loss_train: 0.0035 acc_train: 1.0000 loss_val: 0.0074 acc_val: 1.0000\n",
            "Epoch: 0091 loss_train: 0.0031 acc_train: 1.0000 loss_val: 0.0065 acc_val: 1.0000\n",
            "Epoch: 0092 loss_train: 0.0028 acc_train: 1.0000 loss_val: 0.0071 acc_val: 1.0000\n",
            "Epoch: 0093 loss_train: 0.0025 acc_train: 1.0000 loss_val: 0.0085 acc_val: 0.9982\n",
            "Epoch: 0094 loss_train: 0.0024 acc_train: 1.0000 loss_val: 0.0086 acc_val: 0.9963\n",
            "Epoch: 0095 loss_train: 0.0023 acc_train: 1.0000 loss_val: 0.0072 acc_val: 0.9982\n",
            "Epoch: 0096 loss_train: 0.0021 acc_train: 1.0000 loss_val: 0.0059 acc_val: 1.0000\n",
            "Epoch: 0097 loss_train: 0.0019 acc_train: 1.0000 loss_val: 0.0051 acc_val: 1.0000\n",
            "Epoch: 0098 loss_train: 0.0018 acc_train: 1.0000 loss_val: 0.0045 acc_val: 1.0000\n",
            "Epoch: 0099 loss_train: 0.0017 acc_train: 1.0000 loss_val: 0.0042 acc_val: 1.0000\n",
            "Epoch: 0100 loss_train: 0.0015 acc_train: 1.0000 loss_val: 0.0044 acc_val: 1.0000\n",
            "Epoch: 0101 loss_train: 0.0014 acc_train: 1.0000 loss_val: 0.0049 acc_val: 1.0000\n",
            "Epoch: 0102 loss_train: 0.0014 acc_train: 1.0000 loss_val: 0.0051 acc_val: 1.0000\n",
            "Epoch: 0103 loss_train: 0.0014 acc_train: 1.0000 loss_val: 0.0047 acc_val: 1.0000\n",
            "Epoch: 0104 loss_train: 0.0012 acc_train: 1.0000 loss_val: 0.0042 acc_val: 1.0000\n",
            "Epoch: 0105 loss_train: 0.0012 acc_train: 1.0000 loss_val: 0.0040 acc_val: 1.0000\n",
            "Epoch: 0106 loss_train: 0.0011 acc_train: 1.0000 loss_val: 0.0040 acc_val: 1.0000\n",
            "Epoch: 0107 loss_train: 0.0011 acc_train: 1.0000 loss_val: 0.0042 acc_val: 1.0000\n",
            "Epoch: 0108 loss_train: 0.0010 acc_train: 1.0000 loss_val: 0.0044 acc_val: 1.0000\n",
            "Epoch: 0109 loss_train: 0.0010 acc_train: 1.0000 loss_val: 0.0045 acc_val: 1.0000\n",
            "Epoch: 0110 loss_train: 0.0010 acc_train: 1.0000 loss_val: 0.0045 acc_val: 1.0000\n",
            "Epoch: 0111 loss_train: 0.0009 acc_train: 1.0000 loss_val: 0.0043 acc_val: 1.0000\n",
            "Epoch: 0112 loss_train: 0.0009 acc_train: 1.0000 loss_val: 0.0040 acc_val: 1.0000\n",
            "Epoch: 0113 loss_train: 0.0008 acc_train: 1.0000 loss_val: 0.0036 acc_val: 1.0000\n",
            "Epoch: 0114 loss_train: 0.0008 acc_train: 1.0000 loss_val: 0.0032 acc_val: 1.0000\n",
            "Epoch: 0115 loss_train: 0.0008 acc_train: 1.0000 loss_val: 0.0030 acc_val: 1.0000\n",
            "Epoch: 0116 loss_train: 0.0008 acc_train: 1.0000 loss_val: 0.0029 acc_val: 1.0000\n",
            "Epoch: 0117 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0029 acc_val: 1.0000\n",
            "Epoch: 0118 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0030 acc_val: 1.0000\n",
            "Epoch: 0119 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0031 acc_val: 1.0000\n",
            "Epoch: 0120 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0031 acc_val: 1.0000\n",
            "Epoch: 0121 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0032 acc_val: 1.0000\n",
            "Epoch: 0122 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0030 acc_val: 1.0000\n",
            "Epoch: 0123 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0028 acc_val: 1.0000\n",
            "Epoch: 0124 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0027 acc_val: 1.0000\n",
            "Epoch: 0125 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0026 acc_val: 1.0000\n",
            "Epoch: 0126 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0025 acc_val: 1.0000\n",
            "Epoch: 0127 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0025 acc_val: 1.0000\n",
            "Epoch: 0128 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0026 acc_val: 1.0000\n",
            "Epoch: 0129 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0028 acc_val: 1.0000\n",
            "Epoch: 0130 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0030 acc_val: 1.0000\n",
            "Epoch: 0131 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0030 acc_val: 1.0000\n",
            "Epoch: 0132 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0029 acc_val: 1.0000\n",
            "Epoch: 0133 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0027 acc_val: 1.0000\n",
            "Epoch: 0134 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0024 acc_val: 1.0000\n",
            "Epoch: 0135 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0022 acc_val: 1.0000\n",
            "Epoch: 0136 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0020 acc_val: 1.0000\n",
            "Epoch: 0137 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0019 acc_val: 1.0000\n",
            "Epoch: 0138 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0019 acc_val: 1.0000\n",
            "Epoch: 0139 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0020 acc_val: 1.0000\n",
            "Epoch: 0140 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0021 acc_val: 1.0000\n",
            "Epoch: 0141 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0022 acc_val: 1.0000\n",
            "Epoch: 0142 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0022 acc_val: 1.0000\n",
            "Epoch: 0143 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0022 acc_val: 1.0000\n",
            "Epoch: 0144 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0020 acc_val: 1.0000\n",
            "Epoch: 0145 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0019 acc_val: 1.0000\n",
            "Epoch: 0146 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0018 acc_val: 1.0000\n",
            "Epoch: 0147 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0017 acc_val: 1.0000\n",
            "Epoch: 0148 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0017 acc_val: 1.0000\n",
            "Epoch: 0149 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0016 acc_val: 1.0000\n",
            "Epoch: 0150 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0016 acc_val: 1.0000\n",
            "Epoch: 0151 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0016 acc_val: 1.0000\n",
            "Epoch: 0152 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0017 acc_val: 1.0000\n",
            "Epoch: 0153 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0017 acc_val: 1.0000\n",
            "Epoch: 0154 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0017 acc_val: 1.0000\n",
            "Epoch: 0155 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0016 acc_val: 1.0000\n",
            "Epoch: 0156 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0017 acc_val: 1.0000\n",
            "Epoch: 0157 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0017 acc_val: 1.0000\n",
            "Epoch: 0158 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0017 acc_val: 1.0000\n",
            "Epoch: 0159 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0016 acc_val: 1.0000\n",
            "Epoch: 0160 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0016 acc_val: 1.0000\n",
            "Epoch: 0161 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0015 acc_val: 1.0000\n",
            "Epoch: 0162 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0014 acc_val: 1.0000\n",
            "Epoch: 0163 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0013 acc_val: 1.0000\n",
            "Epoch: 0164 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0013 acc_val: 1.0000\n",
            "Epoch: 0165 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0013 acc_val: 1.0000\n",
            "Epoch: 0166 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0013 acc_val: 1.0000\n",
            "Epoch: 0167 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0013 acc_val: 1.0000\n",
            "Epoch: 0168 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0013 acc_val: 1.0000\n",
            "Epoch: 0169 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0013 acc_val: 1.0000\n",
            "Epoch: 0170 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0013 acc_val: 1.0000\n",
            "Epoch: 0171 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0013 acc_val: 1.0000\n",
            "Epoch: 0172 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0013 acc_val: 1.0000\n",
            "Epoch: 0173 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0013 acc_val: 1.0000\n",
            "Epoch: 0174 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0013 acc_val: 1.0000\n",
            "Epoch: 0175 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0012 acc_val: 1.0000\n",
            "Epoch: 0176 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0011 acc_val: 1.0000\n",
            "Epoch: 0177 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0010 acc_val: 1.0000\n",
            "Epoch: 0178 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0010 acc_val: 1.0000\n",
            "Epoch: 0179 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0010 acc_val: 1.0000\n",
            "Epoch: 0180 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0011 acc_val: 1.0000\n",
            "Epoch: 0181 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0012 acc_val: 1.0000\n",
            "Epoch: 0182 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0012 acc_val: 1.0000\n",
            "Epoch: 0183 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0012 acc_val: 1.0000\n",
            "Epoch: 0184 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0011 acc_val: 1.0000\n",
            "Epoch: 0185 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0010 acc_val: 1.0000\n",
            "Epoch: 0186 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0009 acc_val: 1.0000\n",
            "Epoch: 0187 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0008 acc_val: 1.0000\n",
            "Epoch: 0188 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0008 acc_val: 1.0000\n",
            "Epoch: 0189 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0009 acc_val: 1.0000\n",
            "Epoch: 0190 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0011 acc_val: 1.0000\n",
            "Epoch: 0191 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0012 acc_val: 1.0000\n",
            "Epoch: 0192 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0013 acc_val: 1.0000\n",
            "Epoch: 0193 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0011 acc_val: 1.0000\n",
            "Epoch: 0194 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0010 acc_val: 1.0000\n",
            "Epoch: 0195 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0009 acc_val: 1.0000\n",
            "Epoch: 0196 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0008 acc_val: 1.0000\n",
            "Epoch: 0197 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0008 acc_val: 1.0000\n",
            "Epoch: 0198 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0007 acc_val: 1.0000\n",
            "Epoch: 0199 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0007 acc_val: 1.0000\n",
            "Epoch: 0200 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0007 acc_val: 1.0000\n",
            "Optimization Finished!\n",
            "Train cost: 26.8047s\n",
            "Loading 199th epoch\n",
            "Test set results: loss= 0.0006 accuracy= 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --dataset cora --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 5  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack none --attperc 20 --TrainTest train --weight_decay=1e-05\n",
        "\n",
        "!python train.py --dataset cora --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 5  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack l2_weak --attperc 10 --TrainTest train --weight_decay=1e-05\n",
        "\n",
        "!python train.py --dataset cora --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 5  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack l2_weak --attperc 40 --TrainTest train --weight_decay=1e-05\n",
        "\n",
        "!python train.py --dataset cora --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 5  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack l2_weak --attperc 60 --TrainTest train --weight_decay=1e-05\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jo-PbtmjaA9r",
        "outputId": "c2269a3b-c587-44e8-fcc0-7519873a0314"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=1448, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=7, bias=True)\n",
            ")\n",
            "total params: 2979849\n",
            "Epoch: 0001 loss_train: 1.9727 acc_train: 0.1156 loss_val: 1.9686 acc_val: 0.1310\n",
            "Epoch: 0002 loss_train: 1.9669 acc_train: 0.1279 loss_val: 1.9561 acc_val: 0.1384\n",
            "Epoch: 0003 loss_train: 1.9547 acc_train: 0.1396 loss_val: 1.9375 acc_val: 0.1587\n",
            "Epoch: 0004 loss_train: 1.9373 acc_train: 0.1525 loss_val: 1.9130 acc_val: 0.2122\n",
            "Epoch: 0005 loss_train: 1.9160 acc_train: 0.1925 loss_val: 1.8830 acc_val: 0.3118\n",
            "Epoch: 0006 loss_train: 1.8886 acc_train: 0.2768 loss_val: 1.8478 acc_val: 0.4428\n",
            "Epoch: 0007 loss_train: 1.8561 acc_train: 0.3930 loss_val: 1.8078 acc_val: 0.4815\n",
            "Epoch: 0008 loss_train: 1.8199 acc_train: 0.4736 loss_val: 1.7640 acc_val: 0.5055\n",
            "Epoch: 0009 loss_train: 1.7805 acc_train: 0.4889 loss_val: 1.7170 acc_val: 0.5055\n",
            "Epoch: 0010 loss_train: 1.7355 acc_train: 0.5092 loss_val: 1.6678 acc_val: 0.5166\n",
            "Epoch: 0011 loss_train: 1.6880 acc_train: 0.5203 loss_val: 1.6169 acc_val: 0.5277\n",
            "Epoch: 0012 loss_train: 1.6405 acc_train: 0.5295 loss_val: 1.5647 acc_val: 0.5461\n",
            "Epoch: 0013 loss_train: 1.5898 acc_train: 0.5387 loss_val: 1.5112 acc_val: 0.5517\n",
            "Epoch: 0014 loss_train: 1.5383 acc_train: 0.5541 loss_val: 1.4567 acc_val: 0.5627\n",
            "Epoch: 0015 loss_train: 1.4837 acc_train: 0.5658 loss_val: 1.4014 acc_val: 0.5701\n",
            "Epoch: 0016 loss_train: 1.4274 acc_train: 0.5824 loss_val: 1.3452 acc_val: 0.5849\n",
            "Epoch: 0017 loss_train: 1.3709 acc_train: 0.5996 loss_val: 1.2880 acc_val: 0.6070\n",
            "Epoch: 0018 loss_train: 1.3128 acc_train: 0.6193 loss_val: 1.2305 acc_val: 0.6494\n",
            "Epoch: 0019 loss_train: 1.2546 acc_train: 0.6513 loss_val: 1.1740 acc_val: 0.6734\n",
            "Epoch: 0020 loss_train: 1.1946 acc_train: 0.6845 loss_val: 1.1196 acc_val: 0.6882\n",
            "Epoch: 0021 loss_train: 1.1373 acc_train: 0.7103 loss_val: 1.0686 acc_val: 0.6993\n",
            "Epoch: 0022 loss_train: 1.0810 acc_train: 0.7269 loss_val: 1.0214 acc_val: 0.6974\n",
            "Epoch: 0023 loss_train: 1.0270 acc_train: 0.7386 loss_val: 0.9779 acc_val: 0.7103\n",
            "Epoch: 0024 loss_train: 0.9762 acc_train: 0.7491 loss_val: 0.9369 acc_val: 0.7214\n",
            "Epoch: 0025 loss_train: 0.9308 acc_train: 0.7558 loss_val: 0.8978 acc_val: 0.7288\n",
            "Epoch: 0026 loss_train: 0.8845 acc_train: 0.7706 loss_val: 0.8600 acc_val: 0.7306\n",
            "Epoch: 0027 loss_train: 0.8421 acc_train: 0.7811 loss_val: 0.8239 acc_val: 0.7472\n",
            "Epoch: 0028 loss_train: 0.8022 acc_train: 0.8007 loss_val: 0.7899 acc_val: 0.7509\n",
            "Epoch: 0029 loss_train: 0.7630 acc_train: 0.8093 loss_val: 0.7580 acc_val: 0.7638\n",
            "Epoch: 0030 loss_train: 0.7271 acc_train: 0.8167 loss_val: 0.7286 acc_val: 0.7768\n",
            "Epoch: 0031 loss_train: 0.6939 acc_train: 0.8266 loss_val: 0.7010 acc_val: 0.7804\n",
            "Epoch: 0032 loss_train: 0.6632 acc_train: 0.8284 loss_val: 0.6747 acc_val: 0.7823\n",
            "Epoch: 0033 loss_train: 0.6327 acc_train: 0.8346 loss_val: 0.6496 acc_val: 0.7823\n",
            "Epoch: 0034 loss_train: 0.6044 acc_train: 0.8432 loss_val: 0.6263 acc_val: 0.7878\n",
            "Epoch: 0035 loss_train: 0.5759 acc_train: 0.8481 loss_val: 0.6049 acc_val: 0.8044\n",
            "Epoch: 0036 loss_train: 0.5496 acc_train: 0.8524 loss_val: 0.5844 acc_val: 0.8081\n",
            "Epoch: 0037 loss_train: 0.5268 acc_train: 0.8598 loss_val: 0.5635 acc_val: 0.8118\n",
            "Epoch: 0038 loss_train: 0.5036 acc_train: 0.8598 loss_val: 0.5414 acc_val: 0.8192\n",
            "Epoch: 0039 loss_train: 0.4787 acc_train: 0.8647 loss_val: 0.5187 acc_val: 0.8266\n",
            "Epoch: 0040 loss_train: 0.4569 acc_train: 0.8684 loss_val: 0.4966 acc_val: 0.8266\n",
            "Epoch: 0041 loss_train: 0.4365 acc_train: 0.8739 loss_val: 0.4760 acc_val: 0.8303\n",
            "Epoch: 0042 loss_train: 0.4171 acc_train: 0.8733 loss_val: 0.4578 acc_val: 0.8413\n",
            "Epoch: 0043 loss_train: 0.3960 acc_train: 0.8819 loss_val: 0.4426 acc_val: 0.8561\n",
            "Epoch: 0044 loss_train: 0.3774 acc_train: 0.8887 loss_val: 0.4296 acc_val: 0.8635\n",
            "Epoch: 0045 loss_train: 0.3595 acc_train: 0.8918 loss_val: 0.4170 acc_val: 0.8690\n",
            "Epoch: 0046 loss_train: 0.3405 acc_train: 0.8998 loss_val: 0.4041 acc_val: 0.8727\n",
            "Epoch: 0047 loss_train: 0.3229 acc_train: 0.9090 loss_val: 0.3923 acc_val: 0.8745\n",
            "Epoch: 0048 loss_train: 0.3065 acc_train: 0.9157 loss_val: 0.3836 acc_val: 0.8838\n",
            "Epoch: 0049 loss_train: 0.2879 acc_train: 0.9194 loss_val: 0.3782 acc_val: 0.8875\n",
            "Epoch: 0050 loss_train: 0.2725 acc_train: 0.9274 loss_val: 0.3758 acc_val: 0.8893\n",
            "Epoch: 0051 loss_train: 0.2565 acc_train: 0.9293 loss_val: 0.3755 acc_val: 0.8838\n",
            "Epoch: 0052 loss_train: 0.2410 acc_train: 0.9342 loss_val: 0.3742 acc_val: 0.8819\n",
            "Epoch: 0053 loss_train: 0.2288 acc_train: 0.9403 loss_val: 0.3732 acc_val: 0.8838\n",
            "Epoch: 0054 loss_train: 0.2129 acc_train: 0.9434 loss_val: 0.3749 acc_val: 0.8819\n",
            "Epoch: 0055 loss_train: 0.1947 acc_train: 0.9508 loss_val: 0.3805 acc_val: 0.8782\n",
            "Epoch: 0056 loss_train: 0.1813 acc_train: 0.9533 loss_val: 0.3863 acc_val: 0.8801\n",
            "Epoch: 0057 loss_train: 0.1677 acc_train: 0.9576 loss_val: 0.3874 acc_val: 0.8782\n",
            "Epoch: 0058 loss_train: 0.1540 acc_train: 0.9619 loss_val: 0.3872 acc_val: 0.8782\n",
            "Epoch: 0059 loss_train: 0.1432 acc_train: 0.9656 loss_val: 0.3902 acc_val: 0.8782\n",
            "Epoch: 0060 loss_train: 0.1315 acc_train: 0.9686 loss_val: 0.3960 acc_val: 0.8801\n",
            "Epoch: 0061 loss_train: 0.1190 acc_train: 0.9729 loss_val: 0.4023 acc_val: 0.8782\n",
            "Epoch: 0062 loss_train: 0.1102 acc_train: 0.9760 loss_val: 0.4080 acc_val: 0.8764\n",
            "Epoch: 0063 loss_train: 0.1000 acc_train: 0.9822 loss_val: 0.4138 acc_val: 0.8764\n",
            "Epoch: 0064 loss_train: 0.0907 acc_train: 0.9859 loss_val: 0.4196 acc_val: 0.8764\n",
            "Epoch: 0065 loss_train: 0.0822 acc_train: 0.9865 loss_val: 0.4272 acc_val: 0.8801\n",
            "Epoch: 0066 loss_train: 0.0764 acc_train: 0.9852 loss_val: 0.4338 acc_val: 0.8745\n",
            "Epoch: 0067 loss_train: 0.0687 acc_train: 0.9871 loss_val: 0.4419 acc_val: 0.8727\n",
            "Epoch: 0068 loss_train: 0.0618 acc_train: 0.9895 loss_val: 0.4517 acc_val: 0.8708\n",
            "Epoch: 0069 loss_train: 0.0564 acc_train: 0.9914 loss_val: 0.4612 acc_val: 0.8727\n",
            "Epoch: 0070 loss_train: 0.0516 acc_train: 0.9908 loss_val: 0.4702 acc_val: 0.8708\n",
            "Epoch: 0071 loss_train: 0.0470 acc_train: 0.9920 loss_val: 0.4785 acc_val: 0.8690\n",
            "Epoch: 0072 loss_train: 0.0425 acc_train: 0.9926 loss_val: 0.4848 acc_val: 0.8690\n",
            "Epoch: 0073 loss_train: 0.0387 acc_train: 0.9945 loss_val: 0.4902 acc_val: 0.8690\n",
            "Epoch: 0074 loss_train: 0.0355 acc_train: 0.9938 loss_val: 0.4948 acc_val: 0.8672\n",
            "Epoch: 0075 loss_train: 0.0312 acc_train: 0.9945 loss_val: 0.4993 acc_val: 0.8672\n",
            "Epoch: 0076 loss_train: 0.0288 acc_train: 0.9963 loss_val: 0.5046 acc_val: 0.8690\n",
            "Epoch: 0077 loss_train: 0.0262 acc_train: 0.9951 loss_val: 0.5107 acc_val: 0.8690\n",
            "Epoch: 0078 loss_train: 0.0229 acc_train: 0.9988 loss_val: 0.5159 acc_val: 0.8708\n",
            "Epoch: 0079 loss_train: 0.0213 acc_train: 0.9982 loss_val: 0.5216 acc_val: 0.8708\n",
            "Epoch: 0080 loss_train: 0.0198 acc_train: 0.9975 loss_val: 0.5286 acc_val: 0.8708\n",
            "Epoch: 0081 loss_train: 0.0173 acc_train: 0.9982 loss_val: 0.5363 acc_val: 0.8690\n",
            "Epoch: 0082 loss_train: 0.0152 acc_train: 0.9994 loss_val: 0.5455 acc_val: 0.8708\n",
            "Epoch: 0083 loss_train: 0.0139 acc_train: 0.9988 loss_val: 0.5557 acc_val: 0.8727\n",
            "Epoch: 0084 loss_train: 0.0124 acc_train: 0.9994 loss_val: 0.5655 acc_val: 0.8708\n",
            "Epoch: 0085 loss_train: 0.0114 acc_train: 0.9994 loss_val: 0.5740 acc_val: 0.8690\n",
            "Epoch: 0086 loss_train: 0.0101 acc_train: 1.0000 loss_val: 0.5804 acc_val: 0.8690\n",
            "Epoch: 0087 loss_train: 0.0090 acc_train: 1.0000 loss_val: 0.5846 acc_val: 0.8708\n",
            "Epoch: 0088 loss_train: 0.0084 acc_train: 1.0000 loss_val: 0.5882 acc_val: 0.8690\n",
            "Epoch: 0089 loss_train: 0.0082 acc_train: 0.9994 loss_val: 0.5925 acc_val: 0.8708\n",
            "Epoch: 0090 loss_train: 0.0073 acc_train: 1.0000 loss_val: 0.5984 acc_val: 0.8708\n",
            "Epoch: 0091 loss_train: 0.0067 acc_train: 1.0000 loss_val: 0.6056 acc_val: 0.8690\n",
            "Epoch: 0092 loss_train: 0.0060 acc_train: 1.0000 loss_val: 0.6134 acc_val: 0.8690\n",
            "Epoch: 0093 loss_train: 0.0053 acc_train: 1.0000 loss_val: 0.6213 acc_val: 0.8708\n",
            "Epoch: 0094 loss_train: 0.0052 acc_train: 1.0000 loss_val: 0.6275 acc_val: 0.8690\n",
            "Epoch: 0095 loss_train: 0.0047 acc_train: 1.0000 loss_val: 0.6331 acc_val: 0.8672\n",
            "Epoch: 0096 loss_train: 0.0043 acc_train: 1.0000 loss_val: 0.6373 acc_val: 0.8672\n",
            "Epoch: 0097 loss_train: 0.0041 acc_train: 1.0000 loss_val: 0.6404 acc_val: 0.8690\n",
            "Epoch: 0098 loss_train: 0.0040 acc_train: 1.0000 loss_val: 0.6428 acc_val: 0.8672\n",
            "Epoch: 0099 loss_train: 0.0035 acc_train: 1.0000 loss_val: 0.6451 acc_val: 0.8672\n",
            "Epoch: 0100 loss_train: 0.0035 acc_train: 1.0000 loss_val: 0.6471 acc_val: 0.8635\n",
            "Epoch: 0101 loss_train: 0.0032 acc_train: 1.0000 loss_val: 0.6496 acc_val: 0.8635\n",
            "Epoch: 0102 loss_train: 0.0031 acc_train: 1.0000 loss_val: 0.6526 acc_val: 0.8635\n",
            "Epoch: 0103 loss_train: 0.0029 acc_train: 1.0000 loss_val: 0.6555 acc_val: 0.8635\n",
            "Optimization Finished!\n",
            "Train cost: 13.9824s\n",
            "Loading 50th epoch\n",
            "Test set results: loss= 0.3476 accuracy= 0.9019\n",
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:354: DGLWarning: DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\n",
            "  dgl_warning(\"DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\")\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=1448, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=7, bias=True)\n",
            ")\n",
            "total params: 2979849\n",
            "Epoch: 0001 loss_train: 1.9680 acc_train: 0.1199 loss_val: 1.9617 acc_val: 0.1347\n",
            "Epoch: 0002 loss_train: 1.9616 acc_train: 0.1292 loss_val: 1.9496 acc_val: 0.1458\n",
            "Epoch: 0003 loss_train: 1.9493 acc_train: 0.1433 loss_val: 1.9316 acc_val: 0.1697\n",
            "Epoch: 0004 loss_train: 1.9332 acc_train: 0.1697 loss_val: 1.9080 acc_val: 0.2269\n",
            "Epoch: 0005 loss_train: 1.9117 acc_train: 0.2060 loss_val: 1.8788 acc_val: 0.2841\n",
            "Epoch: 0006 loss_train: 1.8865 acc_train: 0.2854 loss_val: 1.8444 acc_val: 0.3893\n",
            "Epoch: 0007 loss_train: 1.8539 acc_train: 0.3696 loss_val: 1.8052 acc_val: 0.4686\n",
            "Epoch: 0008 loss_train: 1.8189 acc_train: 0.4440 loss_val: 1.7618 acc_val: 0.5092\n",
            "Epoch: 0009 loss_train: 1.7789 acc_train: 0.4963 loss_val: 1.7148 acc_val: 0.5203\n",
            "Epoch: 0010 loss_train: 1.7348 acc_train: 0.5068 loss_val: 1.6652 acc_val: 0.5277\n",
            "Epoch: 0011 loss_train: 1.6868 acc_train: 0.5246 loss_val: 1.6132 acc_val: 0.5314\n",
            "Epoch: 0012 loss_train: 1.6376 acc_train: 0.5301 loss_val: 1.5596 acc_val: 0.5443\n",
            "Epoch: 0013 loss_train: 1.5869 acc_train: 0.5412 loss_val: 1.5047 acc_val: 0.5646\n",
            "Epoch: 0014 loss_train: 1.5343 acc_train: 0.5572 loss_val: 1.4487 acc_val: 0.5646\n",
            "Epoch: 0015 loss_train: 1.4780 acc_train: 0.5640 loss_val: 1.3915 acc_val: 0.5904\n",
            "Epoch: 0016 loss_train: 1.4233 acc_train: 0.5855 loss_val: 1.3332 acc_val: 0.6199\n",
            "Epoch: 0017 loss_train: 1.3650 acc_train: 0.6162 loss_val: 1.2746 acc_val: 0.6476\n",
            "Epoch: 0018 loss_train: 1.3069 acc_train: 0.6470 loss_val: 1.2167 acc_val: 0.6827\n",
            "Epoch: 0019 loss_train: 1.2481 acc_train: 0.6704 loss_val: 1.1606 acc_val: 0.6863\n",
            "Epoch: 0020 loss_train: 1.1897 acc_train: 0.6974 loss_val: 1.1077 acc_val: 0.6845\n",
            "Epoch: 0021 loss_train: 1.1361 acc_train: 0.7054 loss_val: 1.0583 acc_val: 0.6919\n",
            "Epoch: 0022 loss_train: 1.0813 acc_train: 0.7196 loss_val: 1.0122 acc_val: 0.6974\n",
            "Epoch: 0023 loss_train: 1.0293 acc_train: 0.7300 loss_val: 0.9689 acc_val: 0.7048\n",
            "Epoch: 0024 loss_train: 0.9810 acc_train: 0.7380 loss_val: 0.9279 acc_val: 0.7214\n",
            "Epoch: 0025 loss_train: 0.9358 acc_train: 0.7546 loss_val: 0.8893 acc_val: 0.7325\n",
            "Epoch: 0026 loss_train: 0.8921 acc_train: 0.7626 loss_val: 0.8532 acc_val: 0.7454\n",
            "Epoch: 0027 loss_train: 0.8520 acc_train: 0.7718 loss_val: 0.8198 acc_val: 0.7491\n",
            "Epoch: 0028 loss_train: 0.8141 acc_train: 0.7811 loss_val: 0.7893 acc_val: 0.7601\n",
            "Epoch: 0029 loss_train: 0.7789 acc_train: 0.7860 loss_val: 0.7609 acc_val: 0.7675\n",
            "Epoch: 0030 loss_train: 0.7475 acc_train: 0.7891 loss_val: 0.7339 acc_val: 0.7694\n",
            "Epoch: 0031 loss_train: 0.7160 acc_train: 0.8014 loss_val: 0.7081 acc_val: 0.7731\n",
            "Epoch: 0032 loss_train: 0.6877 acc_train: 0.8106 loss_val: 0.6832 acc_val: 0.7749\n",
            "Epoch: 0033 loss_train: 0.6595 acc_train: 0.8143 loss_val: 0.6594 acc_val: 0.7804\n",
            "Epoch: 0034 loss_train: 0.6319 acc_train: 0.8198 loss_val: 0.6361 acc_val: 0.7952\n",
            "Epoch: 0035 loss_train: 0.6062 acc_train: 0.8272 loss_val: 0.6129 acc_val: 0.8081\n",
            "Epoch: 0036 loss_train: 0.5789 acc_train: 0.8303 loss_val: 0.5891 acc_val: 0.8118\n",
            "Epoch: 0037 loss_train: 0.5552 acc_train: 0.8432 loss_val: 0.5651 acc_val: 0.8155\n",
            "Epoch: 0038 loss_train: 0.5307 acc_train: 0.8401 loss_val: 0.5414 acc_val: 0.8210\n",
            "Epoch: 0039 loss_train: 0.5057 acc_train: 0.8506 loss_val: 0.5186 acc_val: 0.8210\n",
            "Epoch: 0040 loss_train: 0.4834 acc_train: 0.8506 loss_val: 0.4972 acc_val: 0.8247\n",
            "Epoch: 0041 loss_train: 0.4620 acc_train: 0.8585 loss_val: 0.4777 acc_val: 0.8413\n",
            "Epoch: 0042 loss_train: 0.4395 acc_train: 0.8641 loss_val: 0.4604 acc_val: 0.8487\n",
            "Epoch: 0043 loss_train: 0.4170 acc_train: 0.8727 loss_val: 0.4449 acc_val: 0.8524\n",
            "Epoch: 0044 loss_train: 0.3968 acc_train: 0.8764 loss_val: 0.4292 acc_val: 0.8598\n",
            "Epoch: 0045 loss_train: 0.3759 acc_train: 0.8905 loss_val: 0.4139 acc_val: 0.8782\n",
            "Epoch: 0046 loss_train: 0.3544 acc_train: 0.8961 loss_val: 0.4005 acc_val: 0.8819\n",
            "Epoch: 0047 loss_train: 0.3367 acc_train: 0.9034 loss_val: 0.3897 acc_val: 0.8838\n",
            "Epoch: 0048 loss_train: 0.3157 acc_train: 0.9102 loss_val: 0.3818 acc_val: 0.8875\n",
            "Epoch: 0049 loss_train: 0.2973 acc_train: 0.9133 loss_val: 0.3765 acc_val: 0.8838\n",
            "Epoch: 0050 loss_train: 0.2781 acc_train: 0.9200 loss_val: 0.3724 acc_val: 0.8801\n",
            "Epoch: 0051 loss_train: 0.2606 acc_train: 0.9287 loss_val: 0.3700 acc_val: 0.8819\n",
            "Epoch: 0052 loss_train: 0.2442 acc_train: 0.9354 loss_val: 0.3695 acc_val: 0.8838\n",
            "Epoch: 0053 loss_train: 0.2295 acc_train: 0.9410 loss_val: 0.3718 acc_val: 0.8801\n",
            "Epoch: 0054 loss_train: 0.2126 acc_train: 0.9490 loss_val: 0.3761 acc_val: 0.8801\n",
            "Epoch: 0055 loss_train: 0.1939 acc_train: 0.9533 loss_val: 0.3789 acc_val: 0.8838\n",
            "Epoch: 0056 loss_train: 0.1791 acc_train: 0.9539 loss_val: 0.3789 acc_val: 0.8838\n",
            "Epoch: 0057 loss_train: 0.1636 acc_train: 0.9606 loss_val: 0.3791 acc_val: 0.8911\n",
            "Epoch: 0058 loss_train: 0.1526 acc_train: 0.9613 loss_val: 0.3830 acc_val: 0.8911\n",
            "Epoch: 0059 loss_train: 0.1405 acc_train: 0.9662 loss_val: 0.3920 acc_val: 0.8875\n",
            "Epoch: 0060 loss_train: 0.1271 acc_train: 0.9699 loss_val: 0.4003 acc_val: 0.8801\n",
            "Epoch: 0061 loss_train: 0.1149 acc_train: 0.9760 loss_val: 0.4037 acc_val: 0.8801\n",
            "Epoch: 0062 loss_train: 0.1052 acc_train: 0.9797 loss_val: 0.4056 acc_val: 0.8764\n",
            "Epoch: 0063 loss_train: 0.0953 acc_train: 0.9834 loss_val: 0.4115 acc_val: 0.8745\n",
            "Epoch: 0064 loss_train: 0.0854 acc_train: 0.9846 loss_val: 0.4224 acc_val: 0.8782\n",
            "Epoch: 0065 loss_train: 0.0769 acc_train: 0.9840 loss_val: 0.4361 acc_val: 0.8764\n",
            "Epoch: 0066 loss_train: 0.0714 acc_train: 0.9859 loss_val: 0.4448 acc_val: 0.8764\n",
            "Epoch: 0067 loss_train: 0.0639 acc_train: 0.9889 loss_val: 0.4496 acc_val: 0.8764\n",
            "Epoch: 0068 loss_train: 0.0566 acc_train: 0.9889 loss_val: 0.4558 acc_val: 0.8727\n",
            "Epoch: 0069 loss_train: 0.0525 acc_train: 0.9914 loss_val: 0.4649 acc_val: 0.8690\n",
            "Epoch: 0070 loss_train: 0.0472 acc_train: 0.9920 loss_val: 0.4768 acc_val: 0.8708\n",
            "Epoch: 0071 loss_train: 0.0428 acc_train: 0.9926 loss_val: 0.4881 acc_val: 0.8708\n",
            "Epoch: 0072 loss_train: 0.0384 acc_train: 0.9945 loss_val: 0.4938 acc_val: 0.8727\n",
            "Epoch: 0073 loss_train: 0.0351 acc_train: 0.9951 loss_val: 0.4949 acc_val: 0.8708\n",
            "Epoch: 0074 loss_train: 0.0323 acc_train: 0.9938 loss_val: 0.4962 acc_val: 0.8708\n",
            "Epoch: 0075 loss_train: 0.0291 acc_train: 0.9957 loss_val: 0.5000 acc_val: 0.8690\n",
            "Epoch: 0076 loss_train: 0.0268 acc_train: 0.9951 loss_val: 0.5088 acc_val: 0.8690\n",
            "Epoch: 0077 loss_train: 0.0242 acc_train: 0.9963 loss_val: 0.5207 acc_val: 0.8690\n",
            "Epoch: 0078 loss_train: 0.0208 acc_train: 0.9982 loss_val: 0.5318 acc_val: 0.8708\n",
            "Epoch: 0079 loss_train: 0.0194 acc_train: 0.9982 loss_val: 0.5404 acc_val: 0.8708\n",
            "Epoch: 0080 loss_train: 0.0183 acc_train: 0.9988 loss_val: 0.5461 acc_val: 0.8708\n",
            "Epoch: 0081 loss_train: 0.0169 acc_train: 0.9988 loss_val: 0.5505 acc_val: 0.8745\n",
            "Epoch: 0082 loss_train: 0.0152 acc_train: 0.9988 loss_val: 0.5549 acc_val: 0.8745\n",
            "Epoch: 0083 loss_train: 0.0143 acc_train: 0.9988 loss_val: 0.5609 acc_val: 0.8727\n",
            "Epoch: 0084 loss_train: 0.0129 acc_train: 0.9988 loss_val: 0.5681 acc_val: 0.8708\n",
            "Epoch: 0085 loss_train: 0.0116 acc_train: 0.9994 loss_val: 0.5765 acc_val: 0.8690\n",
            "Epoch: 0086 loss_train: 0.0102 acc_train: 0.9994 loss_val: 0.5853 acc_val: 0.8708\n",
            "Epoch: 0087 loss_train: 0.0091 acc_train: 0.9994 loss_val: 0.5937 acc_val: 0.8690\n",
            "Epoch: 0088 loss_train: 0.0083 acc_train: 0.9994 loss_val: 0.6011 acc_val: 0.8690\n",
            "Epoch: 0089 loss_train: 0.0079 acc_train: 1.0000 loss_val: 0.6077 acc_val: 0.8690\n",
            "Epoch: 0090 loss_train: 0.0069 acc_train: 1.0000 loss_val: 0.6139 acc_val: 0.8690\n",
            "Epoch: 0091 loss_train: 0.0067 acc_train: 1.0000 loss_val: 0.6196 acc_val: 0.8690\n",
            "Epoch: 0092 loss_train: 0.0059 acc_train: 1.0000 loss_val: 0.6249 acc_val: 0.8690\n",
            "Epoch: 0093 loss_train: 0.0054 acc_train: 1.0000 loss_val: 0.6300 acc_val: 0.8690\n",
            "Epoch: 0094 loss_train: 0.0053 acc_train: 1.0000 loss_val: 0.6335 acc_val: 0.8690\n",
            "Epoch: 0095 loss_train: 0.0047 acc_train: 1.0000 loss_val: 0.6375 acc_val: 0.8690\n",
            "Epoch: 0096 loss_train: 0.0044 acc_train: 1.0000 loss_val: 0.6415 acc_val: 0.8690\n",
            "Epoch: 0097 loss_train: 0.0042 acc_train: 1.0000 loss_val: 0.6453 acc_val: 0.8708\n",
            "Epoch: 0098 loss_train: 0.0041 acc_train: 1.0000 loss_val: 0.6494 acc_val: 0.8708\n",
            "Epoch: 0099 loss_train: 0.0036 acc_train: 1.0000 loss_val: 0.6533 acc_val: 0.8672\n",
            "Epoch: 0100 loss_train: 0.0036 acc_train: 1.0000 loss_val: 0.6570 acc_val: 0.8672\n",
            "Epoch: 0101 loss_train: 0.0033 acc_train: 1.0000 loss_val: 0.6602 acc_val: 0.8653\n",
            "Epoch: 0102 loss_train: 0.0032 acc_train: 1.0000 loss_val: 0.6631 acc_val: 0.8672\n",
            "Epoch: 0103 loss_train: 0.0030 acc_train: 1.0000 loss_val: 0.6652 acc_val: 0.8672\n",
            "Epoch: 0104 loss_train: 0.0027 acc_train: 1.0000 loss_val: 0.6664 acc_val: 0.8672\n",
            "Epoch: 0105 loss_train: 0.0025 acc_train: 1.0000 loss_val: 0.6670 acc_val: 0.8672\n",
            "Epoch: 0106 loss_train: 0.0025 acc_train: 1.0000 loss_val: 0.6677 acc_val: 0.8653\n",
            "Epoch: 0107 loss_train: 0.0024 acc_train: 1.0000 loss_val: 0.6682 acc_val: 0.8653\n",
            "Epoch: 0108 loss_train: 0.0023 acc_train: 1.0000 loss_val: 0.6687 acc_val: 0.8653\n",
            "Optimization Finished!\n",
            "Train cost: 15.0464s\n",
            "Loading 57th epoch\n",
            "Test set results: loss= 0.3490 accuracy= 0.9000\n",
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:354: DGLWarning: DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\n",
            "  dgl_warning(\"DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\")\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=1448, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=7, bias=True)\n",
            ")\n",
            "total params: 2979849\n",
            "Epoch: 0001 loss_train: 1.9677 acc_train: 0.1384 loss_val: 1.9615 acc_val: 0.1421\n",
            "Epoch: 0002 loss_train: 1.9623 acc_train: 0.1396 loss_val: 1.9501 acc_val: 0.1568\n",
            "Epoch: 0003 loss_train: 1.9510 acc_train: 0.1451 loss_val: 1.9331 acc_val: 0.1808\n",
            "Epoch: 0004 loss_train: 1.9359 acc_train: 0.1753 loss_val: 1.9107 acc_val: 0.2214\n",
            "Epoch: 0005 loss_train: 1.9158 acc_train: 0.2042 loss_val: 1.8832 acc_val: 0.2841\n",
            "Epoch: 0006 loss_train: 1.8920 acc_train: 0.2786 loss_val: 1.8508 acc_val: 0.3672\n",
            "Epoch: 0007 loss_train: 1.8619 acc_train: 0.3438 loss_val: 1.8142 acc_val: 0.4649\n",
            "Epoch: 0008 loss_train: 1.8300 acc_train: 0.4379 loss_val: 1.7741 acc_val: 0.4834\n",
            "Epoch: 0009 loss_train: 1.7936 acc_train: 0.4643 loss_val: 1.7308 acc_val: 0.4889\n",
            "Epoch: 0010 loss_train: 1.7540 acc_train: 0.4490 loss_val: 1.6854 acc_val: 0.4834\n",
            "Epoch: 0011 loss_train: 1.7114 acc_train: 0.4619 loss_val: 1.6383 acc_val: 0.4908\n",
            "Epoch: 0012 loss_train: 1.6676 acc_train: 0.4594 loss_val: 1.5901 acc_val: 0.4945\n",
            "Epoch: 0013 loss_train: 1.6227 acc_train: 0.4705 loss_val: 1.5406 acc_val: 0.4982\n",
            "Epoch: 0014 loss_train: 1.5777 acc_train: 0.4809 loss_val: 1.4899 acc_val: 0.5074\n",
            "Epoch: 0015 loss_train: 1.5281 acc_train: 0.4902 loss_val: 1.4373 acc_val: 0.5148\n",
            "Epoch: 0016 loss_train: 1.4778 acc_train: 0.4994 loss_val: 1.3822 acc_val: 0.5351\n",
            "Epoch: 0017 loss_train: 1.4243 acc_train: 0.5160 loss_val: 1.3247 acc_val: 0.5720\n",
            "Epoch: 0018 loss_train: 1.3684 acc_train: 0.5498 loss_val: 1.2661 acc_val: 0.6292\n",
            "Epoch: 0019 loss_train: 1.3127 acc_train: 0.5873 loss_val: 1.2091 acc_val: 0.6734\n",
            "Epoch: 0020 loss_train: 1.2549 acc_train: 0.6427 loss_val: 1.1554 acc_val: 0.6882\n",
            "Epoch: 0021 loss_train: 1.2013 acc_train: 0.6759 loss_val: 1.1056 acc_val: 0.6937\n",
            "Epoch: 0022 loss_train: 1.1481 acc_train: 0.6882 loss_val: 1.0593 acc_val: 0.7085\n",
            "Epoch: 0023 loss_train: 1.0974 acc_train: 0.7079 loss_val: 1.0153 acc_val: 0.7159\n",
            "Epoch: 0024 loss_train: 1.0487 acc_train: 0.7208 loss_val: 0.9731 acc_val: 0.7251\n",
            "Epoch: 0025 loss_train: 1.0025 acc_train: 0.7325 loss_val: 0.9332 acc_val: 0.7306\n",
            "Epoch: 0026 loss_train: 0.9564 acc_train: 0.7423 loss_val: 0.8963 acc_val: 0.7435\n",
            "Epoch: 0027 loss_train: 0.9151 acc_train: 0.7558 loss_val: 0.8628 acc_val: 0.7417\n",
            "Epoch: 0028 loss_train: 0.8774 acc_train: 0.7632 loss_val: 0.8320 acc_val: 0.7454\n",
            "Epoch: 0029 loss_train: 0.8408 acc_train: 0.7706 loss_val: 0.8028 acc_val: 0.7491\n",
            "Epoch: 0030 loss_train: 0.8073 acc_train: 0.7811 loss_val: 0.7748 acc_val: 0.7546\n",
            "Epoch: 0031 loss_train: 0.7709 acc_train: 0.7891 loss_val: 0.7485 acc_val: 0.7638\n",
            "Epoch: 0032 loss_train: 0.7391 acc_train: 0.7946 loss_val: 0.7239 acc_val: 0.7694\n",
            "Epoch: 0033 loss_train: 0.7106 acc_train: 0.8032 loss_val: 0.6998 acc_val: 0.7712\n",
            "Epoch: 0034 loss_train: 0.6795 acc_train: 0.8063 loss_val: 0.6754 acc_val: 0.7897\n",
            "Epoch: 0035 loss_train: 0.6529 acc_train: 0.8100 loss_val: 0.6512 acc_val: 0.7897\n",
            "Epoch: 0036 loss_train: 0.6214 acc_train: 0.8173 loss_val: 0.6276 acc_val: 0.7934\n",
            "Epoch: 0037 loss_train: 0.5955 acc_train: 0.8235 loss_val: 0.6055 acc_val: 0.8063\n",
            "Epoch: 0038 loss_train: 0.5699 acc_train: 0.8266 loss_val: 0.5848 acc_val: 0.8100\n",
            "Epoch: 0039 loss_train: 0.5408 acc_train: 0.8321 loss_val: 0.5651 acc_val: 0.8118\n",
            "Epoch: 0040 loss_train: 0.5149 acc_train: 0.8438 loss_val: 0.5463 acc_val: 0.8229\n",
            "Epoch: 0041 loss_train: 0.4907 acc_train: 0.8462 loss_val: 0.5281 acc_val: 0.8192\n",
            "Epoch: 0042 loss_train: 0.4648 acc_train: 0.8549 loss_val: 0.5106 acc_val: 0.8247\n",
            "Epoch: 0043 loss_train: 0.4405 acc_train: 0.8629 loss_val: 0.4944 acc_val: 0.8303\n",
            "Epoch: 0044 loss_train: 0.4203 acc_train: 0.8721 loss_val: 0.4798 acc_val: 0.8395\n",
            "Epoch: 0045 loss_train: 0.3995 acc_train: 0.8819 loss_val: 0.4678 acc_val: 0.8432\n",
            "Epoch: 0046 loss_train: 0.3764 acc_train: 0.8924 loss_val: 0.4594 acc_val: 0.8413\n",
            "Epoch: 0047 loss_train: 0.3558 acc_train: 0.8991 loss_val: 0.4531 acc_val: 0.8487\n",
            "Epoch: 0048 loss_train: 0.3365 acc_train: 0.9059 loss_val: 0.4469 acc_val: 0.8469\n",
            "Epoch: 0049 loss_train: 0.3161 acc_train: 0.9084 loss_val: 0.4420 acc_val: 0.8561\n",
            "Epoch: 0050 loss_train: 0.2962 acc_train: 0.9182 loss_val: 0.4399 acc_val: 0.8561\n",
            "Epoch: 0051 loss_train: 0.2759 acc_train: 0.9219 loss_val: 0.4420 acc_val: 0.8598\n",
            "Epoch: 0052 loss_train: 0.2582 acc_train: 0.9311 loss_val: 0.4445 acc_val: 0.8598\n",
            "Epoch: 0053 loss_train: 0.2399 acc_train: 0.9348 loss_val: 0.4455 acc_val: 0.8616\n",
            "Epoch: 0054 loss_train: 0.2229 acc_train: 0.9403 loss_val: 0.4450 acc_val: 0.8616\n",
            "Epoch: 0055 loss_train: 0.2028 acc_train: 0.9440 loss_val: 0.4472 acc_val: 0.8635\n",
            "Epoch: 0056 loss_train: 0.1866 acc_train: 0.9508 loss_val: 0.4516 acc_val: 0.8635\n",
            "Epoch: 0057 loss_train: 0.1709 acc_train: 0.9557 loss_val: 0.4556 acc_val: 0.8653\n",
            "Epoch: 0058 loss_train: 0.1599 acc_train: 0.9588 loss_val: 0.4586 acc_val: 0.8635\n",
            "Epoch: 0059 loss_train: 0.1464 acc_train: 0.9656 loss_val: 0.4645 acc_val: 0.8653\n",
            "Epoch: 0060 loss_train: 0.1341 acc_train: 0.9686 loss_val: 0.4723 acc_val: 0.8672\n",
            "Epoch: 0061 loss_train: 0.1205 acc_train: 0.9748 loss_val: 0.4799 acc_val: 0.8708\n",
            "Epoch: 0062 loss_train: 0.1108 acc_train: 0.9760 loss_val: 0.4849 acc_val: 0.8708\n",
            "Epoch: 0063 loss_train: 0.0987 acc_train: 0.9815 loss_val: 0.4909 acc_val: 0.8653\n",
            "Epoch: 0064 loss_train: 0.0877 acc_train: 0.9828 loss_val: 0.5001 acc_val: 0.8616\n",
            "Epoch: 0065 loss_train: 0.0791 acc_train: 0.9877 loss_val: 0.5130 acc_val: 0.8635\n",
            "Epoch: 0066 loss_train: 0.0715 acc_train: 0.9877 loss_val: 0.5219 acc_val: 0.8635\n",
            "Epoch: 0067 loss_train: 0.0636 acc_train: 0.9895 loss_val: 0.5262 acc_val: 0.8616\n",
            "Epoch: 0068 loss_train: 0.0569 acc_train: 0.9926 loss_val: 0.5305 acc_val: 0.8579\n",
            "Epoch: 0069 loss_train: 0.0516 acc_train: 0.9920 loss_val: 0.5382 acc_val: 0.8542\n",
            "Epoch: 0070 loss_train: 0.0451 acc_train: 0.9938 loss_val: 0.5499 acc_val: 0.8579\n",
            "Epoch: 0071 loss_train: 0.0399 acc_train: 0.9945 loss_val: 0.5596 acc_val: 0.8598\n",
            "Epoch: 0072 loss_train: 0.0364 acc_train: 0.9951 loss_val: 0.5659 acc_val: 0.8598\n",
            "Epoch: 0073 loss_train: 0.0319 acc_train: 0.9963 loss_val: 0.5695 acc_val: 0.8598\n",
            "Epoch: 0074 loss_train: 0.0283 acc_train: 0.9963 loss_val: 0.5746 acc_val: 0.8653\n",
            "Epoch: 0075 loss_train: 0.0248 acc_train: 0.9969 loss_val: 0.5824 acc_val: 0.8635\n",
            "Epoch: 0076 loss_train: 0.0224 acc_train: 0.9975 loss_val: 0.5939 acc_val: 0.8653\n",
            "Epoch: 0077 loss_train: 0.0188 acc_train: 0.9994 loss_val: 0.6078 acc_val: 0.8635\n",
            "Epoch: 0078 loss_train: 0.0163 acc_train: 1.0000 loss_val: 0.6191 acc_val: 0.8616\n",
            "Epoch: 0079 loss_train: 0.0145 acc_train: 1.0000 loss_val: 0.6285 acc_val: 0.8598\n",
            "Epoch: 0080 loss_train: 0.0139 acc_train: 1.0000 loss_val: 0.6352 acc_val: 0.8598\n",
            "Epoch: 0081 loss_train: 0.0120 acc_train: 1.0000 loss_val: 0.6420 acc_val: 0.8561\n",
            "Epoch: 0082 loss_train: 0.0110 acc_train: 1.0000 loss_val: 0.6501 acc_val: 0.8542\n",
            "Epoch: 0083 loss_train: 0.0102 acc_train: 1.0000 loss_val: 0.6601 acc_val: 0.8579\n",
            "Epoch: 0084 loss_train: 0.0093 acc_train: 1.0000 loss_val: 0.6700 acc_val: 0.8561\n",
            "Epoch: 0085 loss_train: 0.0084 acc_train: 1.0000 loss_val: 0.6776 acc_val: 0.8542\n",
            "Epoch: 0086 loss_train: 0.0077 acc_train: 1.0000 loss_val: 0.6836 acc_val: 0.8542\n",
            "Epoch: 0087 loss_train: 0.0070 acc_train: 1.0000 loss_val: 0.6876 acc_val: 0.8561\n",
            "Epoch: 0088 loss_train: 0.0063 acc_train: 1.0000 loss_val: 0.6910 acc_val: 0.8561\n",
            "Epoch: 0089 loss_train: 0.0059 acc_train: 1.0000 loss_val: 0.6944 acc_val: 0.8542\n",
            "Epoch: 0090 loss_train: 0.0055 acc_train: 1.0000 loss_val: 0.6999 acc_val: 0.8598\n",
            "Epoch: 0091 loss_train: 0.0049 acc_train: 1.0000 loss_val: 0.7067 acc_val: 0.8598\n",
            "Epoch: 0092 loss_train: 0.0046 acc_train: 1.0000 loss_val: 0.7139 acc_val: 0.8598\n",
            "Epoch: 0093 loss_train: 0.0042 acc_train: 1.0000 loss_val: 0.7208 acc_val: 0.8598\n",
            "Epoch: 0094 loss_train: 0.0041 acc_train: 1.0000 loss_val: 0.7261 acc_val: 0.8598\n",
            "Epoch: 0095 loss_train: 0.0037 acc_train: 1.0000 loss_val: 0.7296 acc_val: 0.8579\n",
            "Epoch: 0096 loss_train: 0.0034 acc_train: 1.0000 loss_val: 0.7318 acc_val: 0.8579\n",
            "Epoch: 0097 loss_train: 0.0033 acc_train: 1.0000 loss_val: 0.7331 acc_val: 0.8561\n",
            "Epoch: 0098 loss_train: 0.0031 acc_train: 1.0000 loss_val: 0.7340 acc_val: 0.8561\n",
            "Epoch: 0099 loss_train: 0.0028 acc_train: 1.0000 loss_val: 0.7356 acc_val: 0.8561\n",
            "Epoch: 0100 loss_train: 0.0028 acc_train: 1.0000 loss_val: 0.7377 acc_val: 0.8561\n",
            "Epoch: 0101 loss_train: 0.0025 acc_train: 1.0000 loss_val: 0.7408 acc_val: 0.8561\n",
            "Epoch: 0102 loss_train: 0.0025 acc_train: 1.0000 loss_val: 0.7450 acc_val: 0.8561\n",
            "Epoch: 0103 loss_train: 0.0023 acc_train: 1.0000 loss_val: 0.7498 acc_val: 0.8561\n",
            "Epoch: 0104 loss_train: 0.0022 acc_train: 1.0000 loss_val: 0.7547 acc_val: 0.8561\n",
            "Epoch: 0105 loss_train: 0.0021 acc_train: 1.0000 loss_val: 0.7594 acc_val: 0.8561\n",
            "Epoch: 0106 loss_train: 0.0020 acc_train: 1.0000 loss_val: 0.7637 acc_val: 0.8542\n",
            "Epoch: 0107 loss_train: 0.0020 acc_train: 1.0000 loss_val: 0.7675 acc_val: 0.8542\n",
            "Epoch: 0108 loss_train: 0.0019 acc_train: 1.0000 loss_val: 0.7705 acc_val: 0.8561\n",
            "Epoch: 0109 loss_train: 0.0018 acc_train: 1.0000 loss_val: 0.7727 acc_val: 0.8561\n",
            "Epoch: 0110 loss_train: 0.0018 acc_train: 1.0000 loss_val: 0.7740 acc_val: 0.8561\n",
            "Epoch: 0111 loss_train: 0.0017 acc_train: 1.0000 loss_val: 0.7748 acc_val: 0.8579\n",
            "Epoch: 0112 loss_train: 0.0016 acc_train: 1.0000 loss_val: 0.7754 acc_val: 0.8579\n",
            "Optimization Finished!\n",
            "Train cost: 15.5170s\n",
            "Loading 61th epoch\n",
            "Test set results: loss= 0.3972 accuracy= 0.8833\n",
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:354: DGLWarning: DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\n",
            "  dgl_warning(\"DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\")\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=1448, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=7, bias=True)\n",
            ")\n",
            "total params: 2979849\n",
            "Epoch: 0001 loss_train: 1.9815 acc_train: 0.1187 loss_val: 1.9746 acc_val: 0.1218\n",
            "Epoch: 0002 loss_train: 1.9777 acc_train: 0.1181 loss_val: 1.9630 acc_val: 0.1421\n",
            "Epoch: 0003 loss_train: 1.9653 acc_train: 0.1267 loss_val: 1.9458 acc_val: 0.1513\n",
            "Epoch: 0004 loss_train: 1.9507 acc_train: 0.1365 loss_val: 1.9233 acc_val: 0.1919\n",
            "Epoch: 0005 loss_train: 1.9308 acc_train: 0.1562 loss_val: 1.8959 acc_val: 0.2380\n",
            "Epoch: 0006 loss_train: 1.9077 acc_train: 0.2085 loss_val: 1.8639 acc_val: 0.3155\n",
            "Epoch: 0007 loss_train: 1.8783 acc_train: 0.2854 loss_val: 1.8280 acc_val: 0.3893\n",
            "Epoch: 0008 loss_train: 1.8461 acc_train: 0.3622 loss_val: 1.7886 acc_val: 0.4483\n",
            "Epoch: 0009 loss_train: 1.8100 acc_train: 0.4096 loss_val: 1.7467 acc_val: 0.4502\n",
            "Epoch: 0010 loss_train: 1.7718 acc_train: 0.4188 loss_val: 1.7026 acc_val: 0.4428\n",
            "Epoch: 0011 loss_train: 1.7304 acc_train: 0.4360 loss_val: 1.6572 acc_val: 0.4483\n",
            "Epoch: 0012 loss_train: 1.6887 acc_train: 0.4397 loss_val: 1.6109 acc_val: 0.4649\n",
            "Epoch: 0013 loss_train: 1.6459 acc_train: 0.4483 loss_val: 1.5638 acc_val: 0.4760\n",
            "Epoch: 0014 loss_train: 1.6026 acc_train: 0.4625 loss_val: 1.5151 acc_val: 0.4945\n",
            "Epoch: 0015 loss_train: 1.5561 acc_train: 0.4692 loss_val: 1.4642 acc_val: 0.5037\n",
            "Epoch: 0016 loss_train: 1.5082 acc_train: 0.4852 loss_val: 1.4105 acc_val: 0.5203\n",
            "Epoch: 0017 loss_train: 1.4558 acc_train: 0.5031 loss_val: 1.3540 acc_val: 0.5498\n",
            "Epoch: 0018 loss_train: 1.4017 acc_train: 0.5332 loss_val: 1.2962 acc_val: 0.6052\n",
            "Epoch: 0019 loss_train: 1.3457 acc_train: 0.5683 loss_val: 1.2393 acc_val: 0.6587\n",
            "Epoch: 0020 loss_train: 1.2891 acc_train: 0.6218 loss_val: 1.1853 acc_val: 0.6697\n",
            "Epoch: 0021 loss_train: 1.2366 acc_train: 0.6544 loss_val: 1.1346 acc_val: 0.6679\n",
            "Epoch: 0022 loss_train: 1.1818 acc_train: 0.6685 loss_val: 1.0869 acc_val: 0.6808\n",
            "Epoch: 0023 loss_train: 1.1295 acc_train: 0.6808 loss_val: 1.0417 acc_val: 0.6919\n",
            "Epoch: 0024 loss_train: 1.0816 acc_train: 0.6913 loss_val: 0.9982 acc_val: 0.7011\n",
            "Epoch: 0025 loss_train: 1.0346 acc_train: 0.7054 loss_val: 0.9577 acc_val: 0.7103\n",
            "Epoch: 0026 loss_train: 0.9893 acc_train: 0.7134 loss_val: 0.9213 acc_val: 0.7196\n",
            "Epoch: 0027 loss_train: 0.9470 acc_train: 0.7202 loss_val: 0.8887 acc_val: 0.7325\n",
            "Epoch: 0028 loss_train: 0.9119 acc_train: 0.7355 loss_val: 0.8591 acc_val: 0.7380\n",
            "Epoch: 0029 loss_train: 0.8769 acc_train: 0.7435 loss_val: 0.8310 acc_val: 0.7528\n",
            "Epoch: 0030 loss_train: 0.8451 acc_train: 0.7503 loss_val: 0.8043 acc_val: 0.7565\n",
            "Epoch: 0031 loss_train: 0.8105 acc_train: 0.7626 loss_val: 0.7792 acc_val: 0.7583\n",
            "Epoch: 0032 loss_train: 0.7827 acc_train: 0.7718 loss_val: 0.7556 acc_val: 0.7657\n",
            "Epoch: 0033 loss_train: 0.7522 acc_train: 0.7780 loss_val: 0.7326 acc_val: 0.7694\n",
            "Epoch: 0034 loss_train: 0.7246 acc_train: 0.7798 loss_val: 0.7096 acc_val: 0.7657\n",
            "Epoch: 0035 loss_train: 0.6980 acc_train: 0.7872 loss_val: 0.6870 acc_val: 0.7675\n",
            "Epoch: 0036 loss_train: 0.6675 acc_train: 0.7927 loss_val: 0.6656 acc_val: 0.7712\n",
            "Epoch: 0037 loss_train: 0.6414 acc_train: 0.7989 loss_val: 0.6453 acc_val: 0.7749\n",
            "Epoch: 0038 loss_train: 0.6156 acc_train: 0.8032 loss_val: 0.6257 acc_val: 0.7804\n",
            "Epoch: 0039 loss_train: 0.5870 acc_train: 0.8137 loss_val: 0.6064 acc_val: 0.7915\n",
            "Epoch: 0040 loss_train: 0.5608 acc_train: 0.8216 loss_val: 0.5874 acc_val: 0.8026\n",
            "Epoch: 0041 loss_train: 0.5338 acc_train: 0.8339 loss_val: 0.5686 acc_val: 0.8007\n",
            "Epoch: 0042 loss_train: 0.5080 acc_train: 0.8383 loss_val: 0.5502 acc_val: 0.8081\n",
            "Epoch: 0043 loss_train: 0.4827 acc_train: 0.8481 loss_val: 0.5329 acc_val: 0.8118\n",
            "Epoch: 0044 loss_train: 0.4604 acc_train: 0.8573 loss_val: 0.5174 acc_val: 0.8192\n",
            "Epoch: 0045 loss_train: 0.4356 acc_train: 0.8702 loss_val: 0.5049 acc_val: 0.8247\n",
            "Epoch: 0046 loss_train: 0.4115 acc_train: 0.8807 loss_val: 0.4953 acc_val: 0.8321\n",
            "Epoch: 0047 loss_train: 0.3886 acc_train: 0.8850 loss_val: 0.4870 acc_val: 0.8376\n",
            "Epoch: 0048 loss_train: 0.3648 acc_train: 0.8985 loss_val: 0.4786 acc_val: 0.8413\n",
            "Epoch: 0049 loss_train: 0.3423 acc_train: 0.9096 loss_val: 0.4715 acc_val: 0.8450\n",
            "Epoch: 0050 loss_train: 0.3209 acc_train: 0.9133 loss_val: 0.4673 acc_val: 0.8469\n",
            "Epoch: 0051 loss_train: 0.2981 acc_train: 0.9188 loss_val: 0.4674 acc_val: 0.8469\n",
            "Epoch: 0052 loss_train: 0.2776 acc_train: 0.9262 loss_val: 0.4687 acc_val: 0.8469\n",
            "Epoch: 0053 loss_train: 0.2559 acc_train: 0.9305 loss_val: 0.4690 acc_val: 0.8450\n",
            "Epoch: 0054 loss_train: 0.2363 acc_train: 0.9385 loss_val: 0.4688 acc_val: 0.8450\n",
            "Epoch: 0055 loss_train: 0.2140 acc_train: 0.9422 loss_val: 0.4720 acc_val: 0.8450\n",
            "Epoch: 0056 loss_train: 0.1967 acc_train: 0.9496 loss_val: 0.4779 acc_val: 0.8524\n",
            "Epoch: 0057 loss_train: 0.1783 acc_train: 0.9576 loss_val: 0.4828 acc_val: 0.8561\n",
            "Epoch: 0058 loss_train: 0.1659 acc_train: 0.9613 loss_val: 0.4854 acc_val: 0.8542\n",
            "Epoch: 0059 loss_train: 0.1511 acc_train: 0.9649 loss_val: 0.4915 acc_val: 0.8469\n",
            "Epoch: 0060 loss_train: 0.1355 acc_train: 0.9699 loss_val: 0.5016 acc_val: 0.8450\n",
            "Epoch: 0061 loss_train: 0.1226 acc_train: 0.9742 loss_val: 0.5119 acc_val: 0.8506\n",
            "Epoch: 0062 loss_train: 0.1114 acc_train: 0.9754 loss_val: 0.5183 acc_val: 0.8450\n",
            "Epoch: 0063 loss_train: 0.0992 acc_train: 0.9797 loss_val: 0.5253 acc_val: 0.8524\n",
            "Epoch: 0064 loss_train: 0.0870 acc_train: 0.9815 loss_val: 0.5349 acc_val: 0.8524\n",
            "Epoch: 0065 loss_train: 0.0781 acc_train: 0.9852 loss_val: 0.5477 acc_val: 0.8542\n",
            "Epoch: 0066 loss_train: 0.0702 acc_train: 0.9877 loss_val: 0.5581 acc_val: 0.8469\n",
            "Epoch: 0067 loss_train: 0.0624 acc_train: 0.9914 loss_val: 0.5638 acc_val: 0.8413\n",
            "Epoch: 0068 loss_train: 0.0559 acc_train: 0.9932 loss_val: 0.5694 acc_val: 0.8395\n",
            "Epoch: 0069 loss_train: 0.0499 acc_train: 0.9932 loss_val: 0.5789 acc_val: 0.8376\n",
            "Epoch: 0070 loss_train: 0.0441 acc_train: 0.9938 loss_val: 0.5913 acc_val: 0.8358\n",
            "Epoch: 0071 loss_train: 0.0398 acc_train: 0.9951 loss_val: 0.6014 acc_val: 0.8376\n",
            "Epoch: 0072 loss_train: 0.0358 acc_train: 0.9963 loss_val: 0.6084 acc_val: 0.8395\n",
            "Epoch: 0073 loss_train: 0.0315 acc_train: 0.9969 loss_val: 0.6136 acc_val: 0.8376\n",
            "Epoch: 0074 loss_train: 0.0281 acc_train: 0.9963 loss_val: 0.6206 acc_val: 0.8358\n",
            "Epoch: 0075 loss_train: 0.0257 acc_train: 0.9975 loss_val: 0.6301 acc_val: 0.8376\n",
            "Epoch: 0076 loss_train: 0.0238 acc_train: 0.9969 loss_val: 0.6430 acc_val: 0.8358\n",
            "Epoch: 0077 loss_train: 0.0210 acc_train: 0.9988 loss_val: 0.6566 acc_val: 0.8321\n",
            "Epoch: 0078 loss_train: 0.0188 acc_train: 0.9994 loss_val: 0.6681 acc_val: 0.8303\n",
            "Epoch: 0079 loss_train: 0.0172 acc_train: 0.9988 loss_val: 0.6771 acc_val: 0.8339\n",
            "Epoch: 0080 loss_train: 0.0166 acc_train: 0.9988 loss_val: 0.6845 acc_val: 0.8321\n",
            "Epoch: 0081 loss_train: 0.0148 acc_train: 0.9988 loss_val: 0.6923 acc_val: 0.8321\n",
            "Epoch: 0082 loss_train: 0.0136 acc_train: 0.9988 loss_val: 0.7015 acc_val: 0.8321\n",
            "Epoch: 0083 loss_train: 0.0129 acc_train: 0.9988 loss_val: 0.7132 acc_val: 0.8303\n",
            "Epoch: 0084 loss_train: 0.0117 acc_train: 0.9988 loss_val: 0.7242 acc_val: 0.8284\n",
            "Epoch: 0085 loss_train: 0.0104 acc_train: 0.9994 loss_val: 0.7336 acc_val: 0.8284\n",
            "Epoch: 0086 loss_train: 0.0090 acc_train: 0.9994 loss_val: 0.7414 acc_val: 0.8266\n",
            "Epoch: 0087 loss_train: 0.0083 acc_train: 0.9994 loss_val: 0.7475 acc_val: 0.8266\n",
            "Epoch: 0088 loss_train: 0.0072 acc_train: 0.9994 loss_val: 0.7532 acc_val: 0.8266\n",
            "Epoch: 0089 loss_train: 0.0063 acc_train: 1.0000 loss_val: 0.7590 acc_val: 0.8284\n",
            "Epoch: 0090 loss_train: 0.0060 acc_train: 1.0000 loss_val: 0.7661 acc_val: 0.8284\n",
            "Epoch: 0091 loss_train: 0.0053 acc_train: 1.0000 loss_val: 0.7747 acc_val: 0.8247\n",
            "Epoch: 0092 loss_train: 0.0050 acc_train: 1.0000 loss_val: 0.7844 acc_val: 0.8247\n",
            "Epoch: 0093 loss_train: 0.0045 acc_train: 1.0000 loss_val: 0.7934 acc_val: 0.8247\n",
            "Epoch: 0094 loss_train: 0.0045 acc_train: 1.0000 loss_val: 0.7994 acc_val: 0.8247\n",
            "Epoch: 0095 loss_train: 0.0041 acc_train: 1.0000 loss_val: 0.8031 acc_val: 0.8229\n",
            "Epoch: 0096 loss_train: 0.0038 acc_train: 1.0000 loss_val: 0.8052 acc_val: 0.8247\n",
            "Epoch: 0097 loss_train: 0.0035 acc_train: 1.0000 loss_val: 0.8066 acc_val: 0.8247\n",
            "Epoch: 0098 loss_train: 0.0033 acc_train: 1.0000 loss_val: 0.8080 acc_val: 0.8247\n",
            "Epoch: 0099 loss_train: 0.0031 acc_train: 1.0000 loss_val: 0.8099 acc_val: 0.8247\n",
            "Epoch: 0100 loss_train: 0.0031 acc_train: 1.0000 loss_val: 0.8129 acc_val: 0.8247\n",
            "Epoch: 0101 loss_train: 0.0028 acc_train: 1.0000 loss_val: 0.8166 acc_val: 0.8247\n",
            "Epoch: 0102 loss_train: 0.0027 acc_train: 1.0000 loss_val: 0.8204 acc_val: 0.8247\n",
            "Epoch: 0103 loss_train: 0.0027 acc_train: 1.0000 loss_val: 0.8242 acc_val: 0.8247\n",
            "Epoch: 0104 loss_train: 0.0026 acc_train: 1.0000 loss_val: 0.8267 acc_val: 0.8247\n",
            "Epoch: 0105 loss_train: 0.0022 acc_train: 1.0000 loss_val: 0.8292 acc_val: 0.8266\n",
            "Epoch: 0106 loss_train: 0.0021 acc_train: 1.0000 loss_val: 0.8322 acc_val: 0.8247\n",
            "Epoch: 0107 loss_train: 0.0023 acc_train: 1.0000 loss_val: 0.8362 acc_val: 0.8247\n",
            "Optimization Finished!\n",
            "Train cost: 14.4119s\n",
            "Loading 57th epoch\n",
            "Test set results: loss= 0.4142 accuracy= 0.8722\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --dataset cora --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 5  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack none --attperc 20 --TrainTest train --weight_decay=1e-05\n",
        "\n",
        "!python train.py --dataset cora --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 5  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack l2_strong --attperc 10 --TrainTest train --weight_decay=1e-05\n",
        "\n",
        "!python train.py --dataset cora --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 5  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack l2_strong --attperc 40 --TrainTest train --weight_decay=1e-05\n",
        "\n",
        "!python train.py --dataset cora --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 5  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack l2_strong --attperc 60 --TrainTest train --weight_decay=1e-05\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjEy5DcJZ251",
        "outputId": "95a16f6d-e673-40d5-a81b-400c2ffaaeb8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=1448, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=7, bias=True)\n",
            ")\n",
            "total params: 2979849\n",
            "Epoch: 0001 loss_train: 1.9727 acc_train: 0.1156 loss_val: 1.9686 acc_val: 0.1310\n",
            "Epoch: 0002 loss_train: 1.9669 acc_train: 0.1279 loss_val: 1.9561 acc_val: 0.1384\n",
            "Epoch: 0003 loss_train: 1.9547 acc_train: 0.1396 loss_val: 1.9375 acc_val: 0.1587\n",
            "Epoch: 0004 loss_train: 1.9373 acc_train: 0.1525 loss_val: 1.9130 acc_val: 0.2122\n",
            "Epoch: 0005 loss_train: 1.9160 acc_train: 0.1925 loss_val: 1.8830 acc_val: 0.3118\n",
            "Epoch: 0006 loss_train: 1.8886 acc_train: 0.2768 loss_val: 1.8478 acc_val: 0.4428\n",
            "Epoch: 0007 loss_train: 1.8561 acc_train: 0.3930 loss_val: 1.8078 acc_val: 0.4815\n",
            "Epoch: 0008 loss_train: 1.8199 acc_train: 0.4736 loss_val: 1.7640 acc_val: 0.5055\n",
            "Epoch: 0009 loss_train: 1.7805 acc_train: 0.4889 loss_val: 1.7170 acc_val: 0.5055\n",
            "Epoch: 0010 loss_train: 1.7355 acc_train: 0.5092 loss_val: 1.6678 acc_val: 0.5166\n",
            "Epoch: 0011 loss_train: 1.6880 acc_train: 0.5203 loss_val: 1.6169 acc_val: 0.5277\n",
            "Epoch: 0012 loss_train: 1.6405 acc_train: 0.5295 loss_val: 1.5647 acc_val: 0.5461\n",
            "Epoch: 0013 loss_train: 1.5898 acc_train: 0.5387 loss_val: 1.5112 acc_val: 0.5517\n",
            "Epoch: 0014 loss_train: 1.5383 acc_train: 0.5541 loss_val: 1.4567 acc_val: 0.5627\n",
            "Epoch: 0015 loss_train: 1.4837 acc_train: 0.5658 loss_val: 1.4014 acc_val: 0.5701\n",
            "Epoch: 0016 loss_train: 1.4274 acc_train: 0.5824 loss_val: 1.3452 acc_val: 0.5849\n",
            "Epoch: 0017 loss_train: 1.3709 acc_train: 0.5996 loss_val: 1.2880 acc_val: 0.6070\n",
            "Epoch: 0018 loss_train: 1.3128 acc_train: 0.6193 loss_val: 1.2305 acc_val: 0.6494\n",
            "Epoch: 0019 loss_train: 1.2546 acc_train: 0.6513 loss_val: 1.1740 acc_val: 0.6734\n",
            "Epoch: 0020 loss_train: 1.1946 acc_train: 0.6845 loss_val: 1.1196 acc_val: 0.6882\n",
            "Epoch: 0021 loss_train: 1.1373 acc_train: 0.7103 loss_val: 1.0686 acc_val: 0.6993\n",
            "Epoch: 0022 loss_train: 1.0810 acc_train: 0.7269 loss_val: 1.0214 acc_val: 0.6974\n",
            "Epoch: 0023 loss_train: 1.0270 acc_train: 0.7386 loss_val: 0.9779 acc_val: 0.7103\n",
            "Epoch: 0024 loss_train: 0.9762 acc_train: 0.7491 loss_val: 0.9369 acc_val: 0.7214\n",
            "Epoch: 0025 loss_train: 0.9308 acc_train: 0.7558 loss_val: 0.8978 acc_val: 0.7288\n",
            "Epoch: 0026 loss_train: 0.8845 acc_train: 0.7706 loss_val: 0.8600 acc_val: 0.7306\n",
            "Epoch: 0027 loss_train: 0.8421 acc_train: 0.7811 loss_val: 0.8239 acc_val: 0.7472\n",
            "Epoch: 0028 loss_train: 0.8022 acc_train: 0.8007 loss_val: 0.7899 acc_val: 0.7509\n",
            "Epoch: 0029 loss_train: 0.7630 acc_train: 0.8093 loss_val: 0.7580 acc_val: 0.7638\n",
            "Epoch: 0030 loss_train: 0.7271 acc_train: 0.8167 loss_val: 0.7286 acc_val: 0.7768\n",
            "Epoch: 0031 loss_train: 0.6939 acc_train: 0.8266 loss_val: 0.7010 acc_val: 0.7804\n",
            "Epoch: 0032 loss_train: 0.6632 acc_train: 0.8284 loss_val: 0.6747 acc_val: 0.7823\n",
            "Epoch: 0033 loss_train: 0.6327 acc_train: 0.8346 loss_val: 0.6496 acc_val: 0.7823\n",
            "Epoch: 0034 loss_train: 0.6044 acc_train: 0.8432 loss_val: 0.6263 acc_val: 0.7878\n",
            "Epoch: 0035 loss_train: 0.5759 acc_train: 0.8481 loss_val: 0.6049 acc_val: 0.8044\n",
            "Epoch: 0036 loss_train: 0.5496 acc_train: 0.8524 loss_val: 0.5844 acc_val: 0.8081\n",
            "Epoch: 0037 loss_train: 0.5268 acc_train: 0.8598 loss_val: 0.5635 acc_val: 0.8118\n",
            "Epoch: 0038 loss_train: 0.5036 acc_train: 0.8598 loss_val: 0.5414 acc_val: 0.8192\n",
            "Epoch: 0039 loss_train: 0.4787 acc_train: 0.8647 loss_val: 0.5187 acc_val: 0.8266\n",
            "Epoch: 0040 loss_train: 0.4569 acc_train: 0.8684 loss_val: 0.4966 acc_val: 0.8266\n",
            "Epoch: 0041 loss_train: 0.4365 acc_train: 0.8739 loss_val: 0.4760 acc_val: 0.8303\n",
            "Epoch: 0042 loss_train: 0.4171 acc_train: 0.8733 loss_val: 0.4578 acc_val: 0.8413\n",
            "Epoch: 0043 loss_train: 0.3960 acc_train: 0.8819 loss_val: 0.4426 acc_val: 0.8561\n",
            "Epoch: 0044 loss_train: 0.3774 acc_train: 0.8887 loss_val: 0.4296 acc_val: 0.8635\n",
            "Epoch: 0045 loss_train: 0.3595 acc_train: 0.8918 loss_val: 0.4170 acc_val: 0.8690\n",
            "Epoch: 0046 loss_train: 0.3405 acc_train: 0.8998 loss_val: 0.4041 acc_val: 0.8727\n",
            "Epoch: 0047 loss_train: 0.3229 acc_train: 0.9090 loss_val: 0.3923 acc_val: 0.8745\n",
            "Epoch: 0048 loss_train: 0.3065 acc_train: 0.9157 loss_val: 0.3836 acc_val: 0.8838\n",
            "Epoch: 0049 loss_train: 0.2879 acc_train: 0.9194 loss_val: 0.3782 acc_val: 0.8875\n",
            "Epoch: 0050 loss_train: 0.2725 acc_train: 0.9274 loss_val: 0.3758 acc_val: 0.8893\n",
            "Epoch: 0051 loss_train: 0.2565 acc_train: 0.9293 loss_val: 0.3755 acc_val: 0.8838\n",
            "Epoch: 0052 loss_train: 0.2410 acc_train: 0.9342 loss_val: 0.3742 acc_val: 0.8819\n",
            "Epoch: 0053 loss_train: 0.2288 acc_train: 0.9403 loss_val: 0.3732 acc_val: 0.8838\n",
            "Epoch: 0054 loss_train: 0.2129 acc_train: 0.9434 loss_val: 0.3749 acc_val: 0.8819\n",
            "Epoch: 0055 loss_train: 0.1947 acc_train: 0.9508 loss_val: 0.3805 acc_val: 0.8782\n",
            "Epoch: 0056 loss_train: 0.1813 acc_train: 0.9533 loss_val: 0.3863 acc_val: 0.8801\n",
            "Epoch: 0057 loss_train: 0.1677 acc_train: 0.9576 loss_val: 0.3874 acc_val: 0.8782\n",
            "Epoch: 0058 loss_train: 0.1540 acc_train: 0.9619 loss_val: 0.3872 acc_val: 0.8782\n",
            "Epoch: 0059 loss_train: 0.1432 acc_train: 0.9656 loss_val: 0.3902 acc_val: 0.8782\n",
            "Epoch: 0060 loss_train: 0.1315 acc_train: 0.9686 loss_val: 0.3960 acc_val: 0.8801\n",
            "Epoch: 0061 loss_train: 0.1190 acc_train: 0.9729 loss_val: 0.4023 acc_val: 0.8782\n",
            "Epoch: 0062 loss_train: 0.1102 acc_train: 0.9760 loss_val: 0.4080 acc_val: 0.8764\n",
            "Epoch: 0063 loss_train: 0.1000 acc_train: 0.9822 loss_val: 0.4138 acc_val: 0.8764\n",
            "Epoch: 0064 loss_train: 0.0907 acc_train: 0.9859 loss_val: 0.4196 acc_val: 0.8764\n",
            "Epoch: 0065 loss_train: 0.0822 acc_train: 0.9865 loss_val: 0.4272 acc_val: 0.8801\n",
            "Epoch: 0066 loss_train: 0.0764 acc_train: 0.9852 loss_val: 0.4338 acc_val: 0.8745\n",
            "Epoch: 0067 loss_train: 0.0687 acc_train: 0.9871 loss_val: 0.4419 acc_val: 0.8727\n",
            "Epoch: 0068 loss_train: 0.0618 acc_train: 0.9895 loss_val: 0.4517 acc_val: 0.8708\n",
            "Epoch: 0069 loss_train: 0.0564 acc_train: 0.9914 loss_val: 0.4612 acc_val: 0.8727\n",
            "Epoch: 0070 loss_train: 0.0516 acc_train: 0.9908 loss_val: 0.4702 acc_val: 0.8708\n",
            "Epoch: 0071 loss_train: 0.0470 acc_train: 0.9920 loss_val: 0.4785 acc_val: 0.8690\n",
            "Epoch: 0072 loss_train: 0.0425 acc_train: 0.9926 loss_val: 0.4848 acc_val: 0.8690\n",
            "Epoch: 0073 loss_train: 0.0387 acc_train: 0.9945 loss_val: 0.4902 acc_val: 0.8690\n",
            "Epoch: 0074 loss_train: 0.0355 acc_train: 0.9938 loss_val: 0.4948 acc_val: 0.8672\n",
            "Epoch: 0075 loss_train: 0.0312 acc_train: 0.9945 loss_val: 0.4993 acc_val: 0.8672\n",
            "Epoch: 0076 loss_train: 0.0288 acc_train: 0.9963 loss_val: 0.5046 acc_val: 0.8690\n",
            "Epoch: 0077 loss_train: 0.0262 acc_train: 0.9951 loss_val: 0.5107 acc_val: 0.8690\n",
            "Epoch: 0078 loss_train: 0.0229 acc_train: 0.9988 loss_val: 0.5159 acc_val: 0.8708\n",
            "Epoch: 0079 loss_train: 0.0213 acc_train: 0.9982 loss_val: 0.5216 acc_val: 0.8708\n",
            "Epoch: 0080 loss_train: 0.0198 acc_train: 0.9975 loss_val: 0.5286 acc_val: 0.8708\n",
            "Epoch: 0081 loss_train: 0.0173 acc_train: 0.9982 loss_val: 0.5363 acc_val: 0.8690\n",
            "Epoch: 0082 loss_train: 0.0152 acc_train: 0.9994 loss_val: 0.5455 acc_val: 0.8708\n",
            "Epoch: 0083 loss_train: 0.0139 acc_train: 0.9988 loss_val: 0.5557 acc_val: 0.8727\n",
            "Epoch: 0084 loss_train: 0.0124 acc_train: 0.9994 loss_val: 0.5655 acc_val: 0.8708\n",
            "Epoch: 0085 loss_train: 0.0114 acc_train: 0.9994 loss_val: 0.5740 acc_val: 0.8690\n",
            "Epoch: 0086 loss_train: 0.0101 acc_train: 1.0000 loss_val: 0.5804 acc_val: 0.8690\n",
            "Epoch: 0087 loss_train: 0.0090 acc_train: 1.0000 loss_val: 0.5846 acc_val: 0.8708\n",
            "Epoch: 0088 loss_train: 0.0084 acc_train: 1.0000 loss_val: 0.5882 acc_val: 0.8690\n",
            "Epoch: 0089 loss_train: 0.0082 acc_train: 0.9994 loss_val: 0.5925 acc_val: 0.8708\n",
            "Epoch: 0090 loss_train: 0.0073 acc_train: 1.0000 loss_val: 0.5984 acc_val: 0.8708\n",
            "Epoch: 0091 loss_train: 0.0067 acc_train: 1.0000 loss_val: 0.6056 acc_val: 0.8690\n",
            "Epoch: 0092 loss_train: 0.0060 acc_train: 1.0000 loss_val: 0.6134 acc_val: 0.8690\n",
            "Epoch: 0093 loss_train: 0.0053 acc_train: 1.0000 loss_val: 0.6213 acc_val: 0.8708\n",
            "Epoch: 0094 loss_train: 0.0052 acc_train: 1.0000 loss_val: 0.6275 acc_val: 0.8690\n",
            "Epoch: 0095 loss_train: 0.0047 acc_train: 1.0000 loss_val: 0.6331 acc_val: 0.8672\n",
            "Epoch: 0096 loss_train: 0.0043 acc_train: 1.0000 loss_val: 0.6373 acc_val: 0.8672\n",
            "Epoch: 0097 loss_train: 0.0041 acc_train: 1.0000 loss_val: 0.6404 acc_val: 0.8690\n",
            "Epoch: 0098 loss_train: 0.0040 acc_train: 1.0000 loss_val: 0.6428 acc_val: 0.8672\n",
            "Epoch: 0099 loss_train: 0.0035 acc_train: 1.0000 loss_val: 0.6451 acc_val: 0.8672\n",
            "Epoch: 0100 loss_train: 0.0035 acc_train: 1.0000 loss_val: 0.6471 acc_val: 0.8635\n",
            "Epoch: 0101 loss_train: 0.0032 acc_train: 1.0000 loss_val: 0.6496 acc_val: 0.8635\n",
            "Epoch: 0102 loss_train: 0.0031 acc_train: 1.0000 loss_val: 0.6526 acc_val: 0.8635\n",
            "Epoch: 0103 loss_train: 0.0029 acc_train: 1.0000 loss_val: 0.6555 acc_val: 0.8635\n",
            "Optimization Finished!\n",
            "Train cost: 14.0709s\n",
            "Loading 50th epoch\n",
            "Test set results: loss= 0.3476 accuracy= 0.9019\n",
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:354: DGLWarning: DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\n",
            "  dgl_warning(\"DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\")\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=1448, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=7, bias=True)\n",
            ")\n",
            "total params: 2979849\n",
            "Epoch: 0001 loss_train: 1.9910 acc_train: 0.0750 loss_val: 1.9826 acc_val: 0.0923\n",
            "Epoch: 0002 loss_train: 1.9858 acc_train: 0.0818 loss_val: 1.9706 acc_val: 0.1033\n",
            "Epoch: 0003 loss_train: 1.9733 acc_train: 0.0959 loss_val: 1.9529 acc_val: 0.1144\n",
            "Epoch: 0004 loss_train: 1.9570 acc_train: 0.0984 loss_val: 1.9296 acc_val: 0.1439\n",
            "Epoch: 0005 loss_train: 1.9363 acc_train: 0.1175 loss_val: 1.9011 acc_val: 0.2196\n",
            "Epoch: 0006 loss_train: 1.9122 acc_train: 0.1882 loss_val: 1.8679 acc_val: 0.3044\n",
            "Epoch: 0007 loss_train: 1.8811 acc_train: 0.2878 loss_val: 1.8305 acc_val: 0.4225\n",
            "Epoch: 0008 loss_train: 1.8489 acc_train: 0.3893 loss_val: 1.7895 acc_val: 0.4705\n",
            "Epoch: 0009 loss_train: 1.8117 acc_train: 0.4348 loss_val: 1.7455 acc_val: 0.4871\n",
            "Epoch: 0010 loss_train: 1.7700 acc_train: 0.4520 loss_val: 1.6997 acc_val: 0.5000\n",
            "Epoch: 0011 loss_train: 1.7270 acc_train: 0.4797 loss_val: 1.6528 acc_val: 0.5148\n",
            "Epoch: 0012 loss_train: 1.6839 acc_train: 0.4803 loss_val: 1.6051 acc_val: 0.5295\n",
            "Epoch: 0013 loss_train: 1.6401 acc_train: 0.4957 loss_val: 1.5565 acc_val: 0.5332\n",
            "Epoch: 0014 loss_train: 1.5952 acc_train: 0.5055 loss_val: 1.5069 acc_val: 0.5295\n",
            "Epoch: 0015 loss_train: 1.5476 acc_train: 0.5141 loss_val: 1.4555 acc_val: 0.5424\n",
            "Epoch: 0016 loss_train: 1.4984 acc_train: 0.5320 loss_val: 1.4016 acc_val: 0.5517\n",
            "Epoch: 0017 loss_train: 1.4451 acc_train: 0.5523 loss_val: 1.3453 acc_val: 0.5720\n",
            "Epoch: 0018 loss_train: 1.3916 acc_train: 0.5738 loss_val: 1.2874 acc_val: 0.6144\n",
            "Epoch: 0019 loss_train: 1.3356 acc_train: 0.6082 loss_val: 1.2294 acc_val: 0.6605\n",
            "Epoch: 0020 loss_train: 1.2756 acc_train: 0.6593 loss_val: 1.1733 acc_val: 0.6863\n",
            "Epoch: 0021 loss_train: 1.2185 acc_train: 0.6790 loss_val: 1.1205 acc_val: 0.7030\n",
            "Epoch: 0022 loss_train: 1.1614 acc_train: 0.7048 loss_val: 1.0709 acc_val: 0.7159\n",
            "Epoch: 0023 loss_train: 1.1063 acc_train: 0.7232 loss_val: 1.0234 acc_val: 0.7288\n",
            "Epoch: 0024 loss_train: 1.0512 acc_train: 0.7399 loss_val: 0.9769 acc_val: 0.7325\n",
            "Epoch: 0025 loss_train: 1.0001 acc_train: 0.7528 loss_val: 0.9318 acc_val: 0.7343\n",
            "Epoch: 0026 loss_train: 0.9495 acc_train: 0.7632 loss_val: 0.8892 acc_val: 0.7380\n",
            "Epoch: 0027 loss_train: 0.9009 acc_train: 0.7700 loss_val: 0.8498 acc_val: 0.7435\n",
            "Epoch: 0028 loss_train: 0.8580 acc_train: 0.7774 loss_val: 0.8142 acc_val: 0.7528\n",
            "Epoch: 0029 loss_train: 0.8160 acc_train: 0.7854 loss_val: 0.7819 acc_val: 0.7565\n",
            "Epoch: 0030 loss_train: 0.7800 acc_train: 0.7897 loss_val: 0.7519 acc_val: 0.7694\n",
            "Epoch: 0031 loss_train: 0.7423 acc_train: 0.8044 loss_val: 0.7238 acc_val: 0.7731\n",
            "Epoch: 0032 loss_train: 0.7055 acc_train: 0.8118 loss_val: 0.6974 acc_val: 0.7804\n",
            "Epoch: 0033 loss_train: 0.6715 acc_train: 0.8223 loss_val: 0.6721 acc_val: 0.7952\n",
            "Epoch: 0034 loss_train: 0.6401 acc_train: 0.8370 loss_val: 0.6468 acc_val: 0.8026\n",
            "Epoch: 0035 loss_train: 0.6091 acc_train: 0.8462 loss_val: 0.6213 acc_val: 0.8100\n",
            "Epoch: 0036 loss_train: 0.5784 acc_train: 0.8481 loss_val: 0.5964 acc_val: 0.8081\n",
            "Epoch: 0037 loss_train: 0.5502 acc_train: 0.8555 loss_val: 0.5724 acc_val: 0.8100\n",
            "Epoch: 0038 loss_train: 0.5233 acc_train: 0.8573 loss_val: 0.5492 acc_val: 0.8192\n",
            "Epoch: 0039 loss_train: 0.4969 acc_train: 0.8592 loss_val: 0.5271 acc_val: 0.8339\n",
            "Epoch: 0040 loss_train: 0.4708 acc_train: 0.8684 loss_val: 0.5054 acc_val: 0.8432\n",
            "Epoch: 0041 loss_train: 0.4484 acc_train: 0.8733 loss_val: 0.4841 acc_val: 0.8450\n",
            "Epoch: 0042 loss_train: 0.4242 acc_train: 0.8795 loss_val: 0.4638 acc_val: 0.8524\n",
            "Epoch: 0043 loss_train: 0.4002 acc_train: 0.8856 loss_val: 0.4461 acc_val: 0.8598\n",
            "Epoch: 0044 loss_train: 0.3787 acc_train: 0.8905 loss_val: 0.4308 acc_val: 0.8672\n",
            "Epoch: 0045 loss_train: 0.3580 acc_train: 0.8967 loss_val: 0.4171 acc_val: 0.8653\n",
            "Epoch: 0046 loss_train: 0.3362 acc_train: 0.9053 loss_val: 0.4043 acc_val: 0.8690\n",
            "Epoch: 0047 loss_train: 0.3183 acc_train: 0.9133 loss_val: 0.3921 acc_val: 0.8745\n",
            "Epoch: 0048 loss_train: 0.2977 acc_train: 0.9176 loss_val: 0.3815 acc_val: 0.8782\n",
            "Epoch: 0049 loss_train: 0.2789 acc_train: 0.9207 loss_val: 0.3739 acc_val: 0.8856\n",
            "Epoch: 0050 loss_train: 0.2617 acc_train: 0.9311 loss_val: 0.3700 acc_val: 0.8819\n",
            "Epoch: 0051 loss_train: 0.2444 acc_train: 0.9373 loss_val: 0.3696 acc_val: 0.8838\n",
            "Epoch: 0052 loss_train: 0.2294 acc_train: 0.9397 loss_val: 0.3707 acc_val: 0.8819\n",
            "Epoch: 0053 loss_train: 0.2158 acc_train: 0.9428 loss_val: 0.3716 acc_val: 0.8819\n",
            "Epoch: 0054 loss_train: 0.2010 acc_train: 0.9459 loss_val: 0.3732 acc_val: 0.8838\n",
            "Epoch: 0055 loss_train: 0.1848 acc_train: 0.9526 loss_val: 0.3771 acc_val: 0.8819\n",
            "Epoch: 0056 loss_train: 0.1712 acc_train: 0.9569 loss_val: 0.3821 acc_val: 0.8856\n",
            "Epoch: 0057 loss_train: 0.1569 acc_train: 0.9643 loss_val: 0.3865 acc_val: 0.8856\n",
            "Epoch: 0058 loss_train: 0.1474 acc_train: 0.9674 loss_val: 0.3912 acc_val: 0.8856\n",
            "Epoch: 0059 loss_train: 0.1345 acc_train: 0.9699 loss_val: 0.3972 acc_val: 0.8838\n",
            "Epoch: 0060 loss_train: 0.1212 acc_train: 0.9729 loss_val: 0.4035 acc_val: 0.8801\n",
            "Epoch: 0061 loss_train: 0.1102 acc_train: 0.9754 loss_val: 0.4078 acc_val: 0.8782\n",
            "Epoch: 0062 loss_train: 0.1014 acc_train: 0.9797 loss_val: 0.4118 acc_val: 0.8801\n",
            "Epoch: 0063 loss_train: 0.0929 acc_train: 0.9822 loss_val: 0.4172 acc_val: 0.8782\n",
            "Epoch: 0064 loss_train: 0.0834 acc_train: 0.9840 loss_val: 0.4244 acc_val: 0.8745\n",
            "Epoch: 0065 loss_train: 0.0758 acc_train: 0.9852 loss_val: 0.4340 acc_val: 0.8727\n",
            "Epoch: 0066 loss_train: 0.0711 acc_train: 0.9846 loss_val: 0.4422 acc_val: 0.8690\n",
            "Epoch: 0067 loss_train: 0.0637 acc_train: 0.9883 loss_val: 0.4495 acc_val: 0.8708\n",
            "Epoch: 0068 loss_train: 0.0572 acc_train: 0.9902 loss_val: 0.4575 acc_val: 0.8690\n",
            "Epoch: 0069 loss_train: 0.0531 acc_train: 0.9938 loss_val: 0.4664 acc_val: 0.8690\n",
            "Epoch: 0070 loss_train: 0.0491 acc_train: 0.9932 loss_val: 0.4755 acc_val: 0.8690\n",
            "Epoch: 0071 loss_train: 0.0443 acc_train: 0.9920 loss_val: 0.4841 acc_val: 0.8764\n",
            "Epoch: 0072 loss_train: 0.0399 acc_train: 0.9938 loss_val: 0.4903 acc_val: 0.8708\n",
            "Epoch: 0073 loss_train: 0.0371 acc_train: 0.9951 loss_val: 0.4955 acc_val: 0.8745\n",
            "Epoch: 0074 loss_train: 0.0344 acc_train: 0.9945 loss_val: 0.5013 acc_val: 0.8727\n",
            "Epoch: 0075 loss_train: 0.0307 acc_train: 0.9945 loss_val: 0.5078 acc_val: 0.8708\n",
            "Epoch: 0076 loss_train: 0.0283 acc_train: 0.9963 loss_val: 0.5164 acc_val: 0.8708\n",
            "Epoch: 0077 loss_train: 0.0248 acc_train: 0.9957 loss_val: 0.5260 acc_val: 0.8727\n",
            "Epoch: 0078 loss_train: 0.0217 acc_train: 0.9982 loss_val: 0.5346 acc_val: 0.8727\n",
            "Epoch: 0079 loss_train: 0.0189 acc_train: 0.9982 loss_val: 0.5433 acc_val: 0.8727\n",
            "Epoch: 0080 loss_train: 0.0180 acc_train: 0.9982 loss_val: 0.5509 acc_val: 0.8745\n",
            "Epoch: 0081 loss_train: 0.0156 acc_train: 0.9982 loss_val: 0.5577 acc_val: 0.8745\n",
            "Epoch: 0082 loss_train: 0.0140 acc_train: 1.0000 loss_val: 0.5641 acc_val: 0.8745\n",
            "Epoch: 0083 loss_train: 0.0131 acc_train: 0.9994 loss_val: 0.5710 acc_val: 0.8745\n",
            "Epoch: 0084 loss_train: 0.0117 acc_train: 1.0000 loss_val: 0.5783 acc_val: 0.8764\n",
            "Epoch: 0085 loss_train: 0.0107 acc_train: 0.9994 loss_val: 0.5850 acc_val: 0.8745\n",
            "Epoch: 0086 loss_train: 0.0098 acc_train: 1.0000 loss_val: 0.5909 acc_val: 0.8727\n",
            "Epoch: 0087 loss_train: 0.0089 acc_train: 0.9994 loss_val: 0.5954 acc_val: 0.8727\n",
            "Epoch: 0088 loss_train: 0.0079 acc_train: 1.0000 loss_val: 0.5992 acc_val: 0.8727\n",
            "Epoch: 0089 loss_train: 0.0075 acc_train: 1.0000 loss_val: 0.6041 acc_val: 0.8727\n",
            "Epoch: 0090 loss_train: 0.0067 acc_train: 1.0000 loss_val: 0.6108 acc_val: 0.8708\n",
            "Epoch: 0091 loss_train: 0.0065 acc_train: 0.9994 loss_val: 0.6187 acc_val: 0.8708\n",
            "Epoch: 0092 loss_train: 0.0057 acc_train: 1.0000 loss_val: 0.6281 acc_val: 0.8690\n",
            "Epoch: 0093 loss_train: 0.0052 acc_train: 1.0000 loss_val: 0.6378 acc_val: 0.8708\n",
            "Epoch: 0094 loss_train: 0.0053 acc_train: 1.0000 loss_val: 0.6428 acc_val: 0.8708\n",
            "Epoch: 0095 loss_train: 0.0045 acc_train: 1.0000 loss_val: 0.6467 acc_val: 0.8690\n",
            "Epoch: 0096 loss_train: 0.0043 acc_train: 1.0000 loss_val: 0.6488 acc_val: 0.8708\n",
            "Epoch: 0097 loss_train: 0.0041 acc_train: 1.0000 loss_val: 0.6487 acc_val: 0.8708\n",
            "Epoch: 0098 loss_train: 0.0039 acc_train: 1.0000 loss_val: 0.6484 acc_val: 0.8708\n",
            "Epoch: 0099 loss_train: 0.0035 acc_train: 1.0000 loss_val: 0.6485 acc_val: 0.8708\n",
            "Epoch: 0100 loss_train: 0.0035 acc_train: 1.0000 loss_val: 0.6491 acc_val: 0.8690\n",
            "Epoch: 0101 loss_train: 0.0033 acc_train: 1.0000 loss_val: 0.6511 acc_val: 0.8690\n",
            "Epoch: 0102 loss_train: 0.0031 acc_train: 1.0000 loss_val: 0.6551 acc_val: 0.8672\n",
            "Epoch: 0103 loss_train: 0.0029 acc_train: 1.0000 loss_val: 0.6593 acc_val: 0.8672\n",
            "Epoch: 0104 loss_train: 0.0027 acc_train: 1.0000 loss_val: 0.6634 acc_val: 0.8672\n",
            "Epoch: 0105 loss_train: 0.0025 acc_train: 1.0000 loss_val: 0.6675 acc_val: 0.8653\n",
            "Epoch: 0106 loss_train: 0.0025 acc_train: 1.0000 loss_val: 0.6715 acc_val: 0.8672\n",
            "Epoch: 0107 loss_train: 0.0024 acc_train: 1.0000 loss_val: 0.6748 acc_val: 0.8672\n",
            "Epoch: 0108 loss_train: 0.0023 acc_train: 1.0000 loss_val: 0.6771 acc_val: 0.8690\n",
            "Optimization Finished!\n",
            "Train cost: 14.3984s\n",
            "Loading 49th epoch\n",
            "Test set results: loss= 0.3473 accuracy= 0.8981\n",
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:354: DGLWarning: DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\n",
            "  dgl_warning(\"DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\")\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=1448, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=7, bias=True)\n",
            ")\n",
            "total params: 2979849\n",
            "Epoch: 0001 loss_train: 1.9656 acc_train: 0.1371 loss_val: 1.9597 acc_val: 0.1587\n",
            "Epoch: 0002 loss_train: 1.9608 acc_train: 0.1427 loss_val: 1.9482 acc_val: 0.1624\n",
            "Epoch: 0003 loss_train: 1.9516 acc_train: 0.1427 loss_val: 1.9312 acc_val: 0.1734\n",
            "Epoch: 0004 loss_train: 1.9362 acc_train: 0.1605 loss_val: 1.9090 acc_val: 0.2380\n",
            "Epoch: 0005 loss_train: 1.9177 acc_train: 0.1986 loss_val: 1.8824 acc_val: 0.3266\n",
            "Epoch: 0006 loss_train: 1.8954 acc_train: 0.2891 loss_val: 1.8521 acc_val: 0.3911\n",
            "Epoch: 0007 loss_train: 1.8688 acc_train: 0.3678 loss_val: 1.8190 acc_val: 0.4151\n",
            "Epoch: 0008 loss_train: 1.8410 acc_train: 0.3733 loss_val: 1.7844 acc_val: 0.4114\n",
            "Epoch: 0009 loss_train: 1.8135 acc_train: 0.3715 loss_val: 1.7491 acc_val: 0.4170\n",
            "Epoch: 0010 loss_train: 1.7814 acc_train: 0.3672 loss_val: 1.7136 acc_val: 0.4133\n",
            "Epoch: 0011 loss_train: 1.7509 acc_train: 0.3795 loss_val: 1.6779 acc_val: 0.4225\n",
            "Epoch: 0012 loss_train: 1.7213 acc_train: 0.3770 loss_val: 1.6415 acc_val: 0.4391\n",
            "Epoch: 0013 loss_train: 1.6917 acc_train: 0.3887 loss_val: 1.6033 acc_val: 0.4520\n",
            "Epoch: 0014 loss_train: 1.6598 acc_train: 0.3998 loss_val: 1.5620 acc_val: 0.4705\n",
            "Epoch: 0015 loss_train: 1.6239 acc_train: 0.4102 loss_val: 1.5162 acc_val: 0.4908\n",
            "Epoch: 0016 loss_train: 1.5879 acc_train: 0.4219 loss_val: 1.4653 acc_val: 0.5185\n",
            "Epoch: 0017 loss_train: 1.5421 acc_train: 0.4502 loss_val: 1.4102 acc_val: 0.5498\n",
            "Epoch: 0018 loss_train: 1.4935 acc_train: 0.4791 loss_val: 1.3533 acc_val: 0.5793\n",
            "Epoch: 0019 loss_train: 1.4420 acc_train: 0.5228 loss_val: 1.2971 acc_val: 0.6089\n",
            "Epoch: 0020 loss_train: 1.3879 acc_train: 0.5615 loss_val: 1.2426 acc_val: 0.6494\n",
            "Epoch: 0021 loss_train: 1.3357 acc_train: 0.6058 loss_val: 1.1886 acc_val: 0.6845\n",
            "Epoch: 0022 loss_train: 1.2787 acc_train: 0.6494 loss_val: 1.1332 acc_val: 0.6956\n",
            "Epoch: 0023 loss_train: 1.2229 acc_train: 0.6679 loss_val: 1.0779 acc_val: 0.7048\n",
            "Epoch: 0024 loss_train: 1.1627 acc_train: 0.6753 loss_val: 1.0259 acc_val: 0.7030\n",
            "Epoch: 0025 loss_train: 1.1073 acc_train: 0.6765 loss_val: 0.9781 acc_val: 0.7085\n",
            "Epoch: 0026 loss_train: 1.0522 acc_train: 0.6839 loss_val: 0.9335 acc_val: 0.7251\n",
            "Epoch: 0027 loss_train: 0.9974 acc_train: 0.7036 loss_val: 0.8933 acc_val: 0.7306\n",
            "Epoch: 0028 loss_train: 0.9438 acc_train: 0.7442 loss_val: 0.8583 acc_val: 0.7435\n",
            "Epoch: 0029 loss_train: 0.8958 acc_train: 0.7761 loss_val: 0.8246 acc_val: 0.7509\n",
            "Epoch: 0030 loss_train: 0.8458 acc_train: 0.7946 loss_val: 0.7900 acc_val: 0.7565\n",
            "Epoch: 0031 loss_train: 0.7932 acc_train: 0.8081 loss_val: 0.7577 acc_val: 0.7657\n",
            "Epoch: 0032 loss_train: 0.7496 acc_train: 0.8057 loss_val: 0.7282 acc_val: 0.7804\n",
            "Epoch: 0033 loss_train: 0.7048 acc_train: 0.8216 loss_val: 0.7015 acc_val: 0.7878\n",
            "Epoch: 0034 loss_train: 0.6652 acc_train: 0.8383 loss_val: 0.6736 acc_val: 0.7934\n",
            "Epoch: 0035 loss_train: 0.6260 acc_train: 0.8487 loss_val: 0.6417 acc_val: 0.8118\n",
            "Epoch: 0036 loss_train: 0.5868 acc_train: 0.8506 loss_val: 0.6112 acc_val: 0.8173\n",
            "Epoch: 0037 loss_train: 0.5514 acc_train: 0.8604 loss_val: 0.5865 acc_val: 0.8247\n",
            "Epoch: 0038 loss_train: 0.5197 acc_train: 0.8678 loss_val: 0.5669 acc_val: 0.8247\n",
            "Epoch: 0039 loss_train: 0.4910 acc_train: 0.8776 loss_val: 0.5458 acc_val: 0.8284\n",
            "Epoch: 0040 loss_train: 0.4594 acc_train: 0.8801 loss_val: 0.5260 acc_val: 0.8303\n",
            "Epoch: 0041 loss_train: 0.4358 acc_train: 0.8825 loss_val: 0.5103 acc_val: 0.8358\n",
            "Epoch: 0042 loss_train: 0.4063 acc_train: 0.8905 loss_val: 0.4956 acc_val: 0.8395\n",
            "Epoch: 0043 loss_train: 0.3812 acc_train: 0.8930 loss_val: 0.4778 acc_val: 0.8395\n",
            "Epoch: 0044 loss_train: 0.3580 acc_train: 0.8967 loss_val: 0.4608 acc_val: 0.8413\n",
            "Epoch: 0045 loss_train: 0.3391 acc_train: 0.9010 loss_val: 0.4487 acc_val: 0.8432\n",
            "Epoch: 0046 loss_train: 0.3161 acc_train: 0.9084 loss_val: 0.4398 acc_val: 0.8506\n",
            "Epoch: 0047 loss_train: 0.2955 acc_train: 0.9207 loss_val: 0.4298 acc_val: 0.8506\n",
            "Epoch: 0048 loss_train: 0.2769 acc_train: 0.9213 loss_val: 0.4174 acc_val: 0.8579\n",
            "Epoch: 0049 loss_train: 0.2581 acc_train: 0.9280 loss_val: 0.4076 acc_val: 0.8616\n",
            "Epoch: 0050 loss_train: 0.2422 acc_train: 0.9305 loss_val: 0.4026 acc_val: 0.8635\n",
            "Epoch: 0051 loss_train: 0.2249 acc_train: 0.9342 loss_val: 0.3988 acc_val: 0.8708\n",
            "Epoch: 0052 loss_train: 0.2095 acc_train: 0.9446 loss_val: 0.3907 acc_val: 0.8672\n",
            "Epoch: 0053 loss_train: 0.1951 acc_train: 0.9459 loss_val: 0.3841 acc_val: 0.8690\n",
            "Epoch: 0054 loss_train: 0.1784 acc_train: 0.9545 loss_val: 0.3833 acc_val: 0.8653\n",
            "Epoch: 0055 loss_train: 0.1616 acc_train: 0.9649 loss_val: 0.3886 acc_val: 0.8653\n",
            "Epoch: 0056 loss_train: 0.1501 acc_train: 0.9643 loss_val: 0.3948 acc_val: 0.8690\n",
            "Epoch: 0057 loss_train: 0.1370 acc_train: 0.9711 loss_val: 0.3991 acc_val: 0.8690\n",
            "Epoch: 0058 loss_train: 0.1295 acc_train: 0.9717 loss_val: 0.4048 acc_val: 0.8708\n",
            "Epoch: 0059 loss_train: 0.1199 acc_train: 0.9736 loss_val: 0.4150 acc_val: 0.8708\n",
            "Epoch: 0060 loss_train: 0.1079 acc_train: 0.9779 loss_val: 0.4294 acc_val: 0.8727\n",
            "Epoch: 0061 loss_train: 0.0990 acc_train: 0.9822 loss_val: 0.4396 acc_val: 0.8764\n",
            "Epoch: 0062 loss_train: 0.0903 acc_train: 0.9834 loss_val: 0.4450 acc_val: 0.8745\n",
            "Epoch: 0063 loss_train: 0.0808 acc_train: 0.9859 loss_val: 0.4534 acc_val: 0.8708\n",
            "Epoch: 0064 loss_train: 0.0720 acc_train: 0.9865 loss_val: 0.4658 acc_val: 0.8672\n",
            "Epoch: 0065 loss_train: 0.0655 acc_train: 0.9871 loss_val: 0.4780 acc_val: 0.8635\n",
            "Epoch: 0066 loss_train: 0.0612 acc_train: 0.9889 loss_val: 0.4867 acc_val: 0.8635\n",
            "Epoch: 0067 loss_train: 0.0548 acc_train: 0.9908 loss_val: 0.4925 acc_val: 0.8672\n",
            "Epoch: 0068 loss_train: 0.0485 acc_train: 0.9914 loss_val: 0.4987 acc_val: 0.8690\n",
            "Epoch: 0069 loss_train: 0.0457 acc_train: 0.9920 loss_val: 0.5062 acc_val: 0.8672\n",
            "Epoch: 0070 loss_train: 0.0425 acc_train: 0.9932 loss_val: 0.5170 acc_val: 0.8653\n",
            "Epoch: 0071 loss_train: 0.0372 acc_train: 0.9938 loss_val: 0.5267 acc_val: 0.8653\n",
            "Epoch: 0072 loss_train: 0.0339 acc_train: 0.9957 loss_val: 0.5326 acc_val: 0.8653\n",
            "Epoch: 0073 loss_train: 0.0315 acc_train: 0.9975 loss_val: 0.5329 acc_val: 0.8690\n",
            "Epoch: 0074 loss_train: 0.0287 acc_train: 0.9975 loss_val: 0.5345 acc_val: 0.8690\n",
            "Epoch: 0075 loss_train: 0.0265 acc_train: 0.9975 loss_val: 0.5412 acc_val: 0.8690\n",
            "Epoch: 0076 loss_train: 0.0248 acc_train: 0.9969 loss_val: 0.5533 acc_val: 0.8653\n",
            "Epoch: 0077 loss_train: 0.0217 acc_train: 0.9969 loss_val: 0.5635 acc_val: 0.8635\n",
            "Epoch: 0078 loss_train: 0.0199 acc_train: 0.9975 loss_val: 0.5671 acc_val: 0.8635\n",
            "Epoch: 0079 loss_train: 0.0176 acc_train: 0.9988 loss_val: 0.5704 acc_val: 0.8672\n",
            "Epoch: 0080 loss_train: 0.0164 acc_train: 0.9988 loss_val: 0.5744 acc_val: 0.8708\n",
            "Epoch: 0081 loss_train: 0.0147 acc_train: 0.9988 loss_val: 0.5810 acc_val: 0.8690\n",
            "Epoch: 0082 loss_train: 0.0132 acc_train: 0.9994 loss_val: 0.5903 acc_val: 0.8672\n",
            "Epoch: 0083 loss_train: 0.0121 acc_train: 0.9988 loss_val: 0.6009 acc_val: 0.8672\n",
            "Epoch: 0084 loss_train: 0.0102 acc_train: 0.9994 loss_val: 0.6117 acc_val: 0.8672\n",
            "Epoch: 0085 loss_train: 0.0091 acc_train: 1.0000 loss_val: 0.6216 acc_val: 0.8672\n",
            "Epoch: 0086 loss_train: 0.0076 acc_train: 1.0000 loss_val: 0.6306 acc_val: 0.8653\n",
            "Epoch: 0087 loss_train: 0.0071 acc_train: 1.0000 loss_val: 0.6381 acc_val: 0.8635\n",
            "Epoch: 0088 loss_train: 0.0065 acc_train: 1.0000 loss_val: 0.6416 acc_val: 0.8635\n",
            "Epoch: 0089 loss_train: 0.0061 acc_train: 1.0000 loss_val: 0.6433 acc_val: 0.8635\n",
            "Epoch: 0090 loss_train: 0.0057 acc_train: 1.0000 loss_val: 0.6442 acc_val: 0.8635\n",
            "Epoch: 0091 loss_train: 0.0052 acc_train: 1.0000 loss_val: 0.6462 acc_val: 0.8690\n",
            "Epoch: 0092 loss_train: 0.0048 acc_train: 1.0000 loss_val: 0.6500 acc_val: 0.8708\n",
            "Epoch: 0093 loss_train: 0.0044 acc_train: 1.0000 loss_val: 0.6559 acc_val: 0.8653\n",
            "Epoch: 0094 loss_train: 0.0046 acc_train: 1.0000 loss_val: 0.6639 acc_val: 0.8635\n",
            "Epoch: 0095 loss_train: 0.0040 acc_train: 1.0000 loss_val: 0.6733 acc_val: 0.8653\n",
            "Epoch: 0096 loss_train: 0.0037 acc_train: 1.0000 loss_val: 0.6816 acc_val: 0.8653\n",
            "Epoch: 0097 loss_train: 0.0036 acc_train: 1.0000 loss_val: 0.6868 acc_val: 0.8653\n",
            "Epoch: 0098 loss_train: 0.0033 acc_train: 1.0000 loss_val: 0.6896 acc_val: 0.8653\n",
            "Epoch: 0099 loss_train: 0.0031 acc_train: 1.0000 loss_val: 0.6902 acc_val: 0.8653\n",
            "Epoch: 0100 loss_train: 0.0030 acc_train: 1.0000 loss_val: 0.6893 acc_val: 0.8672\n",
            "Epoch: 0101 loss_train: 0.0028 acc_train: 1.0000 loss_val: 0.6877 acc_val: 0.8653\n",
            "Epoch: 0102 loss_train: 0.0027 acc_train: 1.0000 loss_val: 0.6872 acc_val: 0.8653\n",
            "Epoch: 0103 loss_train: 0.0027 acc_train: 1.0000 loss_val: 0.6883 acc_val: 0.8653\n",
            "Epoch: 0104 loss_train: 0.0025 acc_train: 1.0000 loss_val: 0.6909 acc_val: 0.8653\n",
            "Epoch: 0105 loss_train: 0.0023 acc_train: 1.0000 loss_val: 0.6950 acc_val: 0.8635\n",
            "Epoch: 0106 loss_train: 0.0022 acc_train: 1.0000 loss_val: 0.7001 acc_val: 0.8616\n",
            "Epoch: 0107 loss_train: 0.0022 acc_train: 1.0000 loss_val: 0.7055 acc_val: 0.8635\n",
            "Epoch: 0108 loss_train: 0.0021 acc_train: 1.0000 loss_val: 0.7103 acc_val: 0.8616\n",
            "Epoch: 0109 loss_train: 0.0020 acc_train: 1.0000 loss_val: 0.7145 acc_val: 0.8616\n",
            "Epoch: 0110 loss_train: 0.0019 acc_train: 1.0000 loss_val: 0.7185 acc_val: 0.8616\n",
            "Epoch: 0111 loss_train: 0.0018 acc_train: 1.0000 loss_val: 0.7212 acc_val: 0.8616\n",
            "Optimization Finished!\n",
            "Train cost: 15.6863s\n",
            "Loading 61th epoch\n",
            "Test set results: loss= 0.3408 accuracy= 0.9037\n",
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:354: DGLWarning: DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\n",
            "  dgl_warning(\"DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\")\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=1448, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=7, bias=True)\n",
            ")\n",
            "total params: 2979849\n",
            "Epoch: 0001 loss_train: 1.9862 acc_train: 0.0843 loss_val: 1.9878 acc_val: 0.0756\n",
            "Epoch: 0002 loss_train: 1.9809 acc_train: 0.0867 loss_val: 1.9766 acc_val: 0.0812\n",
            "Epoch: 0003 loss_train: 1.9720 acc_train: 0.0978 loss_val: 1.9601 acc_val: 0.0904\n",
            "Epoch: 0004 loss_train: 1.9573 acc_train: 0.1002 loss_val: 1.9387 acc_val: 0.1070\n",
            "Epoch: 0005 loss_train: 1.9390 acc_train: 0.1125 loss_val: 1.9133 acc_val: 0.2159\n",
            "Epoch: 0006 loss_train: 1.9183 acc_train: 0.2153 loss_val: 1.8849 acc_val: 0.2804\n",
            "Epoch: 0007 loss_train: 1.8944 acc_train: 0.2835 loss_val: 1.8546 acc_val: 0.2915\n",
            "Epoch: 0008 loss_train: 1.8698 acc_train: 0.2952 loss_val: 1.8240 acc_val: 0.3026\n",
            "Epoch: 0009 loss_train: 1.8439 acc_train: 0.2970 loss_val: 1.7934 acc_val: 0.3026\n",
            "Epoch: 0010 loss_train: 1.8169 acc_train: 0.2983 loss_val: 1.7635 acc_val: 0.3100\n",
            "Epoch: 0011 loss_train: 1.7934 acc_train: 0.3038 loss_val: 1.7343 acc_val: 0.3118\n",
            "Epoch: 0012 loss_train: 1.7706 acc_train: 0.3069 loss_val: 1.7055 acc_val: 0.3155\n",
            "Epoch: 0013 loss_train: 1.7495 acc_train: 0.3081 loss_val: 1.6758 acc_val: 0.3303\n",
            "Epoch: 0014 loss_train: 1.7282 acc_train: 0.3149 loss_val: 1.6445 acc_val: 0.3672\n",
            "Epoch: 0015 loss_train: 1.7017 acc_train: 0.3303 loss_val: 1.6093 acc_val: 0.3930\n",
            "Epoch: 0016 loss_train: 1.6772 acc_train: 0.3487 loss_val: 1.5687 acc_val: 0.4354\n",
            "Epoch: 0017 loss_train: 1.6457 acc_train: 0.3733 loss_val: 1.5229 acc_val: 0.4668\n",
            "Epoch: 0018 loss_train: 1.6101 acc_train: 0.3918 loss_val: 1.4739 acc_val: 0.5166\n",
            "Epoch: 0019 loss_train: 1.5702 acc_train: 0.4336 loss_val: 1.4234 acc_val: 0.5683\n",
            "Epoch: 0020 loss_train: 1.5273 acc_train: 0.4889 loss_val: 1.3715 acc_val: 0.6162\n",
            "Epoch: 0021 loss_train: 1.4824 acc_train: 0.5369 loss_val: 1.3165 acc_val: 0.6642\n",
            "Epoch: 0022 loss_train: 1.4271 acc_train: 0.5800 loss_val: 1.2573 acc_val: 0.6716\n",
            "Epoch: 0023 loss_train: 1.3701 acc_train: 0.6058 loss_val: 1.1958 acc_val: 0.6771\n",
            "Epoch: 0024 loss_train: 1.3066 acc_train: 0.6132 loss_val: 1.1358 acc_val: 0.6919\n",
            "Epoch: 0025 loss_train: 1.2461 acc_train: 0.6193 loss_val: 1.0800 acc_val: 0.6956\n",
            "Epoch: 0026 loss_train: 1.1849 acc_train: 0.6371 loss_val: 1.0294 acc_val: 0.7066\n",
            "Epoch: 0027 loss_train: 1.1245 acc_train: 0.6667 loss_val: 0.9858 acc_val: 0.7232\n",
            "Epoch: 0028 loss_train: 1.0665 acc_train: 0.7048 loss_val: 0.9475 acc_val: 0.7288\n",
            "Epoch: 0029 loss_train: 1.0137 acc_train: 0.7472 loss_val: 0.9098 acc_val: 0.7269\n",
            "Epoch: 0030 loss_train: 0.9542 acc_train: 0.7595 loss_val: 0.8729 acc_val: 0.7269\n",
            "Epoch: 0031 loss_train: 0.9002 acc_train: 0.7589 loss_val: 0.8374 acc_val: 0.7288\n",
            "Epoch: 0032 loss_train: 0.8458 acc_train: 0.7774 loss_val: 0.8027 acc_val: 0.7399\n",
            "Epoch: 0033 loss_train: 0.7930 acc_train: 0.8026 loss_val: 0.7656 acc_val: 0.7565\n",
            "Epoch: 0034 loss_train: 0.7428 acc_train: 0.8173 loss_val: 0.7250 acc_val: 0.7638\n",
            "Epoch: 0035 loss_train: 0.6959 acc_train: 0.8216 loss_val: 0.6876 acc_val: 0.7823\n",
            "Epoch: 0036 loss_train: 0.6489 acc_train: 0.8370 loss_val: 0.6549 acc_val: 0.7934\n",
            "Epoch: 0037 loss_train: 0.6028 acc_train: 0.8518 loss_val: 0.6251 acc_val: 0.8100\n",
            "Epoch: 0038 loss_train: 0.5669 acc_train: 0.8561 loss_val: 0.5967 acc_val: 0.8173\n",
            "Epoch: 0039 loss_train: 0.5273 acc_train: 0.8678 loss_val: 0.5744 acc_val: 0.8247\n",
            "Epoch: 0040 loss_train: 0.4914 acc_train: 0.8733 loss_val: 0.5547 acc_val: 0.8358\n",
            "Epoch: 0041 loss_train: 0.4594 acc_train: 0.8887 loss_val: 0.5306 acc_val: 0.8339\n",
            "Epoch: 0042 loss_train: 0.4272 acc_train: 0.8911 loss_val: 0.5114 acc_val: 0.8339\n",
            "Epoch: 0043 loss_train: 0.3978 acc_train: 0.8942 loss_val: 0.5006 acc_val: 0.8376\n",
            "Epoch: 0044 loss_train: 0.3715 acc_train: 0.9053 loss_val: 0.4866 acc_val: 0.8358\n",
            "Epoch: 0045 loss_train: 0.3487 acc_train: 0.9102 loss_val: 0.4716 acc_val: 0.8487\n",
            "Epoch: 0046 loss_train: 0.3216 acc_train: 0.9145 loss_val: 0.4599 acc_val: 0.8506\n",
            "Epoch: 0047 loss_train: 0.3016 acc_train: 0.9225 loss_val: 0.4490 acc_val: 0.8524\n",
            "Epoch: 0048 loss_train: 0.2809 acc_train: 0.9225 loss_val: 0.4387 acc_val: 0.8561\n",
            "Epoch: 0049 loss_train: 0.2621 acc_train: 0.9280 loss_val: 0.4311 acc_val: 0.8524\n",
            "Epoch: 0050 loss_train: 0.2457 acc_train: 0.9342 loss_val: 0.4253 acc_val: 0.8524\n",
            "Epoch: 0051 loss_train: 0.2269 acc_train: 0.9397 loss_val: 0.4181 acc_val: 0.8561\n",
            "Epoch: 0052 loss_train: 0.2146 acc_train: 0.9446 loss_val: 0.4080 acc_val: 0.8616\n",
            "Epoch: 0053 loss_train: 0.1989 acc_train: 0.9465 loss_val: 0.4036 acc_val: 0.8598\n",
            "Epoch: 0054 loss_train: 0.1852 acc_train: 0.9496 loss_val: 0.4052 acc_val: 0.8653\n",
            "Epoch: 0055 loss_train: 0.1702 acc_train: 0.9576 loss_val: 0.4081 acc_val: 0.8616\n",
            "Epoch: 0056 loss_train: 0.1588 acc_train: 0.9606 loss_val: 0.4041 acc_val: 0.8579\n",
            "Epoch: 0057 loss_train: 0.1462 acc_train: 0.9637 loss_val: 0.4027 acc_val: 0.8561\n",
            "Epoch: 0058 loss_train: 0.1362 acc_train: 0.9656 loss_val: 0.4085 acc_val: 0.8561\n",
            "Epoch: 0059 loss_train: 0.1255 acc_train: 0.9680 loss_val: 0.4208 acc_val: 0.8598\n",
            "Epoch: 0060 loss_train: 0.1143 acc_train: 0.9723 loss_val: 0.4292 acc_val: 0.8598\n",
            "Epoch: 0061 loss_train: 0.1023 acc_train: 0.9766 loss_val: 0.4353 acc_val: 0.8598\n",
            "Epoch: 0062 loss_train: 0.0940 acc_train: 0.9809 loss_val: 0.4437 acc_val: 0.8616\n",
            "Epoch: 0063 loss_train: 0.0852 acc_train: 0.9822 loss_val: 0.4565 acc_val: 0.8561\n",
            "Epoch: 0064 loss_train: 0.0760 acc_train: 0.9840 loss_val: 0.4718 acc_val: 0.8579\n",
            "Epoch: 0065 loss_train: 0.0696 acc_train: 0.9859 loss_val: 0.4819 acc_val: 0.8579\n",
            "Epoch: 0066 loss_train: 0.0641 acc_train: 0.9871 loss_val: 0.4904 acc_val: 0.8579\n",
            "Epoch: 0067 loss_train: 0.0582 acc_train: 0.9889 loss_val: 0.5018 acc_val: 0.8561\n",
            "Epoch: 0068 loss_train: 0.0508 acc_train: 0.9926 loss_val: 0.5154 acc_val: 0.8524\n",
            "Epoch: 0069 loss_train: 0.0485 acc_train: 0.9908 loss_val: 0.5274 acc_val: 0.8487\n",
            "Epoch: 0070 loss_train: 0.0442 acc_train: 0.9926 loss_val: 0.5352 acc_val: 0.8487\n",
            "Epoch: 0071 loss_train: 0.0380 acc_train: 0.9951 loss_val: 0.5399 acc_val: 0.8524\n",
            "Epoch: 0072 loss_train: 0.0347 acc_train: 0.9957 loss_val: 0.5422 acc_val: 0.8524\n",
            "Epoch: 0073 loss_train: 0.0317 acc_train: 0.9969 loss_val: 0.5442 acc_val: 0.8579\n",
            "Epoch: 0074 loss_train: 0.0281 acc_train: 0.9969 loss_val: 0.5499 acc_val: 0.8561\n",
            "Epoch: 0075 loss_train: 0.0259 acc_train: 0.9969 loss_val: 0.5589 acc_val: 0.8524\n",
            "Epoch: 0076 loss_train: 0.0248 acc_train: 0.9975 loss_val: 0.5690 acc_val: 0.8524\n",
            "Epoch: 0077 loss_train: 0.0217 acc_train: 0.9988 loss_val: 0.5758 acc_val: 0.8524\n",
            "Epoch: 0078 loss_train: 0.0198 acc_train: 0.9988 loss_val: 0.5791 acc_val: 0.8542\n",
            "Epoch: 0079 loss_train: 0.0177 acc_train: 0.9988 loss_val: 0.5840 acc_val: 0.8561\n",
            "Epoch: 0080 loss_train: 0.0170 acc_train: 0.9988 loss_val: 0.5897 acc_val: 0.8561\n",
            "Epoch: 0081 loss_train: 0.0146 acc_train: 0.9994 loss_val: 0.5957 acc_val: 0.8542\n",
            "Epoch: 0082 loss_train: 0.0131 acc_train: 0.9994 loss_val: 0.6027 acc_val: 0.8542\n",
            "Epoch: 0083 loss_train: 0.0118 acc_train: 0.9994 loss_val: 0.6105 acc_val: 0.8524\n",
            "Epoch: 0084 loss_train: 0.0100 acc_train: 0.9994 loss_val: 0.6176 acc_val: 0.8524\n",
            "Epoch: 0085 loss_train: 0.0092 acc_train: 0.9994 loss_val: 0.6244 acc_val: 0.8524\n",
            "Epoch: 0086 loss_train: 0.0079 acc_train: 1.0000 loss_val: 0.6321 acc_val: 0.8506\n",
            "Epoch: 0087 loss_train: 0.0073 acc_train: 0.9994 loss_val: 0.6405 acc_val: 0.8487\n",
            "Epoch: 0088 loss_train: 0.0066 acc_train: 1.0000 loss_val: 0.6470 acc_val: 0.8506\n",
            "Epoch: 0089 loss_train: 0.0059 acc_train: 1.0000 loss_val: 0.6518 acc_val: 0.8506\n",
            "Epoch: 0090 loss_train: 0.0057 acc_train: 1.0000 loss_val: 0.6557 acc_val: 0.8487\n",
            "Epoch: 0091 loss_train: 0.0053 acc_train: 1.0000 loss_val: 0.6584 acc_val: 0.8487\n",
            "Epoch: 0092 loss_train: 0.0048 acc_train: 1.0000 loss_val: 0.6611 acc_val: 0.8469\n",
            "Epoch: 0093 loss_train: 0.0046 acc_train: 1.0000 loss_val: 0.6640 acc_val: 0.8450\n",
            "Epoch: 0094 loss_train: 0.0043 acc_train: 1.0000 loss_val: 0.6678 acc_val: 0.8432\n",
            "Epoch: 0095 loss_train: 0.0042 acc_train: 1.0000 loss_val: 0.6731 acc_val: 0.8432\n",
            "Epoch: 0096 loss_train: 0.0038 acc_train: 1.0000 loss_val: 0.6786 acc_val: 0.8487\n",
            "Epoch: 0097 loss_train: 0.0035 acc_train: 1.0000 loss_val: 0.6838 acc_val: 0.8487\n",
            "Epoch: 0098 loss_train: 0.0034 acc_train: 1.0000 loss_val: 0.6894 acc_val: 0.8469\n",
            "Epoch: 0099 loss_train: 0.0031 acc_train: 1.0000 loss_val: 0.6934 acc_val: 0.8469\n",
            "Epoch: 0100 loss_train: 0.0030 acc_train: 1.0000 loss_val: 0.6962 acc_val: 0.8487\n",
            "Epoch: 0101 loss_train: 0.0028 acc_train: 1.0000 loss_val: 0.6977 acc_val: 0.8506\n",
            "Epoch: 0102 loss_train: 0.0026 acc_train: 1.0000 loss_val: 0.6985 acc_val: 0.8487\n",
            "Epoch: 0103 loss_train: 0.0026 acc_train: 1.0000 loss_val: 0.6999 acc_val: 0.8542\n",
            "Epoch: 0104 loss_train: 0.0025 acc_train: 1.0000 loss_val: 0.7024 acc_val: 0.8542\n",
            "Epoch: 0105 loss_train: 0.0023 acc_train: 1.0000 loss_val: 0.7055 acc_val: 0.8524\n",
            "Epoch: 0106 loss_train: 0.0021 acc_train: 1.0000 loss_val: 0.7095 acc_val: 0.8524\n",
            "Epoch: 0107 loss_train: 0.0021 acc_train: 1.0000 loss_val: 0.7143 acc_val: 0.8469\n",
            "Optimization Finished!\n",
            "Train cost: 14.8151s\n",
            "Loading 54th epoch\n",
            "Test set results: loss= 0.3287 accuracy= 0.8963\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## citeseer"
      ],
      "metadata": {
        "id": "Sj3k7gEfaFIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --dataset citeseer --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 5  --n_heads 8 --n_layers 1 --pe_dim 40 --peak_lr 0.001  --attack none --attperc 20 --TrainTest train --weight_decay=1e-05\n",
        "\n",
        "!python train.py --dataset citeseer --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 5  --n_heads 8 --n_layers 1 --pe_dim 40 --peak_lr 0.001  --attack dice --attperc 10 --TrainTest train --weight_decay=1e-05\n",
        "\n",
        "!python train.py --dataset citeseer --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 5  --n_heads 8 --n_layers 1 --pe_dim 40 --peak_lr 0.001  --attack dice --attperc 40 --TrainTest train --weight_decay=1e-05\n",
        "\n",
        "!python train.py --dataset citeseer --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 5  --n_heads 8 --n_layers 1 --pe_dim 40 --peak_lr 0.001  --attack dice --attperc 60 --TrainTest train --weight_decay=1e-05\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Wvcjj57Zqoj",
        "outputId": "290163ab-be92-435b-8355-4713e9a0b63b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NumNodes: 3327\n",
            "  NumEdges: 9228\n",
            "  NumFeats: 3703\n",
            "  NumClasses: 6\n",
            "  NumTrainingSamples: 120\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=3743, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=6, bias=True)\n",
            ")\n",
            "total params: 4154632\n",
            "Epoch: 0001 loss_train: 1.8342 acc_train: 0.1412 loss_val: 1.8246 acc_val: 0.1757\n",
            "Epoch: 0002 loss_train: 1.8278 acc_train: 0.1527 loss_val: 1.8145 acc_val: 0.1892\n",
            "Epoch: 0003 loss_train: 1.8206 acc_train: 0.1592 loss_val: 1.7996 acc_val: 0.1877\n",
            "Epoch: 0004 loss_train: 1.8050 acc_train: 0.1738 loss_val: 1.7804 acc_val: 0.1982\n",
            "Epoch: 0005 loss_train: 1.7892 acc_train: 0.1948 loss_val: 1.7574 acc_val: 0.2598\n",
            "Epoch: 0006 loss_train: 1.7646 acc_train: 0.2323 loss_val: 1.7310 acc_val: 0.3273\n",
            "Epoch: 0007 loss_train: 1.7381 acc_train: 0.2969 loss_val: 1.7019 acc_val: 0.4054\n",
            "Epoch: 0008 loss_train: 1.7107 acc_train: 0.3796 loss_val: 1.6705 acc_val: 0.4745\n",
            "Epoch: 0009 loss_train: 1.6795 acc_train: 0.4687 loss_val: 1.6371 acc_val: 0.5180\n",
            "Epoch: 0010 loss_train: 1.6443 acc_train: 0.5238 loss_val: 1.6021 acc_val: 0.5450\n",
            "Epoch: 0011 loss_train: 1.6093 acc_train: 0.5633 loss_val: 1.5659 acc_val: 0.5631\n",
            "Epoch: 0012 loss_train: 1.5712 acc_train: 0.5779 loss_val: 1.5283 acc_val: 0.5751\n",
            "Epoch: 0013 loss_train: 1.5338 acc_train: 0.5939 loss_val: 1.4895 acc_val: 0.5856\n",
            "Epoch: 0014 loss_train: 1.4936 acc_train: 0.6134 loss_val: 1.4499 acc_val: 0.6006\n",
            "Epoch: 0015 loss_train: 1.4514 acc_train: 0.6254 loss_val: 1.4097 acc_val: 0.6126\n",
            "Epoch: 0016 loss_train: 1.4086 acc_train: 0.6304 loss_val: 1.3691 acc_val: 0.6201\n",
            "Epoch: 0017 loss_train: 1.3665 acc_train: 0.6485 loss_val: 1.3284 acc_val: 0.6276\n",
            "Epoch: 0018 loss_train: 1.3224 acc_train: 0.6550 loss_val: 1.2880 acc_val: 0.6396\n",
            "Epoch: 0019 loss_train: 1.2805 acc_train: 0.6660 loss_val: 1.2483 acc_val: 0.6456\n",
            "Epoch: 0020 loss_train: 1.2373 acc_train: 0.6730 loss_val: 1.2094 acc_val: 0.6562\n",
            "Epoch: 0021 loss_train: 1.1942 acc_train: 0.6790 loss_val: 1.1717 acc_val: 0.6682\n",
            "Epoch: 0022 loss_train: 1.1514 acc_train: 0.6960 loss_val: 1.1356 acc_val: 0.6892\n",
            "Epoch: 0023 loss_train: 1.1101 acc_train: 0.7031 loss_val: 1.1012 acc_val: 0.6922\n",
            "Epoch: 0024 loss_train: 1.0701 acc_train: 0.7111 loss_val: 1.0689 acc_val: 0.6937\n",
            "Epoch: 0025 loss_train: 1.0313 acc_train: 0.7171 loss_val: 1.0388 acc_val: 0.6982\n",
            "Epoch: 0026 loss_train: 0.9938 acc_train: 0.7266 loss_val: 1.0108 acc_val: 0.7042\n",
            "Epoch: 0027 loss_train: 0.9575 acc_train: 0.7276 loss_val: 0.9850 acc_val: 0.7042\n",
            "Epoch: 0028 loss_train: 0.9233 acc_train: 0.7336 loss_val: 0.9612 acc_val: 0.7132\n",
            "Epoch: 0029 loss_train: 0.8901 acc_train: 0.7391 loss_val: 0.9397 acc_val: 0.7162\n",
            "Epoch: 0030 loss_train: 0.8563 acc_train: 0.7456 loss_val: 0.9205 acc_val: 0.7192\n",
            "Epoch: 0031 loss_train: 0.8228 acc_train: 0.7531 loss_val: 0.9037 acc_val: 0.7207\n",
            "Epoch: 0032 loss_train: 0.7948 acc_train: 0.7616 loss_val: 0.8892 acc_val: 0.7192\n",
            "Epoch: 0033 loss_train: 0.7652 acc_train: 0.7682 loss_val: 0.8765 acc_val: 0.7207\n",
            "Epoch: 0034 loss_train: 0.7386 acc_train: 0.7722 loss_val: 0.8653 acc_val: 0.7207\n",
            "Epoch: 0035 loss_train: 0.7101 acc_train: 0.7777 loss_val: 0.8560 acc_val: 0.7237\n",
            "Epoch: 0036 loss_train: 0.6839 acc_train: 0.7837 loss_val: 0.8485 acc_val: 0.7267\n",
            "Epoch: 0037 loss_train: 0.6595 acc_train: 0.7867 loss_val: 0.8427 acc_val: 0.7237\n",
            "Epoch: 0038 loss_train: 0.6345 acc_train: 0.7972 loss_val: 0.8381 acc_val: 0.7357\n",
            "Epoch: 0039 loss_train: 0.6103 acc_train: 0.8067 loss_val: 0.8349 acc_val: 0.7372\n",
            "Epoch: 0040 loss_train: 0.5898 acc_train: 0.8137 loss_val: 0.8334 acc_val: 0.7462\n",
            "Epoch: 0041 loss_train: 0.5698 acc_train: 0.8167 loss_val: 0.8337 acc_val: 0.7462\n",
            "Epoch: 0042 loss_train: 0.5483 acc_train: 0.8212 loss_val: 0.8358 acc_val: 0.7477\n",
            "Epoch: 0043 loss_train: 0.5265 acc_train: 0.8247 loss_val: 0.8397 acc_val: 0.7508\n",
            "Epoch: 0044 loss_train: 0.5079 acc_train: 0.8292 loss_val: 0.8456 acc_val: 0.7492\n",
            "Epoch: 0045 loss_train: 0.4877 acc_train: 0.8322 loss_val: 0.8525 acc_val: 0.7447\n",
            "Epoch: 0046 loss_train: 0.4702 acc_train: 0.8388 loss_val: 0.8603 acc_val: 0.7477\n",
            "Epoch: 0047 loss_train: 0.4494 acc_train: 0.8478 loss_val: 0.8698 acc_val: 0.7462\n",
            "Epoch: 0048 loss_train: 0.4337 acc_train: 0.8533 loss_val: 0.8809 acc_val: 0.7417\n",
            "Epoch: 0049 loss_train: 0.4156 acc_train: 0.8578 loss_val: 0.8936 acc_val: 0.7402\n",
            "Epoch: 0050 loss_train: 0.3995 acc_train: 0.8653 loss_val: 0.9074 acc_val: 0.7417\n",
            "Epoch: 0051 loss_train: 0.3842 acc_train: 0.8643 loss_val: 0.9222 acc_val: 0.7372\n",
            "Epoch: 0052 loss_train: 0.3657 acc_train: 0.8773 loss_val: 0.9390 acc_val: 0.7357\n",
            "Epoch: 0053 loss_train: 0.3496 acc_train: 0.8808 loss_val: 0.9587 acc_val: 0.7372\n",
            "Epoch: 0054 loss_train: 0.3342 acc_train: 0.8848 loss_val: 0.9803 acc_val: 0.7312\n",
            "Epoch: 0055 loss_train: 0.3183 acc_train: 0.8918 loss_val: 1.0022 acc_val: 0.7282\n",
            "Epoch: 0056 loss_train: 0.2997 acc_train: 0.9004 loss_val: 1.0245 acc_val: 0.7297\n",
            "Epoch: 0057 loss_train: 0.2826 acc_train: 0.8998 loss_val: 1.0471 acc_val: 0.7282\n",
            "Epoch: 0058 loss_train: 0.2705 acc_train: 0.9014 loss_val: 1.0700 acc_val: 0.7312\n",
            "Epoch: 0059 loss_train: 0.2493 acc_train: 0.9119 loss_val: 1.0930 acc_val: 0.7342\n",
            "Epoch: 0060 loss_train: 0.2338 acc_train: 0.9204 loss_val: 1.1163 acc_val: 0.7342\n",
            "Epoch: 0061 loss_train: 0.2192 acc_train: 0.9274 loss_val: 1.1416 acc_val: 0.7357\n",
            "Epoch: 0062 loss_train: 0.2034 acc_train: 0.9359 loss_val: 1.1693 acc_val: 0.7387\n",
            "Epoch: 0063 loss_train: 0.1880 acc_train: 0.9439 loss_val: 1.1976 acc_val: 0.7432\n",
            "Epoch: 0064 loss_train: 0.1752 acc_train: 0.9464 loss_val: 1.2253 acc_val: 0.7387\n",
            "Epoch: 0065 loss_train: 0.1605 acc_train: 0.9549 loss_val: 1.2564 acc_val: 0.7402\n",
            "Epoch: 0066 loss_train: 0.1493 acc_train: 0.9514 loss_val: 1.2885 acc_val: 0.7387\n",
            "Epoch: 0067 loss_train: 0.1369 acc_train: 0.9629 loss_val: 1.3184 acc_val: 0.7402\n",
            "Epoch: 0068 loss_train: 0.1237 acc_train: 0.9619 loss_val: 1.3475 acc_val: 0.7387\n",
            "Epoch: 0069 loss_train: 0.1141 acc_train: 0.9670 loss_val: 1.3774 acc_val: 0.7372\n",
            "Epoch: 0070 loss_train: 0.1046 acc_train: 0.9685 loss_val: 1.4064 acc_val: 0.7372\n",
            "Epoch: 0071 loss_train: 0.0957 acc_train: 0.9710 loss_val: 1.4324 acc_val: 0.7327\n",
            "Epoch: 0072 loss_train: 0.0864 acc_train: 0.9725 loss_val: 1.4580 acc_val: 0.7282\n",
            "Epoch: 0073 loss_train: 0.0794 acc_train: 0.9745 loss_val: 1.4830 acc_val: 0.7297\n",
            "Epoch: 0074 loss_train: 0.0719 acc_train: 0.9775 loss_val: 1.5067 acc_val: 0.7282\n",
            "Epoch: 0075 loss_train: 0.0650 acc_train: 0.9810 loss_val: 1.5305 acc_val: 0.7297\n",
            "Epoch: 0076 loss_train: 0.0593 acc_train: 0.9825 loss_val: 1.5557 acc_val: 0.7282\n",
            "Epoch: 0077 loss_train: 0.0555 acc_train: 0.9840 loss_val: 1.5821 acc_val: 0.7252\n",
            "Epoch: 0078 loss_train: 0.0480 acc_train: 0.9870 loss_val: 1.6092 acc_val: 0.7267\n",
            "Epoch: 0079 loss_train: 0.0456 acc_train: 0.9875 loss_val: 1.6336 acc_val: 0.7282\n",
            "Epoch: 0080 loss_train: 0.0405 acc_train: 0.9875 loss_val: 1.6592 acc_val: 0.7267\n",
            "Epoch: 0081 loss_train: 0.0359 acc_train: 0.9900 loss_val: 1.6857 acc_val: 0.7282\n",
            "Epoch: 0082 loss_train: 0.0342 acc_train: 0.9925 loss_val: 1.7128 acc_val: 0.7267\n",
            "Epoch: 0083 loss_train: 0.0307 acc_train: 0.9915 loss_val: 1.7375 acc_val: 0.7252\n",
            "Epoch: 0084 loss_train: 0.0291 acc_train: 0.9915 loss_val: 1.7580 acc_val: 0.7237\n",
            "Epoch: 0085 loss_train: 0.0253 acc_train: 0.9950 loss_val: 1.7781 acc_val: 0.7252\n",
            "Epoch: 0086 loss_train: 0.0240 acc_train: 0.9915 loss_val: 1.8004 acc_val: 0.7267\n",
            "Epoch: 0087 loss_train: 0.0223 acc_train: 0.9945 loss_val: 1.8231 acc_val: 0.7222\n",
            "Epoch: 0088 loss_train: 0.0203 acc_train: 0.9950 loss_val: 1.8434 acc_val: 0.7237\n",
            "Epoch: 0089 loss_train: 0.0180 acc_train: 0.9960 loss_val: 1.8600 acc_val: 0.7237\n",
            "Epoch: 0090 loss_train: 0.0178 acc_train: 0.9945 loss_val: 1.8726 acc_val: 0.7267\n",
            "Epoch: 0091 loss_train: 0.0173 acc_train: 0.9950 loss_val: 1.8876 acc_val: 0.7297\n",
            "Epoch: 0092 loss_train: 0.0151 acc_train: 0.9950 loss_val: 1.9051 acc_val: 0.7282\n",
            "Epoch: 0093 loss_train: 0.0131 acc_train: 0.9965 loss_val: 1.9223 acc_val: 0.7282\n",
            "Optimization Finished!\n",
            "Train cost: 29.7458s\n",
            "Loading 43th epoch\n",
            "Test set results: loss= 0.7103 accuracy= 0.7831\n",
            "  NumNodes: 3327\n",
            "  NumEdges: 9228\n",
            "  NumFeats: 3703\n",
            "  NumClasses: 6\n",
            "  NumTrainingSamples: 120\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=3743, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=6, bias=True)\n",
            ")\n",
            "total params: 4154632\n",
            "Epoch: 0001 loss_train: 1.8157 acc_train: 0.1447 loss_val: 1.8085 acc_val: 0.1592\n",
            "Epoch: 0002 loss_train: 1.8101 acc_train: 0.1462 loss_val: 1.8011 acc_val: 0.1757\n",
            "Epoch: 0003 loss_train: 1.8038 acc_train: 0.1662 loss_val: 1.7912 acc_val: 0.1862\n",
            "Epoch: 0004 loss_train: 1.7937 acc_train: 0.1758 loss_val: 1.7800 acc_val: 0.1997\n",
            "Epoch: 0005 loss_train: 1.7832 acc_train: 0.1938 loss_val: 1.7691 acc_val: 0.2102\n",
            "Epoch: 0006 loss_train: 1.7738 acc_train: 0.1848 loss_val: 1.7594 acc_val: 0.2342\n",
            "Epoch: 0007 loss_train: 1.7641 acc_train: 0.2153 loss_val: 1.7512 acc_val: 0.2402\n",
            "Epoch: 0008 loss_train: 1.7555 acc_train: 0.2138 loss_val: 1.7450 acc_val: 0.2417\n",
            "Epoch: 0009 loss_train: 1.7495 acc_train: 0.2238 loss_val: 1.7405 acc_val: 0.2492\n",
            "Epoch: 0010 loss_train: 1.7455 acc_train: 0.2328 loss_val: 1.7370 acc_val: 0.2613\n",
            "Epoch: 0011 loss_train: 1.7416 acc_train: 0.2619 loss_val: 1.7338 acc_val: 0.2718\n",
            "Epoch: 0012 loss_train: 1.7377 acc_train: 0.2524 loss_val: 1.7301 acc_val: 0.2868\n",
            "Epoch: 0013 loss_train: 1.7358 acc_train: 0.2639 loss_val: 1.7252 acc_val: 0.3003\n",
            "Epoch: 0014 loss_train: 1.7311 acc_train: 0.2839 loss_val: 1.7188 acc_val: 0.3153\n",
            "Epoch: 0015 loss_train: 1.7261 acc_train: 0.2934 loss_val: 1.7106 acc_val: 0.3273\n",
            "Epoch: 0016 loss_train: 1.7201 acc_train: 0.2999 loss_val: 1.7003 acc_val: 0.3589\n",
            "Epoch: 0017 loss_train: 1.7109 acc_train: 0.3295 loss_val: 1.6870 acc_val: 0.4144\n",
            "Epoch: 0018 loss_train: 1.7021 acc_train: 0.3610 loss_val: 1.6690 acc_val: 0.4685\n",
            "Epoch: 0019 loss_train: 1.6880 acc_train: 0.3976 loss_val: 1.6445 acc_val: 0.4910\n",
            "Epoch: 0020 loss_train: 1.6709 acc_train: 0.4231 loss_val: 1.6114 acc_val: 0.5270\n",
            "Epoch: 0021 loss_train: 1.6469 acc_train: 0.4657 loss_val: 1.5710 acc_val: 0.5871\n",
            "Epoch: 0022 loss_train: 1.6162 acc_train: 0.5148 loss_val: 1.5241 acc_val: 0.6141\n",
            "Epoch: 0023 loss_train: 1.5800 acc_train: 0.5418 loss_val: 1.4696 acc_val: 0.6306\n",
            "Epoch: 0024 loss_train: 1.5404 acc_train: 0.5578 loss_val: 1.4029 acc_val: 0.6592\n",
            "Epoch: 0025 loss_train: 1.4915 acc_train: 0.5824 loss_val: 1.3241 acc_val: 0.6952\n",
            "Epoch: 0026 loss_train: 1.4284 acc_train: 0.6199 loss_val: 1.2453 acc_val: 0.7177\n",
            "Epoch: 0027 loss_train: 1.3643 acc_train: 0.6485 loss_val: 1.1704 acc_val: 0.7312\n",
            "Epoch: 0028 loss_train: 1.3078 acc_train: 0.6680 loss_val: 1.1037 acc_val: 0.7387\n",
            "Epoch: 0029 loss_train: 1.2527 acc_train: 0.6710 loss_val: 1.0398 acc_val: 0.7372\n",
            "Epoch: 0030 loss_train: 1.1985 acc_train: 0.6885 loss_val: 0.9760 acc_val: 0.7553\n",
            "Epoch: 0031 loss_train: 1.1434 acc_train: 0.6980 loss_val: 0.9188 acc_val: 0.7688\n",
            "Epoch: 0032 loss_train: 1.0908 acc_train: 0.7091 loss_val: 0.8638 acc_val: 0.7778\n",
            "Epoch: 0033 loss_train: 1.0419 acc_train: 0.7156 loss_val: 0.8134 acc_val: 0.7808\n",
            "Epoch: 0034 loss_train: 0.9929 acc_train: 0.7221 loss_val: 0.7685 acc_val: 0.7868\n",
            "Epoch: 0035 loss_train: 0.9484 acc_train: 0.7396 loss_val: 0.7283 acc_val: 0.7988\n",
            "Epoch: 0036 loss_train: 0.9014 acc_train: 0.7451 loss_val: 0.6901 acc_val: 0.7988\n",
            "Epoch: 0037 loss_train: 0.8576 acc_train: 0.7551 loss_val: 0.6620 acc_val: 0.8048\n",
            "Epoch: 0038 loss_train: 0.8240 acc_train: 0.7601 loss_val: 0.6313 acc_val: 0.7988\n",
            "Epoch: 0039 loss_train: 0.7829 acc_train: 0.7697 loss_val: 0.5987 acc_val: 0.8153\n",
            "Epoch: 0040 loss_train: 0.7413 acc_train: 0.7822 loss_val: 0.5814 acc_val: 0.8183\n",
            "Epoch: 0041 loss_train: 0.7138 acc_train: 0.7842 loss_val: 0.5620 acc_val: 0.8168\n",
            "Epoch: 0042 loss_train: 0.6798 acc_train: 0.7937 loss_val: 0.5423 acc_val: 0.8213\n",
            "Epoch: 0043 loss_train: 0.6439 acc_train: 0.8027 loss_val: 0.5337 acc_val: 0.8288\n",
            "Epoch: 0044 loss_train: 0.6139 acc_train: 0.8162 loss_val: 0.5188 acc_val: 0.8243\n",
            "Epoch: 0045 loss_train: 0.5817 acc_train: 0.8237 loss_val: 0.5104 acc_val: 0.8258\n",
            "Epoch: 0046 loss_train: 0.5531 acc_train: 0.8338 loss_val: 0.5055 acc_val: 0.8393\n",
            "Epoch: 0047 loss_train: 0.5261 acc_train: 0.8403 loss_val: 0.4992 acc_val: 0.8363\n",
            "Epoch: 0048 loss_train: 0.4985 acc_train: 0.8483 loss_val: 0.4991 acc_val: 0.8288\n",
            "Epoch: 0049 loss_train: 0.4722 acc_train: 0.8568 loss_val: 0.4965 acc_val: 0.8318\n",
            "Epoch: 0050 loss_train: 0.4464 acc_train: 0.8678 loss_val: 0.4985 acc_val: 0.8258\n",
            "Epoch: 0051 loss_train: 0.4204 acc_train: 0.8718 loss_val: 0.4997 acc_val: 0.8213\n",
            "Epoch: 0052 loss_train: 0.3957 acc_train: 0.8788 loss_val: 0.5040 acc_val: 0.8303\n",
            "Epoch: 0053 loss_train: 0.3762 acc_train: 0.8843 loss_val: 0.5092 acc_val: 0.8288\n",
            "Epoch: 0054 loss_train: 0.3555 acc_train: 0.8888 loss_val: 0.5120 acc_val: 0.8318\n",
            "Epoch: 0055 loss_train: 0.3369 acc_train: 0.8923 loss_val: 0.5198 acc_val: 0.8303\n",
            "Epoch: 0056 loss_train: 0.3183 acc_train: 0.8978 loss_val: 0.5223 acc_val: 0.8348\n",
            "Epoch: 0057 loss_train: 0.3004 acc_train: 0.9014 loss_val: 0.5315 acc_val: 0.8303\n",
            "Epoch: 0058 loss_train: 0.2869 acc_train: 0.9049 loss_val: 0.5378 acc_val: 0.8348\n",
            "Epoch: 0059 loss_train: 0.2719 acc_train: 0.9079 loss_val: 0.5450 acc_val: 0.8393\n",
            "Epoch: 0060 loss_train: 0.2589 acc_train: 0.9094 loss_val: 0.5536 acc_val: 0.8393\n",
            "Epoch: 0061 loss_train: 0.2483 acc_train: 0.9139 loss_val: 0.5578 acc_val: 0.8363\n",
            "Epoch: 0062 loss_train: 0.2384 acc_train: 0.9159 loss_val: 0.5672 acc_val: 0.8393\n",
            "Epoch: 0063 loss_train: 0.2296 acc_train: 0.9149 loss_val: 0.5828 acc_val: 0.8393\n",
            "Epoch: 0064 loss_train: 0.2199 acc_train: 0.9194 loss_val: 0.5966 acc_val: 0.8408\n",
            "Epoch: 0065 loss_train: 0.2146 acc_train: 0.9199 loss_val: 0.6032 acc_val: 0.8333\n",
            "Epoch: 0066 loss_train: 0.2059 acc_train: 0.9214 loss_val: 0.6145 acc_val: 0.8288\n",
            "Epoch: 0067 loss_train: 0.1995 acc_train: 0.9239 loss_val: 0.6282 acc_val: 0.8243\n",
            "Epoch: 0068 loss_train: 0.1942 acc_train: 0.9259 loss_val: 0.6457 acc_val: 0.8168\n",
            "Epoch: 0069 loss_train: 0.1877 acc_train: 0.9254 loss_val: 0.6671 acc_val: 0.8108\n",
            "Epoch: 0070 loss_train: 0.1818 acc_train: 0.9309 loss_val: 0.6845 acc_val: 0.8093\n",
            "Epoch: 0071 loss_train: 0.1772 acc_train: 0.9324 loss_val: 0.6997 acc_val: 0.8108\n",
            "Epoch: 0072 loss_train: 0.1725 acc_train: 0.9344 loss_val: 0.7167 acc_val: 0.8123\n",
            "Epoch: 0073 loss_train: 0.1681 acc_train: 0.9374 loss_val: 0.7350 acc_val: 0.8048\n",
            "Epoch: 0074 loss_train: 0.1636 acc_train: 0.9404 loss_val: 0.7544 acc_val: 0.8003\n",
            "Epoch: 0075 loss_train: 0.1590 acc_train: 0.9404 loss_val: 0.7701 acc_val: 0.8018\n",
            "Epoch: 0076 loss_train: 0.1556 acc_train: 0.9429 loss_val: 0.7859 acc_val: 0.8003\n",
            "Epoch: 0077 loss_train: 0.1500 acc_train: 0.9429 loss_val: 0.8054 acc_val: 0.7973\n",
            "Epoch: 0078 loss_train: 0.1459 acc_train: 0.9464 loss_val: 0.8264 acc_val: 0.7988\n",
            "Epoch: 0079 loss_train: 0.1422 acc_train: 0.9474 loss_val: 0.8462 acc_val: 0.7958\n",
            "Epoch: 0080 loss_train: 0.1375 acc_train: 0.9529 loss_val: 0.8603 acc_val: 0.7898\n",
            "Epoch: 0081 loss_train: 0.1331 acc_train: 0.9554 loss_val: 0.8695 acc_val: 0.7958\n",
            "Epoch: 0082 loss_train: 0.1295 acc_train: 0.9624 loss_val: 0.8812 acc_val: 0.7928\n",
            "Epoch: 0083 loss_train: 0.1242 acc_train: 0.9644 loss_val: 0.8963 acc_val: 0.7913\n",
            "Epoch: 0084 loss_train: 0.1193 acc_train: 0.9735 loss_val: 0.9109 acc_val: 0.7913\n",
            "Epoch: 0085 loss_train: 0.1147 acc_train: 0.9740 loss_val: 0.9326 acc_val: 0.7823\n",
            "Epoch: 0086 loss_train: 0.1107 acc_train: 0.9750 loss_val: 0.9557 acc_val: 0.7763\n",
            "Epoch: 0087 loss_train: 0.1050 acc_train: 0.9815 loss_val: 0.9723 acc_val: 0.7763\n",
            "Epoch: 0088 loss_train: 0.1004 acc_train: 0.9825 loss_val: 0.9854 acc_val: 0.7718\n",
            "Epoch: 0089 loss_train: 0.0951 acc_train: 0.9880 loss_val: 1.0011 acc_val: 0.7703\n",
            "Epoch: 0090 loss_train: 0.0904 acc_train: 0.9915 loss_val: 1.0289 acc_val: 0.7628\n",
            "Epoch: 0091 loss_train: 0.0842 acc_train: 0.9935 loss_val: 1.0540 acc_val: 0.7613\n",
            "Epoch: 0092 loss_train: 0.0796 acc_train: 0.9950 loss_val: 1.0592 acc_val: 0.7598\n",
            "Epoch: 0093 loss_train: 0.0750 acc_train: 0.9965 loss_val: 1.0623 acc_val: 0.7583\n",
            "Epoch: 0094 loss_train: 0.0709 acc_train: 0.9965 loss_val: 1.0785 acc_val: 0.7568\n",
            "Epoch: 0095 loss_train: 0.0653 acc_train: 0.9975 loss_val: 1.1052 acc_val: 0.7583\n",
            "Epoch: 0096 loss_train: 0.0608 acc_train: 0.9980 loss_val: 1.1165 acc_val: 0.7568\n",
            "Epoch: 0097 loss_train: 0.0564 acc_train: 0.9985 loss_val: 1.1150 acc_val: 0.7583\n",
            "Epoch: 0098 loss_train: 0.0519 acc_train: 0.9995 loss_val: 1.1194 acc_val: 0.7658\n",
            "Epoch: 0099 loss_train: 0.0476 acc_train: 1.0000 loss_val: 1.1342 acc_val: 0.7643\n",
            "Epoch: 0100 loss_train: 0.0434 acc_train: 1.0000 loss_val: 1.1510 acc_val: 0.7628\n",
            "Epoch: 0101 loss_train: 0.0397 acc_train: 0.9995 loss_val: 1.1612 acc_val: 0.7643\n",
            "Epoch: 0102 loss_train: 0.0363 acc_train: 1.0000 loss_val: 1.1671 acc_val: 0.7568\n",
            "Epoch: 0103 loss_train: 0.0327 acc_train: 1.0000 loss_val: 1.1742 acc_val: 0.7553\n",
            "Epoch: 0104 loss_train: 0.0299 acc_train: 1.0000 loss_val: 1.1821 acc_val: 0.7583\n",
            "Epoch: 0105 loss_train: 0.0274 acc_train: 1.0000 loss_val: 1.1919 acc_val: 0.7613\n",
            "Epoch: 0106 loss_train: 0.0245 acc_train: 1.0000 loss_val: 1.2015 acc_val: 0.7583\n",
            "Epoch: 0107 loss_train: 0.0224 acc_train: 1.0000 loss_val: 1.2118 acc_val: 0.7583\n",
            "Epoch: 0108 loss_train: 0.0201 acc_train: 1.0000 loss_val: 1.2222 acc_val: 0.7583\n",
            "Epoch: 0109 loss_train: 0.0182 acc_train: 1.0000 loss_val: 1.2300 acc_val: 0.7583\n",
            "Epoch: 0110 loss_train: 0.0168 acc_train: 1.0000 loss_val: 1.2363 acc_val: 0.7643\n",
            "Epoch: 0111 loss_train: 0.0147 acc_train: 1.0000 loss_val: 1.2425 acc_val: 0.7658\n",
            "Epoch: 0112 loss_train: 0.0136 acc_train: 1.0000 loss_val: 1.2504 acc_val: 0.7688\n",
            "Epoch: 0113 loss_train: 0.0119 acc_train: 1.0000 loss_val: 1.2604 acc_val: 0.7673\n",
            "Epoch: 0114 loss_train: 0.0113 acc_train: 1.0000 loss_val: 1.2741 acc_val: 0.7643\n",
            "Optimization Finished!\n",
            "Train cost: 36.1163s\n",
            "Loading 64th epoch\n",
            "Test set results: loss= 0.5337 accuracy= 0.8313\n",
            "  NumNodes: 3327\n",
            "  NumEdges: 9228\n",
            "  NumFeats: 3703\n",
            "  NumClasses: 6\n",
            "  NumTrainingSamples: 120\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=3743, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=6, bias=True)\n",
            ")\n",
            "total params: 4154632\n",
            "Epoch: 0001 loss_train: 1.8142 acc_train: 0.1432 loss_val: 1.8067 acc_val: 0.1607\n",
            "Epoch: 0002 loss_train: 1.8092 acc_train: 0.1452 loss_val: 1.7990 acc_val: 0.1772\n",
            "Epoch: 0003 loss_train: 1.8029 acc_train: 0.1602 loss_val: 1.7888 acc_val: 0.1772\n",
            "Epoch: 0004 loss_train: 1.7929 acc_train: 0.1678 loss_val: 1.7770 acc_val: 0.2057\n",
            "Epoch: 0005 loss_train: 1.7819 acc_train: 0.1883 loss_val: 1.7656 acc_val: 0.2312\n",
            "Epoch: 0006 loss_train: 1.7730 acc_train: 0.1773 loss_val: 1.7559 acc_val: 0.2147\n",
            "Epoch: 0007 loss_train: 1.7637 acc_train: 0.2068 loss_val: 1.7484 acc_val: 0.2147\n",
            "Epoch: 0008 loss_train: 1.7555 acc_train: 0.2113 loss_val: 1.7429 acc_val: 0.2192\n",
            "Epoch: 0009 loss_train: 1.7512 acc_train: 0.2203 loss_val: 1.7392 acc_val: 0.2162\n",
            "Epoch: 0010 loss_train: 1.7476 acc_train: 0.2273 loss_val: 1.7365 acc_val: 0.2132\n",
            "Epoch: 0011 loss_train: 1.7455 acc_train: 0.2429 loss_val: 1.7341 acc_val: 0.2102\n",
            "Epoch: 0012 loss_train: 1.7424 acc_train: 0.2404 loss_val: 1.7303 acc_val: 0.2102\n",
            "Epoch: 0013 loss_train: 1.7416 acc_train: 0.2399 loss_val: 1.7245 acc_val: 0.2117\n",
            "Epoch: 0014 loss_train: 1.7386 acc_train: 0.2559 loss_val: 1.7173 acc_val: 0.2252\n",
            "Epoch: 0015 loss_train: 1.7343 acc_train: 0.2649 loss_val: 1.7082 acc_val: 0.2763\n",
            "Epoch: 0016 loss_train: 1.7303 acc_train: 0.2844 loss_val: 1.6966 acc_val: 0.3514\n",
            "Epoch: 0017 loss_train: 1.7233 acc_train: 0.3170 loss_val: 1.6810 acc_val: 0.5195\n",
            "Epoch: 0018 loss_train: 1.7146 acc_train: 0.3831 loss_val: 1.6589 acc_val: 0.6622\n",
            "Epoch: 0019 loss_train: 1.7010 acc_train: 0.4422 loss_val: 1.6300 acc_val: 0.7177\n",
            "Epoch: 0020 loss_train: 1.6860 acc_train: 0.4822 loss_val: 1.5973 acc_val: 0.7102\n",
            "Epoch: 0021 loss_train: 1.6668 acc_train: 0.4972 loss_val: 1.5583 acc_val: 0.7087\n",
            "Epoch: 0022 loss_train: 1.6419 acc_train: 0.5063 loss_val: 1.5124 acc_val: 0.7222\n",
            "Epoch: 0023 loss_train: 1.6152 acc_train: 0.5283 loss_val: 1.4579 acc_val: 0.7583\n",
            "Epoch: 0024 loss_train: 1.5848 acc_train: 0.5689 loss_val: 1.3895 acc_val: 0.8243\n",
            "Epoch: 0025 loss_train: 1.5469 acc_train: 0.6109 loss_val: 1.3024 acc_val: 0.9069\n",
            "Epoch: 0026 loss_train: 1.4948 acc_train: 0.6695 loss_val: 1.1999 acc_val: 0.9189\n",
            "Epoch: 0027 loss_train: 1.4356 acc_train: 0.6690 loss_val: 1.0862 acc_val: 0.9204\n",
            "Epoch: 0028 loss_train: 1.3743 acc_train: 0.6505 loss_val: 0.9676 acc_val: 0.9204\n",
            "Epoch: 0029 loss_train: 1.3123 acc_train: 0.6440 loss_val: 0.8603 acc_val: 0.9204\n",
            "Epoch: 0030 loss_train: 1.2516 acc_train: 0.6500 loss_val: 0.7561 acc_val: 0.9204\n",
            "Epoch: 0031 loss_train: 1.1898 acc_train: 0.6345 loss_val: 0.6565 acc_val: 0.9204\n",
            "Epoch: 0032 loss_train: 1.1319 acc_train: 0.6340 loss_val: 0.5694 acc_val: 0.9204\n",
            "Epoch: 0033 loss_train: 1.0754 acc_train: 0.6485 loss_val: 0.4905 acc_val: 0.9204\n",
            "Epoch: 0034 loss_train: 1.0229 acc_train: 0.6420 loss_val: 0.4266 acc_val: 0.9219\n",
            "Epoch: 0035 loss_train: 0.9783 acc_train: 0.6390 loss_val: 0.3772 acc_val: 0.9249\n",
            "Epoch: 0036 loss_train: 0.9472 acc_train: 0.6545 loss_val: 0.3700 acc_val: 0.9234\n",
            "Epoch: 0037 loss_train: 0.9254 acc_train: 0.6530 loss_val: 0.3194 acc_val: 0.9384\n",
            "Epoch: 0038 loss_train: 0.9009 acc_train: 0.6445 loss_val: 0.2832 acc_val: 0.9354\n",
            "Epoch: 0039 loss_train: 0.8484 acc_train: 0.6760 loss_val: 0.2913 acc_val: 0.9309\n",
            "Epoch: 0040 loss_train: 0.8415 acc_train: 0.6855 loss_val: 0.2212 acc_val: 0.9745\n",
            "Epoch: 0041 loss_train: 0.7865 acc_train: 0.7046 loss_val: 0.2285 acc_val: 0.9580\n",
            "Epoch: 0042 loss_train: 0.7710 acc_train: 0.7161 loss_val: 0.1775 acc_val: 0.9940\n",
            "Epoch: 0043 loss_train: 0.7074 acc_train: 0.7822 loss_val: 0.1884 acc_val: 0.9685\n",
            "Epoch: 0044 loss_train: 0.7022 acc_train: 0.7361 loss_val: 0.1515 acc_val: 0.9865\n",
            "Epoch: 0045 loss_train: 0.6361 acc_train: 0.8262 loss_val: 0.1646 acc_val: 0.9730\n",
            "Epoch: 0046 loss_train: 0.6302 acc_train: 0.7712 loss_val: 0.1441 acc_val: 0.9925\n",
            "Epoch: 0047 loss_train: 0.5753 acc_train: 0.8523 loss_val: 0.1481 acc_val: 0.9745\n",
            "Epoch: 0048 loss_train: 0.5581 acc_train: 0.8107 loss_val: 0.1206 acc_val: 0.9985\n",
            "Epoch: 0049 loss_train: 0.4849 acc_train: 0.8963 loss_val: 0.1393 acc_val: 0.9910\n",
            "Epoch: 0050 loss_train: 0.5022 acc_train: 0.8768 loss_val: 0.1414 acc_val: 0.9850\n",
            "Epoch: 0051 loss_train: 0.4422 acc_train: 0.8893 loss_val: 0.1502 acc_val: 0.9685\n",
            "Epoch: 0052 loss_train: 0.4417 acc_train: 0.8643 loss_val: 0.1010 acc_val: 1.0000\n",
            "Epoch: 0053 loss_train: 0.3782 acc_train: 0.9329 loss_val: 0.1106 acc_val: 0.9985\n",
            "Epoch: 0054 loss_train: 0.3666 acc_train: 0.9439 loss_val: 0.0873 acc_val: 0.9970\n",
            "Epoch: 0055 loss_train: 0.2979 acc_train: 0.9539 loss_val: 0.0660 acc_val: 1.0000\n",
            "Epoch: 0056 loss_train: 0.2572 acc_train: 0.9644 loss_val: 0.0734 acc_val: 1.0000\n",
            "Epoch: 0057 loss_train: 0.2774 acc_train: 0.9604 loss_val: 0.0473 acc_val: 1.0000\n",
            "Epoch: 0058 loss_train: 0.2045 acc_train: 0.9664 loss_val: 0.0427 acc_val: 0.9985\n",
            "Epoch: 0059 loss_train: 0.1839 acc_train: 0.9700 loss_val: 0.0438 acc_val: 0.9985\n",
            "Epoch: 0060 loss_train: 0.1845 acc_train: 0.9690 loss_val: 0.0414 acc_val: 1.0000\n",
            "Epoch: 0061 loss_train: 0.1761 acc_train: 0.9670 loss_val: 0.0295 acc_val: 1.0000\n",
            "Epoch: 0062 loss_train: 0.1356 acc_train: 0.9725 loss_val: 0.0276 acc_val: 0.9985\n",
            "Epoch: 0063 loss_train: 0.1233 acc_train: 0.9820 loss_val: 0.0274 acc_val: 0.9985\n",
            "Epoch: 0064 loss_train: 0.1187 acc_train: 0.9790 loss_val: 0.0223 acc_val: 1.0000\n",
            "Epoch: 0065 loss_train: 0.0988 acc_train: 0.9820 loss_val: 0.0200 acc_val: 1.0000\n",
            "Epoch: 0066 loss_train: 0.0931 acc_train: 0.9835 loss_val: 0.0162 acc_val: 1.0000\n",
            "Epoch: 0067 loss_train: 0.0733 acc_train: 0.9935 loss_val: 0.0183 acc_val: 0.9985\n",
            "Epoch: 0068 loss_train: 0.0736 acc_train: 0.9935 loss_val: 0.0175 acc_val: 0.9985\n",
            "Epoch: 0069 loss_train: 0.0660 acc_train: 0.9900 loss_val: 0.0123 acc_val: 0.9985\n",
            "Epoch: 0070 loss_train: 0.0510 acc_train: 0.9965 loss_val: 0.0109 acc_val: 1.0000\n",
            "Epoch: 0071 loss_train: 0.0523 acc_train: 0.9960 loss_val: 0.0102 acc_val: 0.9985\n",
            "Epoch: 0072 loss_train: 0.0418 acc_train: 1.0000 loss_val: 0.0114 acc_val: 0.9985\n",
            "Epoch: 0073 loss_train: 0.0402 acc_train: 0.9995 loss_val: 0.0107 acc_val: 0.9985\n",
            "Epoch: 0074 loss_train: 0.0370 acc_train: 0.9980 loss_val: 0.0084 acc_val: 0.9985\n",
            "Epoch: 0075 loss_train: 0.0306 acc_train: 0.9995 loss_val: 0.0067 acc_val: 0.9985\n",
            "Epoch: 0076 loss_train: 0.0263 acc_train: 1.0000 loss_val: 0.0067 acc_val: 0.9985\n",
            "Epoch: 0077 loss_train: 0.0249 acc_train: 1.0000 loss_val: 0.0073 acc_val: 0.9985\n",
            "Epoch: 0078 loss_train: 0.0242 acc_train: 1.0000 loss_val: 0.0074 acc_val: 0.9985\n",
            "Epoch: 0079 loss_train: 0.0222 acc_train: 1.0000 loss_val: 0.0068 acc_val: 0.9985\n",
            "Epoch: 0080 loss_train: 0.0192 acc_train: 1.0000 loss_val: 0.0059 acc_val: 0.9985\n",
            "Epoch: 0081 loss_train: 0.0167 acc_train: 1.0000 loss_val: 0.0052 acc_val: 0.9985\n",
            "Epoch: 0082 loss_train: 0.0153 acc_train: 1.0000 loss_val: 0.0048 acc_val: 0.9985\n",
            "Epoch: 0083 loss_train: 0.0137 acc_train: 1.0000 loss_val: 0.0049 acc_val: 0.9985\n",
            "Epoch: 0084 loss_train: 0.0130 acc_train: 1.0000 loss_val: 0.0052 acc_val: 0.9985\n",
            "Epoch: 0085 loss_train: 0.0127 acc_train: 1.0000 loss_val: 0.0053 acc_val: 0.9985\n",
            "Epoch: 0086 loss_train: 0.0113 acc_train: 1.0000 loss_val: 0.0052 acc_val: 0.9985\n",
            "Epoch: 0087 loss_train: 0.0106 acc_train: 1.0000 loss_val: 0.0048 acc_val: 0.9985\n",
            "Epoch: 0088 loss_train: 0.0095 acc_train: 1.0000 loss_val: 0.0044 acc_val: 0.9985\n",
            "Epoch: 0089 loss_train: 0.0085 acc_train: 1.0000 loss_val: 0.0043 acc_val: 0.9985\n",
            "Epoch: 0090 loss_train: 0.0082 acc_train: 1.0000 loss_val: 0.0046 acc_val: 0.9985\n",
            "Epoch: 0091 loss_train: 0.0080 acc_train: 1.0000 loss_val: 0.0050 acc_val: 0.9985\n",
            "Epoch: 0092 loss_train: 0.0073 acc_train: 1.0000 loss_val: 0.0052 acc_val: 0.9985\n",
            "Epoch: 0093 loss_train: 0.0071 acc_train: 1.0000 loss_val: 0.0052 acc_val: 0.9985\n",
            "Epoch: 0094 loss_train: 0.0066 acc_train: 1.0000 loss_val: 0.0049 acc_val: 0.9985\n",
            "Epoch: 0095 loss_train: 0.0058 acc_train: 1.0000 loss_val: 0.0045 acc_val: 0.9985\n",
            "Epoch: 0096 loss_train: 0.0055 acc_train: 1.0000 loss_val: 0.0042 acc_val: 0.9985\n",
            "Epoch: 0097 loss_train: 0.0052 acc_train: 1.0000 loss_val: 0.0041 acc_val: 0.9985\n",
            "Epoch: 0098 loss_train: 0.0049 acc_train: 1.0000 loss_val: 0.0042 acc_val: 0.9985\n",
            "Epoch: 0099 loss_train: 0.0047 acc_train: 1.0000 loss_val: 0.0044 acc_val: 0.9985\n",
            "Epoch: 0100 loss_train: 0.0046 acc_train: 1.0000 loss_val: 0.0047 acc_val: 0.9985\n",
            "Epoch: 0101 loss_train: 0.0045 acc_train: 1.0000 loss_val: 0.0048 acc_val: 0.9985\n",
            "Epoch: 0102 loss_train: 0.0041 acc_train: 1.0000 loss_val: 0.0049 acc_val: 0.9985\n",
            "Epoch: 0103 loss_train: 0.0039 acc_train: 1.0000 loss_val: 0.0049 acc_val: 0.9985\n",
            "Epoch: 0104 loss_train: 0.0036 acc_train: 1.0000 loss_val: 0.0047 acc_val: 0.9985\n",
            "Epoch: 0105 loss_train: 0.0034 acc_train: 1.0000 loss_val: 0.0046 acc_val: 0.9985\n",
            "Epoch: 0106 loss_train: 0.0033 acc_train: 1.0000 loss_val: 0.0045 acc_val: 0.9985\n",
            "Epoch: 0107 loss_train: 0.0032 acc_train: 1.0000 loss_val: 0.0044 acc_val: 0.9985\n",
            "Epoch: 0108 loss_train: 0.0031 acc_train: 1.0000 loss_val: 0.0044 acc_val: 0.9985\n",
            "Epoch: 0109 loss_train: 0.0029 acc_train: 1.0000 loss_val: 0.0044 acc_val: 0.9985\n",
            "Epoch: 0110 loss_train: 0.0029 acc_train: 1.0000 loss_val: 0.0045 acc_val: 0.9985\n",
            "Epoch: 0111 loss_train: 0.0027 acc_train: 1.0000 loss_val: 0.0046 acc_val: 0.9985\n",
            "Epoch: 0112 loss_train: 0.0026 acc_train: 1.0000 loss_val: 0.0047 acc_val: 0.9985\n",
            "Epoch: 0113 loss_train: 0.0024 acc_train: 1.0000 loss_val: 0.0049 acc_val: 0.9985\n",
            "Epoch: 0114 loss_train: 0.0024 acc_train: 1.0000 loss_val: 0.0049 acc_val: 0.9985\n",
            "Epoch: 0115 loss_train: 0.0024 acc_train: 1.0000 loss_val: 0.0050 acc_val: 0.9985\n",
            "Epoch: 0116 loss_train: 0.0022 acc_train: 1.0000 loss_val: 0.0050 acc_val: 0.9985\n",
            "Epoch: 0117 loss_train: 0.0021 acc_train: 1.0000 loss_val: 0.0049 acc_val: 0.9985\n",
            "Epoch: 0118 loss_train: 0.0021 acc_train: 1.0000 loss_val: 0.0049 acc_val: 0.9985\n",
            "Epoch: 0119 loss_train: 0.0020 acc_train: 1.0000 loss_val: 0.0048 acc_val: 0.9985\n",
            "Epoch: 0120 loss_train: 0.0020 acc_train: 1.0000 loss_val: 0.0047 acc_val: 0.9985\n",
            "Epoch: 0121 loss_train: 0.0019 acc_train: 1.0000 loss_val: 0.0047 acc_val: 0.9985\n",
            "Epoch: 0122 loss_train: 0.0019 acc_train: 1.0000 loss_val: 0.0047 acc_val: 0.9985\n",
            "Epoch: 0123 loss_train: 0.0018 acc_train: 1.0000 loss_val: 0.0048 acc_val: 0.9985\n",
            "Epoch: 0124 loss_train: 0.0017 acc_train: 1.0000 loss_val: 0.0048 acc_val: 0.9985\n",
            "Epoch: 0125 loss_train: 0.0017 acc_train: 1.0000 loss_val: 0.0049 acc_val: 0.9985\n",
            "Epoch: 0126 loss_train: 0.0016 acc_train: 1.0000 loss_val: 0.0050 acc_val: 0.9985\n",
            "Epoch: 0127 loss_train: 0.0016 acc_train: 1.0000 loss_val: 0.0051 acc_val: 0.9985\n",
            "Epoch: 0128 loss_train: 0.0015 acc_train: 1.0000 loss_val: 0.0052 acc_val: 0.9985\n",
            "Epoch: 0129 loss_train: 0.0015 acc_train: 1.0000 loss_val: 0.0052 acc_val: 0.9985\n",
            "Epoch: 0130 loss_train: 0.0015 acc_train: 1.0000 loss_val: 0.0052 acc_val: 0.9985\n",
            "Epoch: 0131 loss_train: 0.0015 acc_train: 1.0000 loss_val: 0.0052 acc_val: 0.9985\n",
            "Epoch: 0132 loss_train: 0.0014 acc_train: 1.0000 loss_val: 0.0052 acc_val: 0.9985\n",
            "Epoch: 0133 loss_train: 0.0014 acc_train: 1.0000 loss_val: 0.0052 acc_val: 0.9985\n",
            "Epoch: 0134 loss_train: 0.0013 acc_train: 1.0000 loss_val: 0.0052 acc_val: 0.9985\n",
            "Epoch: 0135 loss_train: 0.0013 acc_train: 1.0000 loss_val: 0.0052 acc_val: 0.9985\n",
            "Epoch: 0136 loss_train: 0.0013 acc_train: 1.0000 loss_val: 0.0052 acc_val: 0.9985\n",
            "Epoch: 0137 loss_train: 0.0012 acc_train: 1.0000 loss_val: 0.0052 acc_val: 0.9985\n",
            "Epoch: 0138 loss_train: 0.0012 acc_train: 1.0000 loss_val: 0.0052 acc_val: 0.9985\n",
            "Epoch: 0139 loss_train: 0.0012 acc_train: 1.0000 loss_val: 0.0053 acc_val: 0.9985\n",
            "Epoch: 0140 loss_train: 0.0011 acc_train: 1.0000 loss_val: 0.0053 acc_val: 0.9985\n",
            "Epoch: 0141 loss_train: 0.0011 acc_train: 1.0000 loss_val: 0.0053 acc_val: 0.9985\n",
            "Epoch: 0142 loss_train: 0.0011 acc_train: 1.0000 loss_val: 0.0053 acc_val: 0.9985\n",
            "Epoch: 0143 loss_train: 0.0011 acc_train: 1.0000 loss_val: 0.0053 acc_val: 0.9985\n",
            "Epoch: 0144 loss_train: 0.0011 acc_train: 1.0000 loss_val: 0.0054 acc_val: 0.9985\n",
            "Epoch: 0145 loss_train: 0.0010 acc_train: 1.0000 loss_val: 0.0054 acc_val: 0.9985\n",
            "Epoch: 0146 loss_train: 0.0010 acc_train: 1.0000 loss_val: 0.0055 acc_val: 0.9985\n",
            "Epoch: 0147 loss_train: 0.0010 acc_train: 1.0000 loss_val: 0.0055 acc_val: 0.9985\n",
            "Optimization Finished!\n",
            "Train cost: 46.6129s\n",
            "Loading 70th epoch\n",
            "Test set results: loss= 0.0086 accuracy= 1.0000\n",
            "  NumNodes: 3327\n",
            "  NumEdges: 9228\n",
            "  NumFeats: 3703\n",
            "  NumClasses: 6\n",
            "  NumTrainingSamples: 120\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=3743, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=6, bias=True)\n",
            ")\n",
            "total params: 4154632\n",
            "Epoch: 0001 loss_train: 1.8130 acc_train: 0.1532 loss_val: 1.8054 acc_val: 0.1772\n",
            "Epoch: 0002 loss_train: 1.8079 acc_train: 0.1652 loss_val: 1.7974 acc_val: 0.1772\n",
            "Epoch: 0003 loss_train: 1.8018 acc_train: 0.1723 loss_val: 1.7867 acc_val: 0.1847\n",
            "Epoch: 0004 loss_train: 1.7920 acc_train: 0.1803 loss_val: 1.7750 acc_val: 0.2583\n",
            "Epoch: 0005 loss_train: 1.7812 acc_train: 0.1983 loss_val: 1.7642 acc_val: 0.2673\n",
            "Epoch: 0006 loss_train: 1.7729 acc_train: 0.2028 loss_val: 1.7553 acc_val: 0.2132\n",
            "Epoch: 0007 loss_train: 1.7641 acc_train: 0.2138 loss_val: 1.7480 acc_val: 0.2132\n",
            "Epoch: 0008 loss_train: 1.7562 acc_train: 0.2133 loss_val: 1.7431 acc_val: 0.2102\n",
            "Epoch: 0009 loss_train: 1.7517 acc_train: 0.2143 loss_val: 1.7405 acc_val: 0.2102\n",
            "Epoch: 0010 loss_train: 1.7490 acc_train: 0.2208 loss_val: 1.7391 acc_val: 0.2102\n",
            "Epoch: 0011 loss_train: 1.7475 acc_train: 0.2263 loss_val: 1.7376 acc_val: 0.2102\n",
            "Epoch: 0012 loss_train: 1.7456 acc_train: 0.2344 loss_val: 1.7352 acc_val: 0.2102\n",
            "Epoch: 0013 loss_train: 1.7455 acc_train: 0.2258 loss_val: 1.7313 acc_val: 0.2117\n",
            "Epoch: 0014 loss_train: 1.7438 acc_train: 0.2434 loss_val: 1.7259 acc_val: 0.2177\n",
            "Epoch: 0015 loss_train: 1.7422 acc_train: 0.2464 loss_val: 1.7197 acc_val: 0.2432\n",
            "Epoch: 0016 loss_train: 1.7400 acc_train: 0.2564 loss_val: 1.7121 acc_val: 0.2688\n",
            "Epoch: 0017 loss_train: 1.7361 acc_train: 0.2629 loss_val: 1.7029 acc_val: 0.2778\n",
            "Epoch: 0018 loss_train: 1.7322 acc_train: 0.2839 loss_val: 1.6911 acc_val: 0.2778\n",
            "Epoch: 0019 loss_train: 1.7249 acc_train: 0.2784 loss_val: 1.6758 acc_val: 0.2598\n",
            "Epoch: 0020 loss_train: 1.7192 acc_train: 0.2619 loss_val: 1.6564 acc_val: 0.2477\n",
            "Epoch: 0021 loss_train: 1.7079 acc_train: 0.2544 loss_val: 1.6320 acc_val: 0.3273\n",
            "Epoch: 0022 loss_train: 1.6929 acc_train: 0.2789 loss_val: 1.6023 acc_val: 0.4174\n",
            "Epoch: 0023 loss_train: 1.6787 acc_train: 0.3200 loss_val: 1.5667 acc_val: 0.5631\n",
            "Epoch: 0024 loss_train: 1.6591 acc_train: 0.3976 loss_val: 1.5253 acc_val: 0.6456\n",
            "Epoch: 0025 loss_train: 1.6390 acc_train: 0.4487 loss_val: 1.4788 acc_val: 0.7342\n",
            "Epoch: 0026 loss_train: 1.6152 acc_train: 0.4917 loss_val: 1.4219 acc_val: 0.8033\n",
            "Epoch: 0027 loss_train: 1.5829 acc_train: 0.5343 loss_val: 1.3552 acc_val: 0.8333\n",
            "Epoch: 0028 loss_train: 1.5502 acc_train: 0.5543 loss_val: 1.2731 acc_val: 0.8919\n",
            "Epoch: 0029 loss_train: 1.5056 acc_train: 0.5674 loss_val: 1.1728 acc_val: 0.9159\n",
            "Epoch: 0030 loss_train: 1.4558 acc_train: 0.5874 loss_val: 1.0579 acc_val: 0.9204\n",
            "Epoch: 0031 loss_train: 1.3947 acc_train: 0.6119 loss_val: 0.9372 acc_val: 0.9204\n",
            "Epoch: 0032 loss_train: 1.3326 acc_train: 0.6174 loss_val: 0.8169 acc_val: 0.9204\n",
            "Epoch: 0033 loss_train: 1.2704 acc_train: 0.6104 loss_val: 0.7052 acc_val: 0.9204\n",
            "Epoch: 0034 loss_train: 1.2034 acc_train: 0.6365 loss_val: 0.6113 acc_val: 0.9204\n",
            "Epoch: 0035 loss_train: 1.1390 acc_train: 0.6595 loss_val: 0.5313 acc_val: 0.9204\n",
            "Epoch: 0036 loss_train: 1.0734 acc_train: 0.6890 loss_val: 0.5002 acc_val: 0.9204\n",
            "Epoch: 0037 loss_train: 1.0098 acc_train: 0.7056 loss_val: 0.4545 acc_val: 0.9204\n",
            "Epoch: 0038 loss_train: 0.9556 acc_train: 0.7666 loss_val: 0.3837 acc_val: 0.9204\n",
            "Epoch: 0039 loss_train: 0.8450 acc_train: 0.7767 loss_val: 0.3592 acc_val: 0.9204\n",
            "Epoch: 0040 loss_train: 0.7914 acc_train: 0.7972 loss_val: 0.3853 acc_val: 0.9204\n",
            "Epoch: 0041 loss_train: 0.7651 acc_train: 0.8443 loss_val: 0.2949 acc_val: 0.9204\n",
            "Epoch: 0042 loss_train: 0.6320 acc_train: 0.8678 loss_val: 0.3177 acc_val: 0.9204\n",
            "Epoch: 0043 loss_train: 0.6819 acc_train: 0.7682 loss_val: 0.3013 acc_val: 0.9204\n",
            "Epoch: 0044 loss_train: 0.5675 acc_train: 0.9069 loss_val: 0.3373 acc_val: 0.9204\n",
            "Epoch: 0045 loss_train: 0.6158 acc_train: 0.8132 loss_val: 0.2690 acc_val: 0.9204\n",
            "Epoch: 0046 loss_train: 0.4961 acc_train: 0.8953 loss_val: 0.2749 acc_val: 0.9204\n",
            "Epoch: 0047 loss_train: 0.5208 acc_train: 0.8653 loss_val: 0.2215 acc_val: 0.9414\n",
            "Epoch: 0048 loss_train: 0.4201 acc_train: 0.9084 loss_val: 0.1712 acc_val: 0.9955\n",
            "Epoch: 0049 loss_train: 0.3397 acc_train: 0.9569 loss_val: 0.2041 acc_val: 0.9249\n",
            "Epoch: 0050 loss_train: 0.3962 acc_train: 0.9049 loss_val: 0.1535 acc_val: 0.9970\n",
            "Epoch: 0051 loss_train: 0.3149 acc_train: 0.9594 loss_val: 0.1281 acc_val: 1.0000\n",
            "Epoch: 0052 loss_train: 0.2700 acc_train: 0.9599 loss_val: 0.1127 acc_val: 1.0000\n",
            "Epoch: 0053 loss_train: 0.2396 acc_train: 0.9740 loss_val: 0.1168 acc_val: 1.0000\n",
            "Epoch: 0054 loss_train: 0.2413 acc_train: 0.9654 loss_val: 0.1057 acc_val: 0.9985\n",
            "Epoch: 0055 loss_train: 0.2105 acc_train: 0.9619 loss_val: 0.0759 acc_val: 1.0000\n",
            "Epoch: 0056 loss_train: 0.1644 acc_train: 0.9850 loss_val: 0.0733 acc_val: 1.0000\n",
            "Epoch: 0057 loss_train: 0.1726 acc_train: 0.9850 loss_val: 0.0628 acc_val: 1.0000\n",
            "Epoch: 0058 loss_train: 0.1413 acc_train: 0.9915 loss_val: 0.0619 acc_val: 1.0000\n",
            "Epoch: 0059 loss_train: 0.1361 acc_train: 0.9920 loss_val: 0.0474 acc_val: 1.0000\n",
            "Epoch: 0060 loss_train: 0.1128 acc_train: 1.0000 loss_val: 0.0380 acc_val: 1.0000\n",
            "Epoch: 0061 loss_train: 0.0984 acc_train: 1.0000 loss_val: 0.0364 acc_val: 1.0000\n",
            "Epoch: 0062 loss_train: 0.0945 acc_train: 0.9950 loss_val: 0.0330 acc_val: 1.0000\n",
            "Epoch: 0063 loss_train: 0.0833 acc_train: 0.9915 loss_val: 0.0294 acc_val: 1.0000\n",
            "Epoch: 0064 loss_train: 0.0746 acc_train: 0.9995 loss_val: 0.0249 acc_val: 1.0000\n",
            "Epoch: 0065 loss_train: 0.0625 acc_train: 1.0000 loss_val: 0.0206 acc_val: 1.0000\n",
            "Epoch: 0066 loss_train: 0.0558 acc_train: 1.0000 loss_val: 0.0166 acc_val: 1.0000\n",
            "Epoch: 0067 loss_train: 0.0518 acc_train: 1.0000 loss_val: 0.0142 acc_val: 1.0000\n",
            "Epoch: 0068 loss_train: 0.0467 acc_train: 1.0000 loss_val: 0.0132 acc_val: 1.0000\n",
            "Epoch: 0069 loss_train: 0.0393 acc_train: 1.0000 loss_val: 0.0131 acc_val: 1.0000\n",
            "Epoch: 0070 loss_train: 0.0379 acc_train: 1.0000 loss_val: 0.0096 acc_val: 1.0000\n",
            "Epoch: 0071 loss_train: 0.0290 acc_train: 1.0000 loss_val: 0.0076 acc_val: 1.0000\n",
            "Epoch: 0072 loss_train: 0.0268 acc_train: 1.0000 loss_val: 0.0068 acc_val: 1.0000\n",
            "Epoch: 0073 loss_train: 0.0256 acc_train: 1.0000 loss_val: 0.0063 acc_val: 1.0000\n",
            "Epoch: 0074 loss_train: 0.0233 acc_train: 1.0000 loss_val: 0.0052 acc_val: 1.0000\n",
            "Epoch: 0075 loss_train: 0.0187 acc_train: 1.0000 loss_val: 0.0047 acc_val: 1.0000\n",
            "Epoch: 0076 loss_train: 0.0176 acc_train: 1.0000 loss_val: 0.0041 acc_val: 1.0000\n",
            "Epoch: 0077 loss_train: 0.0154 acc_train: 1.0000 loss_val: 0.0033 acc_val: 1.0000\n",
            "Epoch: 0078 loss_train: 0.0130 acc_train: 1.0000 loss_val: 0.0026 acc_val: 1.0000\n",
            "Epoch: 0079 loss_train: 0.0117 acc_train: 1.0000 loss_val: 0.0023 acc_val: 1.0000\n",
            "Epoch: 0080 loss_train: 0.0116 acc_train: 1.0000 loss_val: 0.0020 acc_val: 1.0000\n",
            "Epoch: 0081 loss_train: 0.0104 acc_train: 1.0000 loss_val: 0.0018 acc_val: 1.0000\n",
            "Epoch: 0082 loss_train: 0.0088 acc_train: 1.0000 loss_val: 0.0017 acc_val: 1.0000\n",
            "Epoch: 0083 loss_train: 0.0078 acc_train: 1.0000 loss_val: 0.0016 acc_val: 1.0000\n",
            "Epoch: 0084 loss_train: 0.0072 acc_train: 1.0000 loss_val: 0.0013 acc_val: 1.0000\n",
            "Epoch: 0085 loss_train: 0.0063 acc_train: 1.0000 loss_val: 0.0012 acc_val: 1.0000\n",
            "Epoch: 0086 loss_train: 0.0055 acc_train: 1.0000 loss_val: 0.0011 acc_val: 1.0000\n",
            "Epoch: 0087 loss_train: 0.0050 acc_train: 1.0000 loss_val: 0.0010 acc_val: 1.0000\n",
            "Epoch: 0088 loss_train: 0.0048 acc_train: 1.0000 loss_val: 0.0010 acc_val: 1.0000\n",
            "Epoch: 0089 loss_train: 0.0046 acc_train: 1.0000 loss_val: 0.0009 acc_val: 1.0000\n",
            "Epoch: 0090 loss_train: 0.0042 acc_train: 1.0000 loss_val: 0.0009 acc_val: 1.0000\n",
            "Epoch: 0091 loss_train: 0.0038 acc_train: 1.0000 loss_val: 0.0008 acc_val: 1.0000\n",
            "Epoch: 0092 loss_train: 0.0035 acc_train: 1.0000 loss_val: 0.0007 acc_val: 1.0000\n",
            "Epoch: 0093 loss_train: 0.0032 acc_train: 1.0000 loss_val: 0.0007 acc_val: 1.0000\n",
            "Epoch: 0094 loss_train: 0.0029 acc_train: 1.0000 loss_val: 0.0006 acc_val: 1.0000\n",
            "Epoch: 0095 loss_train: 0.0026 acc_train: 1.0000 loss_val: 0.0006 acc_val: 1.0000\n",
            "Epoch: 0096 loss_train: 0.0024 acc_train: 1.0000 loss_val: 0.0005 acc_val: 1.0000\n",
            "Epoch: 0097 loss_train: 0.0023 acc_train: 1.0000 loss_val: 0.0005 acc_val: 1.0000\n",
            "Epoch: 0098 loss_train: 0.0021 acc_train: 1.0000 loss_val: 0.0005 acc_val: 1.0000\n",
            "Epoch: 0099 loss_train: 0.0020 acc_train: 1.0000 loss_val: 0.0005 acc_val: 1.0000\n",
            "Epoch: 0100 loss_train: 0.0019 acc_train: 1.0000 loss_val: 0.0004 acc_val: 1.0000\n",
            "Epoch: 0101 loss_train: 0.0018 acc_train: 1.0000 loss_val: 0.0004 acc_val: 1.0000\n",
            "Epoch: 0102 loss_train: 0.0017 acc_train: 1.0000 loss_val: 0.0004 acc_val: 1.0000\n",
            "Epoch: 0103 loss_train: 0.0016 acc_train: 1.0000 loss_val: 0.0004 acc_val: 1.0000\n",
            "Epoch: 0104 loss_train: 0.0015 acc_train: 1.0000 loss_val: 0.0004 acc_val: 1.0000\n",
            "Epoch: 0105 loss_train: 0.0015 acc_train: 1.0000 loss_val: 0.0004 acc_val: 1.0000\n",
            "Epoch: 0106 loss_train: 0.0014 acc_train: 1.0000 loss_val: 0.0004 acc_val: 1.0000\n",
            "Epoch: 0107 loss_train: 0.0013 acc_train: 1.0000 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 0108 loss_train: 0.0012 acc_train: 1.0000 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 0109 loss_train: 0.0012 acc_train: 1.0000 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 0110 loss_train: 0.0011 acc_train: 1.0000 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 0111 loss_train: 0.0011 acc_train: 1.0000 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 0112 loss_train: 0.0011 acc_train: 1.0000 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 0113 loss_train: 0.0010 acc_train: 1.0000 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 0114 loss_train: 0.0009 acc_train: 1.0000 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 0115 loss_train: 0.0009 acc_train: 1.0000 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 0116 loss_train: 0.0009 acc_train: 1.0000 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 0117 loss_train: 0.0009 acc_train: 1.0000 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 0118 loss_train: 0.0009 acc_train: 1.0000 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 0119 loss_train: 0.0008 acc_train: 1.0000 loss_val: 0.0003 acc_val: 1.0000\n",
            "Epoch: 0120 loss_train: 0.0008 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 0121 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 0122 loss_train: 0.0008 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 0123 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 0124 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 0125 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 0126 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 0127 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 0128 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 0129 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 0130 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 0131 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 0132 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 0133 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 0134 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 0135 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 0136 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 0137 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 0138 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 0139 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 0140 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 0141 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 0142 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 0143 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 0144 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0002 acc_val: 1.0000\n",
            "Epoch: 0145 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0146 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0147 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0148 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0149 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0150 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0151 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0152 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0153 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0154 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0155 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0156 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0157 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0158 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0159 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0160 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0161 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0162 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0163 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0164 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0165 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0166 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0167 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0168 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0169 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0170 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0171 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0172 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0173 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0174 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0175 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0176 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0177 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0178 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0179 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0180 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0181 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0182 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0183 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0184 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0185 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0186 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0187 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0188 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0189 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0190 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0191 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0192 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0193 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0194 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0195 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0196 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0197 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0198 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0199 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Epoch: 0200 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0001 acc_val: 1.0000\n",
            "Optimization Finished!\n",
            "Train cost: 63.2895s\n",
            "Loading 200th epoch\n",
            "Test set results: loss= 0.0001 accuracy= 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --dataset citeseer --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 5  --n_heads 8 --n_layers 1 --pe_dim 40 --peak_lr 0.001  --attack none --attperc 20 --TrainTest train --weight_decay=1e-05\n",
        "\n",
        "!python train.py --dataset citeseer --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 5  --n_heads 8 --n_layers 1 --pe_dim 40 --peak_lr 0.001  --attack l2_weak --attperc 10 --TrainTest train --weight_decay=1e-05\n",
        "\n",
        "!python train.py --dataset citeseer --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 5  --n_heads 8 --n_layers 1 --pe_dim 40 --peak_lr 0.001  --attack l2_weak --attperc 40 --TrainTest train --weight_decay=1e-05\n",
        "\n",
        "!python train.py --dataset citeseer --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 5  --n_heads 8 --n_layers 1 --pe_dim 40 --peak_lr 0.001  --attack l2_weak --attperc 60 --TrainTest train --weight_decay=1e-05\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lr5lYgDCaNbY",
        "outputId": "4c4664c6-4b60-44f6-de45-d75394192ee3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NumNodes: 3327\n",
            "  NumEdges: 9228\n",
            "  NumFeats: 3703\n",
            "  NumClasses: 6\n",
            "  NumTrainingSamples: 120\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=3743, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=6, bias=True)\n",
            ")\n",
            "total params: 4154632\n",
            "Epoch: 0001 loss_train: 1.8342 acc_train: 0.1412 loss_val: 1.8246 acc_val: 0.1757\n",
            "Epoch: 0002 loss_train: 1.8278 acc_train: 0.1527 loss_val: 1.8145 acc_val: 0.1892\n",
            "Epoch: 0003 loss_train: 1.8206 acc_train: 0.1592 loss_val: 1.7996 acc_val: 0.1877\n",
            "Epoch: 0004 loss_train: 1.8050 acc_train: 0.1738 loss_val: 1.7804 acc_val: 0.1982\n",
            "Epoch: 0005 loss_train: 1.7892 acc_train: 0.1948 loss_val: 1.7574 acc_val: 0.2598\n",
            "Epoch: 0006 loss_train: 1.7646 acc_train: 0.2323 loss_val: 1.7310 acc_val: 0.3273\n",
            "Epoch: 0007 loss_train: 1.7381 acc_train: 0.2969 loss_val: 1.7019 acc_val: 0.4054\n",
            "Epoch: 0008 loss_train: 1.7107 acc_train: 0.3796 loss_val: 1.6705 acc_val: 0.4745\n",
            "Epoch: 0009 loss_train: 1.6795 acc_train: 0.4687 loss_val: 1.6371 acc_val: 0.5180\n",
            "Epoch: 0010 loss_train: 1.6443 acc_train: 0.5238 loss_val: 1.6021 acc_val: 0.5450\n",
            "Epoch: 0011 loss_train: 1.6093 acc_train: 0.5633 loss_val: 1.5659 acc_val: 0.5631\n",
            "Epoch: 0012 loss_train: 1.5712 acc_train: 0.5779 loss_val: 1.5283 acc_val: 0.5751\n",
            "Epoch: 0013 loss_train: 1.5338 acc_train: 0.5939 loss_val: 1.4895 acc_val: 0.5856\n",
            "Epoch: 0014 loss_train: 1.4936 acc_train: 0.6134 loss_val: 1.4499 acc_val: 0.6006\n",
            "Epoch: 0015 loss_train: 1.4514 acc_train: 0.6254 loss_val: 1.4097 acc_val: 0.6126\n",
            "Epoch: 0016 loss_train: 1.4086 acc_train: 0.6304 loss_val: 1.3691 acc_val: 0.6201\n",
            "Epoch: 0017 loss_train: 1.3665 acc_train: 0.6485 loss_val: 1.3284 acc_val: 0.6276\n",
            "Epoch: 0018 loss_train: 1.3224 acc_train: 0.6550 loss_val: 1.2880 acc_val: 0.6396\n",
            "Epoch: 0019 loss_train: 1.2805 acc_train: 0.6660 loss_val: 1.2483 acc_val: 0.6456\n",
            "Epoch: 0020 loss_train: 1.2373 acc_train: 0.6730 loss_val: 1.2094 acc_val: 0.6562\n",
            "Epoch: 0021 loss_train: 1.1942 acc_train: 0.6790 loss_val: 1.1717 acc_val: 0.6682\n",
            "Epoch: 0022 loss_train: 1.1514 acc_train: 0.6960 loss_val: 1.1356 acc_val: 0.6892\n",
            "Epoch: 0023 loss_train: 1.1101 acc_train: 0.7031 loss_val: 1.1012 acc_val: 0.6922\n",
            "Epoch: 0024 loss_train: 1.0701 acc_train: 0.7111 loss_val: 1.0689 acc_val: 0.6937\n",
            "Epoch: 0025 loss_train: 1.0313 acc_train: 0.7171 loss_val: 1.0388 acc_val: 0.6982\n",
            "Epoch: 0026 loss_train: 0.9938 acc_train: 0.7266 loss_val: 1.0108 acc_val: 0.7042\n",
            "Epoch: 0027 loss_train: 0.9575 acc_train: 0.7276 loss_val: 0.9850 acc_val: 0.7042\n",
            "Epoch: 0028 loss_train: 0.9233 acc_train: 0.7336 loss_val: 0.9612 acc_val: 0.7132\n",
            "Epoch: 0029 loss_train: 0.8901 acc_train: 0.7391 loss_val: 0.9397 acc_val: 0.7162\n",
            "Epoch: 0030 loss_train: 0.8563 acc_train: 0.7456 loss_val: 0.9205 acc_val: 0.7192\n",
            "Epoch: 0031 loss_train: 0.8228 acc_train: 0.7531 loss_val: 0.9037 acc_val: 0.7207\n",
            "Epoch: 0032 loss_train: 0.7948 acc_train: 0.7616 loss_val: 0.8892 acc_val: 0.7192\n",
            "Epoch: 0033 loss_train: 0.7652 acc_train: 0.7682 loss_val: 0.8765 acc_val: 0.7207\n",
            "Epoch: 0034 loss_train: 0.7386 acc_train: 0.7722 loss_val: 0.8653 acc_val: 0.7207\n",
            "Epoch: 0035 loss_train: 0.7101 acc_train: 0.7777 loss_val: 0.8560 acc_val: 0.7237\n",
            "Epoch: 0036 loss_train: 0.6839 acc_train: 0.7837 loss_val: 0.8485 acc_val: 0.7267\n",
            "Epoch: 0037 loss_train: 0.6595 acc_train: 0.7867 loss_val: 0.8427 acc_val: 0.7237\n",
            "Epoch: 0038 loss_train: 0.6345 acc_train: 0.7972 loss_val: 0.8381 acc_val: 0.7357\n",
            "Epoch: 0039 loss_train: 0.6103 acc_train: 0.8067 loss_val: 0.8349 acc_val: 0.7372\n",
            "Epoch: 0040 loss_train: 0.5898 acc_train: 0.8137 loss_val: 0.8334 acc_val: 0.7462\n",
            "Epoch: 0041 loss_train: 0.5698 acc_train: 0.8167 loss_val: 0.8337 acc_val: 0.7462\n",
            "Epoch: 0042 loss_train: 0.5483 acc_train: 0.8212 loss_val: 0.8358 acc_val: 0.7477\n",
            "Epoch: 0043 loss_train: 0.5265 acc_train: 0.8247 loss_val: 0.8397 acc_val: 0.7508\n",
            "Epoch: 0044 loss_train: 0.5079 acc_train: 0.8292 loss_val: 0.8456 acc_val: 0.7492\n",
            "Epoch: 0045 loss_train: 0.4877 acc_train: 0.8322 loss_val: 0.8525 acc_val: 0.7447\n",
            "Epoch: 0046 loss_train: 0.4702 acc_train: 0.8388 loss_val: 0.8603 acc_val: 0.7477\n",
            "Epoch: 0047 loss_train: 0.4494 acc_train: 0.8478 loss_val: 0.8698 acc_val: 0.7462\n",
            "Epoch: 0048 loss_train: 0.4337 acc_train: 0.8533 loss_val: 0.8809 acc_val: 0.7417\n",
            "Epoch: 0049 loss_train: 0.4156 acc_train: 0.8578 loss_val: 0.8936 acc_val: 0.7402\n",
            "Epoch: 0050 loss_train: 0.3995 acc_train: 0.8653 loss_val: 0.9074 acc_val: 0.7417\n",
            "Epoch: 0051 loss_train: 0.3842 acc_train: 0.8643 loss_val: 0.9222 acc_val: 0.7372\n",
            "Epoch: 0052 loss_train: 0.3657 acc_train: 0.8773 loss_val: 0.9390 acc_val: 0.7357\n",
            "Epoch: 0053 loss_train: 0.3496 acc_train: 0.8808 loss_val: 0.9587 acc_val: 0.7372\n",
            "Epoch: 0054 loss_train: 0.3342 acc_train: 0.8848 loss_val: 0.9803 acc_val: 0.7312\n",
            "Epoch: 0055 loss_train: 0.3183 acc_train: 0.8918 loss_val: 1.0022 acc_val: 0.7282\n",
            "Epoch: 0056 loss_train: 0.2997 acc_train: 0.9004 loss_val: 1.0245 acc_val: 0.7297\n",
            "Epoch: 0057 loss_train: 0.2826 acc_train: 0.8998 loss_val: 1.0471 acc_val: 0.7282\n",
            "Epoch: 0058 loss_train: 0.2705 acc_train: 0.9014 loss_val: 1.0700 acc_val: 0.7312\n",
            "Epoch: 0059 loss_train: 0.2493 acc_train: 0.9119 loss_val: 1.0930 acc_val: 0.7342\n",
            "Epoch: 0060 loss_train: 0.2338 acc_train: 0.9204 loss_val: 1.1163 acc_val: 0.7342\n",
            "Epoch: 0061 loss_train: 0.2192 acc_train: 0.9274 loss_val: 1.1416 acc_val: 0.7357\n",
            "Epoch: 0062 loss_train: 0.2034 acc_train: 0.9359 loss_val: 1.1693 acc_val: 0.7387\n",
            "Epoch: 0063 loss_train: 0.1880 acc_train: 0.9439 loss_val: 1.1976 acc_val: 0.7432\n",
            "Epoch: 0064 loss_train: 0.1752 acc_train: 0.9464 loss_val: 1.2253 acc_val: 0.7387\n",
            "Epoch: 0065 loss_train: 0.1605 acc_train: 0.9549 loss_val: 1.2564 acc_val: 0.7402\n",
            "Epoch: 0066 loss_train: 0.1493 acc_train: 0.9514 loss_val: 1.2885 acc_val: 0.7387\n",
            "Epoch: 0067 loss_train: 0.1369 acc_train: 0.9629 loss_val: 1.3184 acc_val: 0.7402\n",
            "Epoch: 0068 loss_train: 0.1237 acc_train: 0.9619 loss_val: 1.3475 acc_val: 0.7387\n",
            "Epoch: 0069 loss_train: 0.1141 acc_train: 0.9670 loss_val: 1.3774 acc_val: 0.7372\n",
            "Epoch: 0070 loss_train: 0.1046 acc_train: 0.9685 loss_val: 1.4064 acc_val: 0.7372\n",
            "Epoch: 0071 loss_train: 0.0957 acc_train: 0.9710 loss_val: 1.4324 acc_val: 0.7327\n",
            "Epoch: 0072 loss_train: 0.0864 acc_train: 0.9725 loss_val: 1.4580 acc_val: 0.7282\n",
            "Epoch: 0073 loss_train: 0.0794 acc_train: 0.9745 loss_val: 1.4830 acc_val: 0.7297\n",
            "Epoch: 0074 loss_train: 0.0719 acc_train: 0.9775 loss_val: 1.5067 acc_val: 0.7282\n",
            "Epoch: 0075 loss_train: 0.0650 acc_train: 0.9810 loss_val: 1.5305 acc_val: 0.7297\n",
            "Epoch: 0076 loss_train: 0.0593 acc_train: 0.9825 loss_val: 1.5557 acc_val: 0.7282\n",
            "Epoch: 0077 loss_train: 0.0555 acc_train: 0.9840 loss_val: 1.5821 acc_val: 0.7252\n",
            "Epoch: 0078 loss_train: 0.0480 acc_train: 0.9870 loss_val: 1.6092 acc_val: 0.7267\n",
            "Epoch: 0079 loss_train: 0.0456 acc_train: 0.9875 loss_val: 1.6336 acc_val: 0.7282\n",
            "Epoch: 0080 loss_train: 0.0405 acc_train: 0.9875 loss_val: 1.6592 acc_val: 0.7267\n",
            "Epoch: 0081 loss_train: 0.0359 acc_train: 0.9900 loss_val: 1.6857 acc_val: 0.7282\n",
            "Epoch: 0082 loss_train: 0.0342 acc_train: 0.9925 loss_val: 1.7128 acc_val: 0.7267\n",
            "Epoch: 0083 loss_train: 0.0307 acc_train: 0.9915 loss_val: 1.7375 acc_val: 0.7252\n",
            "Epoch: 0084 loss_train: 0.0291 acc_train: 0.9915 loss_val: 1.7580 acc_val: 0.7237\n",
            "Epoch: 0085 loss_train: 0.0253 acc_train: 0.9950 loss_val: 1.7781 acc_val: 0.7252\n",
            "Epoch: 0086 loss_train: 0.0240 acc_train: 0.9915 loss_val: 1.8004 acc_val: 0.7267\n",
            "Epoch: 0087 loss_train: 0.0223 acc_train: 0.9945 loss_val: 1.8231 acc_val: 0.7222\n",
            "Epoch: 0088 loss_train: 0.0203 acc_train: 0.9950 loss_val: 1.8434 acc_val: 0.7237\n",
            "Epoch: 0089 loss_train: 0.0180 acc_train: 0.9960 loss_val: 1.8600 acc_val: 0.7237\n",
            "Epoch: 0090 loss_train: 0.0178 acc_train: 0.9945 loss_val: 1.8726 acc_val: 0.7267\n",
            "Epoch: 0091 loss_train: 0.0173 acc_train: 0.9950 loss_val: 1.8876 acc_val: 0.7297\n",
            "Epoch: 0092 loss_train: 0.0151 acc_train: 0.9950 loss_val: 1.9051 acc_val: 0.7282\n",
            "Epoch: 0093 loss_train: 0.0131 acc_train: 0.9965 loss_val: 1.9223 acc_val: 0.7282\n",
            "Optimization Finished!\n",
            "Train cost: 29.9250s\n",
            "Loading 43th epoch\n",
            "Test set results: loss= 0.7103 accuracy= 0.7831\n",
            "  NumNodes: 3327\n",
            "  NumEdges: 9228\n",
            "  NumFeats: 3703\n",
            "  NumClasses: 6\n",
            "  NumTrainingSamples: 120\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:354: DGLWarning: DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\n",
            "  dgl_warning(\"DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\")\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=3743, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=6, bias=True)\n",
            ")\n",
            "total params: 4154632\n",
            "Epoch: 0001 loss_train: 1.8415 acc_train: 0.1127 loss_val: 1.8348 acc_val: 0.0976\n",
            "Epoch: 0002 loss_train: 1.8342 acc_train: 0.1162 loss_val: 1.8249 acc_val: 0.1096\n",
            "Epoch: 0003 loss_train: 1.8256 acc_train: 0.1237 loss_val: 1.8104 acc_val: 0.1246\n",
            "Epoch: 0004 loss_train: 1.8108 acc_train: 0.1447 loss_val: 1.7913 acc_val: 0.1471\n",
            "Epoch: 0005 loss_train: 1.7936 acc_train: 0.1557 loss_val: 1.7681 acc_val: 0.2177\n",
            "Epoch: 0006 loss_train: 1.7703 acc_train: 0.2158 loss_val: 1.7412 acc_val: 0.3138\n",
            "Epoch: 0007 loss_train: 1.7459 acc_train: 0.2994 loss_val: 1.7111 acc_val: 0.3754\n",
            "Epoch: 0008 loss_train: 1.7187 acc_train: 0.3771 loss_val: 1.6783 acc_val: 0.4414\n",
            "Epoch: 0009 loss_train: 1.6874 acc_train: 0.4352 loss_val: 1.6435 acc_val: 0.4985\n",
            "Epoch: 0010 loss_train: 1.6548 acc_train: 0.4922 loss_val: 1.6073 acc_val: 0.5330\n",
            "Epoch: 0011 loss_train: 1.6184 acc_train: 0.5278 loss_val: 1.5704 acc_val: 0.5480\n",
            "Epoch: 0012 loss_train: 1.5825 acc_train: 0.5538 loss_val: 1.5330 acc_val: 0.5631\n",
            "Epoch: 0013 loss_train: 1.5473 acc_train: 0.5749 loss_val: 1.4951 acc_val: 0.5676\n",
            "Epoch: 0014 loss_train: 1.5069 acc_train: 0.5879 loss_val: 1.4567 acc_val: 0.5781\n",
            "Epoch: 0015 loss_train: 1.4678 acc_train: 0.6069 loss_val: 1.4178 acc_val: 0.5991\n",
            "Epoch: 0016 loss_train: 1.4282 acc_train: 0.6154 loss_val: 1.3786 acc_val: 0.6126\n",
            "Epoch: 0017 loss_train: 1.3884 acc_train: 0.6279 loss_val: 1.3390 acc_val: 0.6186\n",
            "Epoch: 0018 loss_train: 1.3485 acc_train: 0.6380 loss_val: 1.2994 acc_val: 0.6276\n",
            "Epoch: 0019 loss_train: 1.3061 acc_train: 0.6470 loss_val: 1.2599 acc_val: 0.6336\n",
            "Epoch: 0020 loss_train: 1.2658 acc_train: 0.6560 loss_val: 1.2210 acc_val: 0.6426\n",
            "Epoch: 0021 loss_train: 1.2266 acc_train: 0.6580 loss_val: 1.1829 acc_val: 0.6532\n",
            "Epoch: 0022 loss_train: 1.1814 acc_train: 0.6715 loss_val: 1.1460 acc_val: 0.6607\n",
            "Epoch: 0023 loss_train: 1.1430 acc_train: 0.6770 loss_val: 1.1105 acc_val: 0.6697\n",
            "Epoch: 0024 loss_train: 1.1046 acc_train: 0.6820 loss_val: 1.0767 acc_val: 0.6772\n",
            "Epoch: 0025 loss_train: 1.0663 acc_train: 0.6870 loss_val: 1.0448 acc_val: 0.6847\n",
            "Epoch: 0026 loss_train: 1.0288 acc_train: 0.7011 loss_val: 1.0149 acc_val: 0.6967\n",
            "Epoch: 0027 loss_train: 0.9933 acc_train: 0.7126 loss_val: 0.9872 acc_val: 0.6967\n",
            "Epoch: 0028 loss_train: 0.9594 acc_train: 0.7156 loss_val: 0.9616 acc_val: 0.7027\n",
            "Epoch: 0029 loss_train: 0.9246 acc_train: 0.7201 loss_val: 0.9382 acc_val: 0.7012\n",
            "Epoch: 0030 loss_train: 0.8904 acc_train: 0.7246 loss_val: 0.9172 acc_val: 0.7027\n",
            "Epoch: 0031 loss_train: 0.8570 acc_train: 0.7301 loss_val: 0.8988 acc_val: 0.7072\n",
            "Epoch: 0032 loss_train: 0.8281 acc_train: 0.7386 loss_val: 0.8828 acc_val: 0.7057\n",
            "Epoch: 0033 loss_train: 0.7946 acc_train: 0.7496 loss_val: 0.8689 acc_val: 0.7087\n",
            "Epoch: 0034 loss_train: 0.7643 acc_train: 0.7536 loss_val: 0.8568 acc_val: 0.7132\n",
            "Epoch: 0035 loss_train: 0.7383 acc_train: 0.7631 loss_val: 0.8468 acc_val: 0.7177\n",
            "Epoch: 0036 loss_train: 0.7065 acc_train: 0.7732 loss_val: 0.8388 acc_val: 0.7252\n",
            "Epoch: 0037 loss_train: 0.6788 acc_train: 0.7782 loss_val: 0.8323 acc_val: 0.7297\n",
            "Epoch: 0038 loss_train: 0.6515 acc_train: 0.7837 loss_val: 0.8275 acc_val: 0.7387\n",
            "Epoch: 0039 loss_train: 0.6258 acc_train: 0.7962 loss_val: 0.8250 acc_val: 0.7447\n",
            "Epoch: 0040 loss_train: 0.6015 acc_train: 0.8062 loss_val: 0.8252 acc_val: 0.7477\n",
            "Epoch: 0041 loss_train: 0.5780 acc_train: 0.8162 loss_val: 0.8282 acc_val: 0.7432\n",
            "Epoch: 0042 loss_train: 0.5536 acc_train: 0.8207 loss_val: 0.8340 acc_val: 0.7432\n",
            "Epoch: 0043 loss_train: 0.5290 acc_train: 0.8272 loss_val: 0.8414 acc_val: 0.7492\n",
            "Epoch: 0044 loss_train: 0.5095 acc_train: 0.8343 loss_val: 0.8496 acc_val: 0.7417\n",
            "Epoch: 0045 loss_train: 0.4866 acc_train: 0.8408 loss_val: 0.8590 acc_val: 0.7447\n",
            "Epoch: 0046 loss_train: 0.4655 acc_train: 0.8468 loss_val: 0.8701 acc_val: 0.7462\n",
            "Epoch: 0047 loss_train: 0.4442 acc_train: 0.8518 loss_val: 0.8825 acc_val: 0.7387\n",
            "Epoch: 0048 loss_train: 0.4277 acc_train: 0.8558 loss_val: 0.8950 acc_val: 0.7417\n",
            "Epoch: 0049 loss_train: 0.4043 acc_train: 0.8628 loss_val: 0.9079 acc_val: 0.7417\n",
            "Epoch: 0050 loss_train: 0.3865 acc_train: 0.8668 loss_val: 0.9240 acc_val: 0.7447\n",
            "Epoch: 0051 loss_train: 0.3662 acc_train: 0.8763 loss_val: 0.9433 acc_val: 0.7477\n",
            "Epoch: 0052 loss_train: 0.3444 acc_train: 0.8843 loss_val: 0.9642 acc_val: 0.7477\n",
            "Epoch: 0053 loss_train: 0.3272 acc_train: 0.8888 loss_val: 0.9859 acc_val: 0.7447\n",
            "Epoch: 0054 loss_train: 0.3105 acc_train: 0.8903 loss_val: 1.0079 acc_val: 0.7402\n",
            "Epoch: 0055 loss_train: 0.2931 acc_train: 0.8998 loss_val: 1.0311 acc_val: 0.7417\n",
            "Epoch: 0056 loss_train: 0.2742 acc_train: 0.9069 loss_val: 1.0564 acc_val: 0.7402\n",
            "Epoch: 0057 loss_train: 0.2545 acc_train: 0.9169 loss_val: 1.0821 acc_val: 0.7372\n",
            "Epoch: 0058 loss_train: 0.2420 acc_train: 0.9194 loss_val: 1.1090 acc_val: 0.7327\n",
            "Epoch: 0059 loss_train: 0.2218 acc_train: 0.9289 loss_val: 1.1372 acc_val: 0.7357\n",
            "Epoch: 0060 loss_train: 0.2082 acc_train: 0.9364 loss_val: 1.1665 acc_val: 0.7282\n",
            "Epoch: 0061 loss_train: 0.1952 acc_train: 0.9414 loss_val: 1.1949 acc_val: 0.7312\n",
            "Epoch: 0062 loss_train: 0.1804 acc_train: 0.9464 loss_val: 1.2231 acc_val: 0.7327\n",
            "Epoch: 0063 loss_train: 0.1662 acc_train: 0.9509 loss_val: 1.2510 acc_val: 0.7312\n",
            "Epoch: 0064 loss_train: 0.1532 acc_train: 0.9569 loss_val: 1.2785 acc_val: 0.7297\n",
            "Epoch: 0065 loss_train: 0.1369 acc_train: 0.9614 loss_val: 1.3058 acc_val: 0.7327\n",
            "Epoch: 0066 loss_train: 0.1269 acc_train: 0.9634 loss_val: 1.3340 acc_val: 0.7342\n",
            "Epoch: 0067 loss_train: 0.1143 acc_train: 0.9680 loss_val: 1.3639 acc_val: 0.7327\n",
            "Epoch: 0068 loss_train: 0.1069 acc_train: 0.9670 loss_val: 1.3910 acc_val: 0.7327\n",
            "Epoch: 0069 loss_train: 0.0975 acc_train: 0.9695 loss_val: 1.4196 acc_val: 0.7312\n",
            "Epoch: 0070 loss_train: 0.0884 acc_train: 0.9750 loss_val: 1.4502 acc_val: 0.7357\n",
            "Epoch: 0071 loss_train: 0.0793 acc_train: 0.9790 loss_val: 1.4793 acc_val: 0.7297\n",
            "Epoch: 0072 loss_train: 0.0703 acc_train: 0.9800 loss_val: 1.5082 acc_val: 0.7312\n",
            "Epoch: 0073 loss_train: 0.0639 acc_train: 0.9815 loss_val: 1.5380 acc_val: 0.7252\n",
            "Epoch: 0074 loss_train: 0.0567 acc_train: 0.9825 loss_val: 1.5673 acc_val: 0.7282\n",
            "Epoch: 0075 loss_train: 0.0513 acc_train: 0.9855 loss_val: 1.5962 acc_val: 0.7327\n",
            "Epoch: 0076 loss_train: 0.0483 acc_train: 0.9865 loss_val: 1.6259 acc_val: 0.7297\n",
            "Epoch: 0077 loss_train: 0.0438 acc_train: 0.9865 loss_val: 1.6546 acc_val: 0.7267\n",
            "Epoch: 0078 loss_train: 0.0373 acc_train: 0.9905 loss_val: 1.6819 acc_val: 0.7252\n",
            "Epoch: 0079 loss_train: 0.0359 acc_train: 0.9900 loss_val: 1.7072 acc_val: 0.7267\n",
            "Epoch: 0080 loss_train: 0.0308 acc_train: 0.9950 loss_val: 1.7306 acc_val: 0.7252\n",
            "Epoch: 0081 loss_train: 0.0280 acc_train: 0.9925 loss_val: 1.7545 acc_val: 0.7282\n",
            "Epoch: 0082 loss_train: 0.0261 acc_train: 0.9935 loss_val: 1.7808 acc_val: 0.7267\n",
            "Epoch: 0083 loss_train: 0.0230 acc_train: 0.9960 loss_val: 1.8057 acc_val: 0.7267\n",
            "Epoch: 0084 loss_train: 0.0211 acc_train: 0.9970 loss_val: 1.8267 acc_val: 0.7237\n",
            "Epoch: 0085 loss_train: 0.0190 acc_train: 0.9965 loss_val: 1.8473 acc_val: 0.7222\n",
            "Epoch: 0086 loss_train: 0.0174 acc_train: 0.9970 loss_val: 1.8689 acc_val: 0.7267\n",
            "Epoch: 0087 loss_train: 0.0159 acc_train: 0.9970 loss_val: 1.8889 acc_val: 0.7222\n",
            "Epoch: 0088 loss_train: 0.0146 acc_train: 0.9970 loss_val: 1.9072 acc_val: 0.7192\n",
            "Epoch: 0089 loss_train: 0.0137 acc_train: 0.9975 loss_val: 1.9253 acc_val: 0.7192\n",
            "Epoch: 0090 loss_train: 0.0124 acc_train: 0.9970 loss_val: 1.9404 acc_val: 0.7252\n",
            "Epoch: 0091 loss_train: 0.0121 acc_train: 0.9970 loss_val: 1.9510 acc_val: 0.7252\n",
            "Epoch: 0092 loss_train: 0.0104 acc_train: 0.9975 loss_val: 1.9629 acc_val: 0.7252\n",
            "Epoch: 0093 loss_train: 0.0095 acc_train: 0.9980 loss_val: 1.9786 acc_val: 0.7222\n",
            "Optimization Finished!\n",
            "Train cost: 29.8348s\n",
            "Loading 43th epoch\n",
            "Test set results: loss= 0.7246 accuracy= 0.7711\n",
            "  NumNodes: 3327\n",
            "  NumEdges: 9228\n",
            "  NumFeats: 3703\n",
            "  NumClasses: 6\n",
            "  NumTrainingSamples: 120\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:354: DGLWarning: DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\n",
            "  dgl_warning(\"DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\")\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=3743, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=6, bias=True)\n",
            ")\n",
            "total params: 4154632\n",
            "Epoch: 0001 loss_train: 1.8184 acc_train: 0.1352 loss_val: 1.8107 acc_val: 0.1607\n",
            "Epoch: 0002 loss_train: 1.8123 acc_train: 0.1487 loss_val: 1.8006 acc_val: 0.1682\n",
            "Epoch: 0003 loss_train: 1.8031 acc_train: 0.1567 loss_val: 1.7859 acc_val: 0.1907\n",
            "Epoch: 0004 loss_train: 1.7885 acc_train: 0.1748 loss_val: 1.7669 acc_val: 0.2538\n",
            "Epoch: 0005 loss_train: 1.7739 acc_train: 0.2148 loss_val: 1.7442 acc_val: 0.3303\n",
            "Epoch: 0006 loss_train: 1.7531 acc_train: 0.2974 loss_val: 1.7184 acc_val: 0.3859\n",
            "Epoch: 0007 loss_train: 1.7291 acc_train: 0.3520 loss_val: 1.6900 acc_val: 0.4294\n",
            "Epoch: 0008 loss_train: 1.7053 acc_train: 0.3951 loss_val: 1.6595 acc_val: 0.4565\n",
            "Epoch: 0009 loss_train: 1.6788 acc_train: 0.4367 loss_val: 1.6275 acc_val: 0.4835\n",
            "Epoch: 0010 loss_train: 1.6484 acc_train: 0.4802 loss_val: 1.5945 acc_val: 0.5180\n",
            "Epoch: 0011 loss_train: 1.6182 acc_train: 0.4932 loss_val: 1.5605 acc_val: 0.5330\n",
            "Epoch: 0012 loss_train: 1.5858 acc_train: 0.5013 loss_val: 1.5258 acc_val: 0.5375\n",
            "Epoch: 0013 loss_train: 1.5558 acc_train: 0.5193 loss_val: 1.4905 acc_val: 0.5495\n",
            "Epoch: 0014 loss_train: 1.5224 acc_train: 0.5253 loss_val: 1.4546 acc_val: 0.5586\n",
            "Epoch: 0015 loss_train: 1.4882 acc_train: 0.5363 loss_val: 1.4178 acc_val: 0.5616\n",
            "Epoch: 0016 loss_train: 1.4544 acc_train: 0.5508 loss_val: 1.3800 acc_val: 0.5661\n",
            "Epoch: 0017 loss_train: 1.4178 acc_train: 0.5653 loss_val: 1.3417 acc_val: 0.5811\n",
            "Epoch: 0018 loss_train: 1.3827 acc_train: 0.5679 loss_val: 1.3029 acc_val: 0.5916\n",
            "Epoch: 0019 loss_train: 1.3422 acc_train: 0.5924 loss_val: 1.2645 acc_val: 0.6036\n",
            "Epoch: 0020 loss_train: 1.3065 acc_train: 0.6004 loss_val: 1.2274 acc_val: 0.6231\n",
            "Epoch: 0021 loss_train: 1.2680 acc_train: 0.6104 loss_val: 1.1923 acc_val: 0.6291\n",
            "Epoch: 0022 loss_train: 1.2313 acc_train: 0.6209 loss_val: 1.1594 acc_val: 0.6366\n",
            "Epoch: 0023 loss_train: 1.1958 acc_train: 0.6319 loss_val: 1.1288 acc_val: 0.6381\n",
            "Epoch: 0024 loss_train: 1.1618 acc_train: 0.6415 loss_val: 1.1002 acc_val: 0.6562\n",
            "Epoch: 0025 loss_train: 1.1263 acc_train: 0.6520 loss_val: 1.0730 acc_val: 0.6757\n",
            "Epoch: 0026 loss_train: 1.0907 acc_train: 0.6605 loss_val: 1.0471 acc_val: 0.6772\n",
            "Epoch: 0027 loss_train: 1.0550 acc_train: 0.6710 loss_val: 1.0225 acc_val: 0.6877\n",
            "Epoch: 0028 loss_train: 1.0172 acc_train: 0.6830 loss_val: 0.9990 acc_val: 0.6922\n",
            "Epoch: 0029 loss_train: 0.9839 acc_train: 0.6975 loss_val: 0.9766 acc_val: 0.6937\n",
            "Epoch: 0030 loss_train: 0.9474 acc_train: 0.7061 loss_val: 0.9549 acc_val: 0.6967\n",
            "Epoch: 0031 loss_train: 0.9108 acc_train: 0.7186 loss_val: 0.9344 acc_val: 0.7042\n",
            "Epoch: 0032 loss_train: 0.8744 acc_train: 0.7291 loss_val: 0.9153 acc_val: 0.7057\n",
            "Epoch: 0033 loss_train: 0.8383 acc_train: 0.7441 loss_val: 0.8976 acc_val: 0.7057\n",
            "Epoch: 0034 loss_train: 0.8035 acc_train: 0.7471 loss_val: 0.8814 acc_val: 0.7042\n",
            "Epoch: 0035 loss_train: 0.7688 acc_train: 0.7556 loss_val: 0.8682 acc_val: 0.7117\n",
            "Epoch: 0036 loss_train: 0.7331 acc_train: 0.7636 loss_val: 0.8586 acc_val: 0.7162\n",
            "Epoch: 0037 loss_train: 0.6979 acc_train: 0.7812 loss_val: 0.8521 acc_val: 0.7192\n",
            "Epoch: 0038 loss_train: 0.6655 acc_train: 0.7872 loss_val: 0.8481 acc_val: 0.7207\n",
            "Epoch: 0039 loss_train: 0.6302 acc_train: 0.8047 loss_val: 0.8458 acc_val: 0.7327\n",
            "Epoch: 0040 loss_train: 0.6027 acc_train: 0.8082 loss_val: 0.8460 acc_val: 0.7312\n",
            "Epoch: 0041 loss_train: 0.5753 acc_train: 0.8137 loss_val: 0.8499 acc_val: 0.7327\n",
            "Epoch: 0042 loss_train: 0.5475 acc_train: 0.8192 loss_val: 0.8563 acc_val: 0.7357\n",
            "Epoch: 0043 loss_train: 0.5167 acc_train: 0.8302 loss_val: 0.8631 acc_val: 0.7357\n",
            "Epoch: 0044 loss_train: 0.4923 acc_train: 0.8373 loss_val: 0.8719 acc_val: 0.7372\n",
            "Epoch: 0045 loss_train: 0.4649 acc_train: 0.8448 loss_val: 0.8830 acc_val: 0.7372\n",
            "Epoch: 0046 loss_train: 0.4394 acc_train: 0.8538 loss_val: 0.8965 acc_val: 0.7387\n",
            "Epoch: 0047 loss_train: 0.4118 acc_train: 0.8613 loss_val: 0.9103 acc_val: 0.7402\n",
            "Epoch: 0048 loss_train: 0.3902 acc_train: 0.8643 loss_val: 0.9251 acc_val: 0.7387\n",
            "Epoch: 0049 loss_train: 0.3620 acc_train: 0.8773 loss_val: 0.9431 acc_val: 0.7387\n",
            "Epoch: 0050 loss_train: 0.3417 acc_train: 0.8803 loss_val: 0.9641 acc_val: 0.7417\n",
            "Epoch: 0051 loss_train: 0.3155 acc_train: 0.8963 loss_val: 0.9850 acc_val: 0.7402\n",
            "Epoch: 0052 loss_train: 0.2915 acc_train: 0.9044 loss_val: 1.0058 acc_val: 0.7447\n",
            "Epoch: 0053 loss_train: 0.2722 acc_train: 0.9114 loss_val: 1.0289 acc_val: 0.7477\n",
            "Epoch: 0054 loss_train: 0.2525 acc_train: 0.9164 loss_val: 1.0534 acc_val: 0.7417\n",
            "Epoch: 0055 loss_train: 0.2333 acc_train: 0.9274 loss_val: 1.0762 acc_val: 0.7432\n",
            "Epoch: 0056 loss_train: 0.2127 acc_train: 0.9339 loss_val: 1.0986 acc_val: 0.7372\n",
            "Epoch: 0057 loss_train: 0.1950 acc_train: 0.9399 loss_val: 1.1244 acc_val: 0.7357\n",
            "Epoch: 0058 loss_train: 0.1810 acc_train: 0.9424 loss_val: 1.1508 acc_val: 0.7387\n",
            "Epoch: 0059 loss_train: 0.1620 acc_train: 0.9519 loss_val: 1.1739 acc_val: 0.7357\n",
            "Epoch: 0060 loss_train: 0.1478 acc_train: 0.9554 loss_val: 1.1999 acc_val: 0.7297\n",
            "Epoch: 0061 loss_train: 0.1361 acc_train: 0.9584 loss_val: 1.2281 acc_val: 0.7267\n",
            "Epoch: 0062 loss_train: 0.1242 acc_train: 0.9624 loss_val: 1.2533 acc_val: 0.7327\n",
            "Epoch: 0063 loss_train: 0.1160 acc_train: 0.9629 loss_val: 1.2807 acc_val: 0.7357\n",
            "Epoch: 0064 loss_train: 0.1030 acc_train: 0.9700 loss_val: 1.3082 acc_val: 0.7357\n",
            "Epoch: 0065 loss_train: 0.0915 acc_train: 0.9715 loss_val: 1.3351 acc_val: 0.7342\n",
            "Epoch: 0066 loss_train: 0.0847 acc_train: 0.9775 loss_val: 1.3596 acc_val: 0.7357\n",
            "Epoch: 0067 loss_train: 0.0746 acc_train: 0.9810 loss_val: 1.3862 acc_val: 0.7312\n",
            "Epoch: 0068 loss_train: 0.0675 acc_train: 0.9830 loss_val: 1.4113 acc_val: 0.7327\n",
            "Epoch: 0069 loss_train: 0.0608 acc_train: 0.9845 loss_val: 1.4387 acc_val: 0.7327\n",
            "Epoch: 0070 loss_train: 0.0564 acc_train: 0.9845 loss_val: 1.4668 acc_val: 0.7312\n",
            "Epoch: 0071 loss_train: 0.0502 acc_train: 0.9835 loss_val: 1.4937 acc_val: 0.7297\n",
            "Epoch: 0072 loss_train: 0.0439 acc_train: 0.9880 loss_val: 1.5206 acc_val: 0.7327\n",
            "Epoch: 0073 loss_train: 0.0406 acc_train: 0.9885 loss_val: 1.5464 acc_val: 0.7372\n",
            "Epoch: 0074 loss_train: 0.0360 acc_train: 0.9900 loss_val: 1.5708 acc_val: 0.7342\n",
            "Epoch: 0075 loss_train: 0.0334 acc_train: 0.9915 loss_val: 1.5955 acc_val: 0.7327\n",
            "Epoch: 0076 loss_train: 0.0314 acc_train: 0.9915 loss_val: 1.6214 acc_val: 0.7327\n",
            "Epoch: 0077 loss_train: 0.0298 acc_train: 0.9900 loss_val: 1.6479 acc_val: 0.7327\n",
            "Epoch: 0078 loss_train: 0.0261 acc_train: 0.9925 loss_val: 1.6734 acc_val: 0.7312\n",
            "Epoch: 0079 loss_train: 0.0250 acc_train: 0.9950 loss_val: 1.6943 acc_val: 0.7327\n",
            "Epoch: 0080 loss_train: 0.0214 acc_train: 0.9965 loss_val: 1.7124 acc_val: 0.7327\n",
            "Epoch: 0081 loss_train: 0.0198 acc_train: 0.9945 loss_val: 1.7299 acc_val: 0.7327\n",
            "Epoch: 0082 loss_train: 0.0180 acc_train: 0.9950 loss_val: 1.7472 acc_val: 0.7312\n",
            "Epoch: 0083 loss_train: 0.0170 acc_train: 0.9965 loss_val: 1.7652 acc_val: 0.7312\n",
            "Epoch: 0084 loss_train: 0.0154 acc_train: 0.9975 loss_val: 1.7849 acc_val: 0.7312\n",
            "Epoch: 0085 loss_train: 0.0147 acc_train: 0.9970 loss_val: 1.8045 acc_val: 0.7297\n",
            "Epoch: 0086 loss_train: 0.0131 acc_train: 0.9975 loss_val: 1.8231 acc_val: 0.7327\n",
            "Epoch: 0087 loss_train: 0.0127 acc_train: 0.9975 loss_val: 1.8412 acc_val: 0.7312\n",
            "Epoch: 0088 loss_train: 0.0111 acc_train: 0.9975 loss_val: 1.8588 acc_val: 0.7327\n",
            "Epoch: 0089 loss_train: 0.0104 acc_train: 0.9980 loss_val: 1.8742 acc_val: 0.7297\n",
            "Epoch: 0090 loss_train: 0.0097 acc_train: 0.9985 loss_val: 1.8893 acc_val: 0.7297\n",
            "Epoch: 0091 loss_train: 0.0090 acc_train: 0.9980 loss_val: 1.9039 acc_val: 0.7297\n",
            "Epoch: 0092 loss_train: 0.0079 acc_train: 0.9985 loss_val: 1.9177 acc_val: 0.7282\n",
            "Epoch: 0093 loss_train: 0.0077 acc_train: 0.9980 loss_val: 1.9306 acc_val: 0.7267\n",
            "Epoch: 0094 loss_train: 0.0075 acc_train: 0.9985 loss_val: 1.9435 acc_val: 0.7267\n",
            "Epoch: 0095 loss_train: 0.0066 acc_train: 0.9985 loss_val: 1.9557 acc_val: 0.7297\n",
            "Epoch: 0096 loss_train: 0.0060 acc_train: 0.9990 loss_val: 1.9668 acc_val: 0.7297\n",
            "Epoch: 0097 loss_train: 0.0055 acc_train: 0.9985 loss_val: 1.9761 acc_val: 0.7267\n",
            "Epoch: 0098 loss_train: 0.0052 acc_train: 0.9995 loss_val: 1.9844 acc_val: 0.7312\n",
            "Epoch: 0099 loss_train: 0.0047 acc_train: 0.9995 loss_val: 1.9925 acc_val: 0.7327\n",
            "Epoch: 0100 loss_train: 0.0047 acc_train: 0.9990 loss_val: 2.0022 acc_val: 0.7327\n",
            "Epoch: 0101 loss_train: 0.0038 acc_train: 0.9995 loss_val: 2.0112 acc_val: 0.7297\n",
            "Epoch: 0102 loss_train: 0.0037 acc_train: 0.9995 loss_val: 2.0193 acc_val: 0.7282\n",
            "Epoch: 0103 loss_train: 0.0039 acc_train: 0.9990 loss_val: 2.0280 acc_val: 0.7297\n",
            "Optimization Finished!\n",
            "Train cost: 32.4745s\n",
            "Loading 53th epoch\n",
            "Test set results: loss= 0.7563 accuracy= 0.7892\n",
            "  NumNodes: 3327\n",
            "  NumEdges: 9228\n",
            "  NumFeats: 3703\n",
            "  NumClasses: 6\n",
            "  NumTrainingSamples: 120\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:354: DGLWarning: DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\n",
            "  dgl_warning(\"DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\")\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=3743, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=6, bias=True)\n",
            ")\n",
            "total params: 4154632\n",
            "Epoch: 0001 loss_train: 1.8282 acc_train: 0.1302 loss_val: 1.8213 acc_val: 0.1471\n",
            "Epoch: 0002 loss_train: 1.8214 acc_train: 0.1417 loss_val: 1.8113 acc_val: 0.1502\n",
            "Epoch: 0003 loss_train: 1.8145 acc_train: 0.1457 loss_val: 1.7968 acc_val: 0.1622\n",
            "Epoch: 0004 loss_train: 1.8013 acc_train: 0.1607 loss_val: 1.7779 acc_val: 0.1952\n",
            "Epoch: 0005 loss_train: 1.7862 acc_train: 0.1768 loss_val: 1.7553 acc_val: 0.2297\n",
            "Epoch: 0006 loss_train: 1.7666 acc_train: 0.2083 loss_val: 1.7296 acc_val: 0.3348\n",
            "Epoch: 0007 loss_train: 1.7462 acc_train: 0.2604 loss_val: 1.7013 acc_val: 0.4099\n",
            "Epoch: 0008 loss_train: 1.7227 acc_train: 0.3365 loss_val: 1.6711 acc_val: 0.4760\n",
            "Epoch: 0009 loss_train: 1.6996 acc_train: 0.3991 loss_val: 1.6397 acc_val: 0.5090\n",
            "Epoch: 0010 loss_train: 1.6732 acc_train: 0.4337 loss_val: 1.6071 acc_val: 0.5195\n",
            "Epoch: 0011 loss_train: 1.6463 acc_train: 0.4567 loss_val: 1.5734 acc_val: 0.5360\n",
            "Epoch: 0012 loss_train: 1.6169 acc_train: 0.4807 loss_val: 1.5387 acc_val: 0.5330\n",
            "Epoch: 0013 loss_train: 1.5905 acc_train: 0.4892 loss_val: 1.5027 acc_val: 0.5450\n",
            "Epoch: 0014 loss_train: 1.5607 acc_train: 0.5018 loss_val: 1.4652 acc_val: 0.5541\n",
            "Epoch: 0015 loss_train: 1.5275 acc_train: 0.5178 loss_val: 1.4261 acc_val: 0.5706\n",
            "Epoch: 0016 loss_train: 1.4939 acc_train: 0.5358 loss_val: 1.3859 acc_val: 0.5856\n",
            "Epoch: 0017 loss_train: 1.4580 acc_train: 0.5388 loss_val: 1.3453 acc_val: 0.5991\n",
            "Epoch: 0018 loss_train: 1.4222 acc_train: 0.5518 loss_val: 1.3051 acc_val: 0.6111\n",
            "Epoch: 0019 loss_train: 1.3816 acc_train: 0.5689 loss_val: 1.2662 acc_val: 0.6231\n",
            "Epoch: 0020 loss_train: 1.3444 acc_train: 0.5859 loss_val: 1.2294 acc_val: 0.6291\n",
            "Epoch: 0021 loss_train: 1.3074 acc_train: 0.5894 loss_val: 1.1948 acc_val: 0.6351\n",
            "Epoch: 0022 loss_train: 1.2693 acc_train: 0.6014 loss_val: 1.1620 acc_val: 0.6502\n",
            "Epoch: 0023 loss_train: 1.2303 acc_train: 0.6149 loss_val: 1.1310 acc_val: 0.6577\n",
            "Epoch: 0024 loss_train: 1.1962 acc_train: 0.6234 loss_val: 1.1014 acc_val: 0.6667\n",
            "Epoch: 0025 loss_train: 1.1567 acc_train: 0.6370 loss_val: 1.0732 acc_val: 0.6727\n",
            "Epoch: 0026 loss_train: 1.1189 acc_train: 0.6485 loss_val: 1.0459 acc_val: 0.6802\n",
            "Epoch: 0027 loss_train: 1.0813 acc_train: 0.6600 loss_val: 1.0195 acc_val: 0.6847\n",
            "Epoch: 0028 loss_train: 1.0410 acc_train: 0.6760 loss_val: 0.9942 acc_val: 0.6907\n",
            "Epoch: 0029 loss_train: 0.9983 acc_train: 0.6925 loss_val: 0.9703 acc_val: 0.6952\n",
            "Epoch: 0030 loss_train: 0.9609 acc_train: 0.7061 loss_val: 0.9475 acc_val: 0.7042\n",
            "Epoch: 0031 loss_train: 0.9159 acc_train: 0.7181 loss_val: 0.9257 acc_val: 0.7147\n",
            "Epoch: 0032 loss_train: 0.8753 acc_train: 0.7291 loss_val: 0.9055 acc_val: 0.7177\n",
            "Epoch: 0033 loss_train: 0.8338 acc_train: 0.7426 loss_val: 0.8878 acc_val: 0.7237\n",
            "Epoch: 0034 loss_train: 0.7966 acc_train: 0.7516 loss_val: 0.8735 acc_val: 0.7207\n",
            "Epoch: 0035 loss_train: 0.7589 acc_train: 0.7656 loss_val: 0.8623 acc_val: 0.7207\n",
            "Epoch: 0036 loss_train: 0.7183 acc_train: 0.7742 loss_val: 0.8521 acc_val: 0.7297\n",
            "Epoch: 0037 loss_train: 0.6841 acc_train: 0.7822 loss_val: 0.8443 acc_val: 0.7282\n",
            "Epoch: 0038 loss_train: 0.6499 acc_train: 0.7942 loss_val: 0.8397 acc_val: 0.7342\n",
            "Epoch: 0039 loss_train: 0.6154 acc_train: 0.8032 loss_val: 0.8386 acc_val: 0.7357\n",
            "Epoch: 0040 loss_train: 0.5855 acc_train: 0.8122 loss_val: 0.8397 acc_val: 0.7357\n",
            "Epoch: 0041 loss_train: 0.5552 acc_train: 0.8242 loss_val: 0.8428 acc_val: 0.7327\n",
            "Epoch: 0042 loss_train: 0.5295 acc_train: 0.8292 loss_val: 0.8473 acc_val: 0.7312\n",
            "Epoch: 0043 loss_train: 0.4977 acc_train: 0.8368 loss_val: 0.8535 acc_val: 0.7327\n",
            "Epoch: 0044 loss_train: 0.4730 acc_train: 0.8423 loss_val: 0.8613 acc_val: 0.7372\n",
            "Epoch: 0045 loss_train: 0.4441 acc_train: 0.8518 loss_val: 0.8714 acc_val: 0.7372\n",
            "Epoch: 0046 loss_train: 0.4164 acc_train: 0.8628 loss_val: 0.8828 acc_val: 0.7357\n",
            "Epoch: 0047 loss_train: 0.3905 acc_train: 0.8733 loss_val: 0.8953 acc_val: 0.7357\n",
            "Epoch: 0048 loss_train: 0.3684 acc_train: 0.8783 loss_val: 0.9101 acc_val: 0.7357\n",
            "Epoch: 0049 loss_train: 0.3421 acc_train: 0.8863 loss_val: 0.9280 acc_val: 0.7312\n",
            "Epoch: 0050 loss_train: 0.3201 acc_train: 0.8953 loss_val: 0.9482 acc_val: 0.7327\n",
            "Epoch: 0051 loss_train: 0.2956 acc_train: 0.9024 loss_val: 0.9685 acc_val: 0.7312\n",
            "Epoch: 0052 loss_train: 0.2727 acc_train: 0.9129 loss_val: 0.9915 acc_val: 0.7327\n",
            "Epoch: 0053 loss_train: 0.2528 acc_train: 0.9179 loss_val: 1.0164 acc_val: 0.7342\n",
            "Epoch: 0054 loss_train: 0.2348 acc_train: 0.9219 loss_val: 1.0383 acc_val: 0.7387\n",
            "Epoch: 0055 loss_train: 0.2149 acc_train: 0.9324 loss_val: 1.0613 acc_val: 0.7387\n",
            "Epoch: 0056 loss_train: 0.1997 acc_train: 0.9374 loss_val: 1.0862 acc_val: 0.7402\n",
            "Epoch: 0057 loss_train: 0.1824 acc_train: 0.9419 loss_val: 1.1117 acc_val: 0.7492\n",
            "Epoch: 0058 loss_train: 0.1681 acc_train: 0.9489 loss_val: 1.1367 acc_val: 0.7523\n",
            "Epoch: 0059 loss_train: 0.1526 acc_train: 0.9544 loss_val: 1.1622 acc_val: 0.7477\n",
            "Epoch: 0060 loss_train: 0.1378 acc_train: 0.9624 loss_val: 1.1918 acc_val: 0.7432\n",
            "Epoch: 0061 loss_train: 0.1276 acc_train: 0.9604 loss_val: 1.2229 acc_val: 0.7387\n",
            "Epoch: 0062 loss_train: 0.1161 acc_train: 0.9690 loss_val: 1.2501 acc_val: 0.7402\n",
            "Epoch: 0063 loss_train: 0.1082 acc_train: 0.9710 loss_val: 1.2793 acc_val: 0.7372\n",
            "Epoch: 0064 loss_train: 0.0976 acc_train: 0.9730 loss_val: 1.3113 acc_val: 0.7297\n",
            "Epoch: 0065 loss_train: 0.0875 acc_train: 0.9770 loss_val: 1.3430 acc_val: 0.7327\n",
            "Epoch: 0066 loss_train: 0.0799 acc_train: 0.9800 loss_val: 1.3731 acc_val: 0.7312\n",
            "Epoch: 0067 loss_train: 0.0707 acc_train: 0.9845 loss_val: 1.4054 acc_val: 0.7327\n",
            "Epoch: 0068 loss_train: 0.0646 acc_train: 0.9835 loss_val: 1.4342 acc_val: 0.7342\n",
            "Epoch: 0069 loss_train: 0.0591 acc_train: 0.9825 loss_val: 1.4620 acc_val: 0.7327\n",
            "Epoch: 0070 loss_train: 0.0550 acc_train: 0.9845 loss_val: 1.4922 acc_val: 0.7357\n",
            "Epoch: 0071 loss_train: 0.0510 acc_train: 0.9830 loss_val: 1.5213 acc_val: 0.7372\n",
            "Epoch: 0072 loss_train: 0.0460 acc_train: 0.9845 loss_val: 1.5507 acc_val: 0.7342\n",
            "Epoch: 0073 loss_train: 0.0427 acc_train: 0.9855 loss_val: 1.5797 acc_val: 0.7357\n",
            "Epoch: 0074 loss_train: 0.0377 acc_train: 0.9905 loss_val: 1.6050 acc_val: 0.7357\n",
            "Epoch: 0075 loss_train: 0.0346 acc_train: 0.9910 loss_val: 1.6284 acc_val: 0.7342\n",
            "Epoch: 0076 loss_train: 0.0325 acc_train: 0.9920 loss_val: 1.6526 acc_val: 0.7297\n",
            "Epoch: 0077 loss_train: 0.0299 acc_train: 0.9915 loss_val: 1.6792 acc_val: 0.7312\n",
            "Epoch: 0078 loss_train: 0.0269 acc_train: 0.9915 loss_val: 1.7054 acc_val: 0.7327\n",
            "Epoch: 0079 loss_train: 0.0250 acc_train: 0.9935 loss_val: 1.7290 acc_val: 0.7342\n",
            "Epoch: 0080 loss_train: 0.0227 acc_train: 0.9925 loss_val: 1.7514 acc_val: 0.7312\n",
            "Epoch: 0081 loss_train: 0.0207 acc_train: 0.9940 loss_val: 1.7734 acc_val: 0.7327\n",
            "Epoch: 0082 loss_train: 0.0192 acc_train: 0.9945 loss_val: 1.7957 acc_val: 0.7327\n",
            "Epoch: 0083 loss_train: 0.0178 acc_train: 0.9955 loss_val: 1.8177 acc_val: 0.7327\n",
            "Epoch: 0084 loss_train: 0.0172 acc_train: 0.9940 loss_val: 1.8382 acc_val: 0.7327\n",
            "Epoch: 0085 loss_train: 0.0154 acc_train: 0.9960 loss_val: 1.8553 acc_val: 0.7312\n",
            "Epoch: 0086 loss_train: 0.0136 acc_train: 0.9965 loss_val: 1.8711 acc_val: 0.7327\n",
            "Epoch: 0087 loss_train: 0.0131 acc_train: 0.9970 loss_val: 1.8890 acc_val: 0.7327\n",
            "Epoch: 0088 loss_train: 0.0123 acc_train: 0.9970 loss_val: 1.9065 acc_val: 0.7342\n",
            "Epoch: 0089 loss_train: 0.0113 acc_train: 0.9975 loss_val: 1.9253 acc_val: 0.7312\n",
            "Epoch: 0090 loss_train: 0.0109 acc_train: 0.9975 loss_val: 1.9422 acc_val: 0.7327\n",
            "Epoch: 0091 loss_train: 0.0103 acc_train: 0.9985 loss_val: 1.9536 acc_val: 0.7327\n",
            "Epoch: 0092 loss_train: 0.0086 acc_train: 0.9990 loss_val: 1.9641 acc_val: 0.7312\n",
            "Epoch: 0093 loss_train: 0.0087 acc_train: 0.9980 loss_val: 1.9804 acc_val: 0.7312\n",
            "Epoch: 0094 loss_train: 0.0085 acc_train: 0.9975 loss_val: 1.9963 acc_val: 0.7312\n",
            "Epoch: 0095 loss_train: 0.0069 acc_train: 0.9985 loss_val: 2.0102 acc_val: 0.7282\n",
            "Epoch: 0096 loss_train: 0.0067 acc_train: 0.9985 loss_val: 2.0180 acc_val: 0.7312\n",
            "Epoch: 0097 loss_train: 0.0062 acc_train: 0.9980 loss_val: 2.0253 acc_val: 0.7357\n",
            "Epoch: 0098 loss_train: 0.0062 acc_train: 0.9985 loss_val: 2.0356 acc_val: 0.7357\n",
            "Epoch: 0099 loss_train: 0.0054 acc_train: 0.9980 loss_val: 2.0469 acc_val: 0.7357\n",
            "Epoch: 0100 loss_train: 0.0051 acc_train: 0.9985 loss_val: 2.0550 acc_val: 0.7327\n",
            "Epoch: 0101 loss_train: 0.0047 acc_train: 0.9990 loss_val: 2.0645 acc_val: 0.7327\n",
            "Epoch: 0102 loss_train: 0.0042 acc_train: 0.9990 loss_val: 2.0754 acc_val: 0.7342\n",
            "Epoch: 0103 loss_train: 0.0045 acc_train: 0.9985 loss_val: 2.0867 acc_val: 0.7327\n",
            "Epoch: 0104 loss_train: 0.0040 acc_train: 0.9990 loss_val: 2.0959 acc_val: 0.7327\n",
            "Epoch: 0105 loss_train: 0.0038 acc_train: 0.9985 loss_val: 2.1048 acc_val: 0.7297\n",
            "Epoch: 0106 loss_train: 0.0036 acc_train: 0.9985 loss_val: 2.1136 acc_val: 0.7297\n",
            "Epoch: 0107 loss_train: 0.0035 acc_train: 0.9995 loss_val: 2.1189 acc_val: 0.7312\n",
            "Epoch: 0108 loss_train: 0.0033 acc_train: 0.9990 loss_val: 2.1215 acc_val: 0.7297\n",
            "Optimization Finished!\n",
            "Train cost: 34.7586s\n",
            "Loading 58th epoch\n",
            "Test set results: loss= 0.8748 accuracy= 0.7696\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --dataset citeseer --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 5  --n_heads 8 --n_layers 1 --pe_dim 40 --peak_lr 0.001  --attack none --attperc 20 --TrainTest train --weight_decay=1e-05\n",
        "\n",
        "!python train.py --dataset citeseer --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 5  --n_heads 8 --n_layers 1 --pe_dim 40 --peak_lr 0.001  --attack l2_strong --attperc 10 --TrainTest train --weight_decay=1e-05\n",
        "\n",
        "!python train.py --dataset citeseer --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 5  --n_heads 8 --n_layers 1 --pe_dim 40 --peak_lr 0.001  --attack l2_strong --attperc 40 --TrainTest train --weight_decay=1e-05\n",
        "\n",
        "!python train.py --dataset citeseer --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 5  --n_heads 8 --n_layers 1 --pe_dim 40 --peak_lr 0.001  --attack l2_strong --attperc 60 --TrainTest train --weight_decay=1e-05\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBro8iTHaOGa",
        "outputId": "26315044-92c7-4cba-95a9-b4f38ea325c6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NumNodes: 3327\n",
            "  NumEdges: 9228\n",
            "  NumFeats: 3703\n",
            "  NumClasses: 6\n",
            "  NumTrainingSamples: 120\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=3743, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=6, bias=True)\n",
            ")\n",
            "total params: 4154632\n",
            "Epoch: 0001 loss_train: 1.8342 acc_train: 0.1412 loss_val: 1.8246 acc_val: 0.1757\n",
            "Epoch: 0002 loss_train: 1.8278 acc_train: 0.1527 loss_val: 1.8145 acc_val: 0.1892\n",
            "Epoch: 0003 loss_train: 1.8206 acc_train: 0.1592 loss_val: 1.7996 acc_val: 0.1877\n",
            "Epoch: 0004 loss_train: 1.8050 acc_train: 0.1738 loss_val: 1.7804 acc_val: 0.1982\n",
            "Epoch: 0005 loss_train: 1.7892 acc_train: 0.1948 loss_val: 1.7574 acc_val: 0.2598\n",
            "Epoch: 0006 loss_train: 1.7646 acc_train: 0.2323 loss_val: 1.7310 acc_val: 0.3273\n",
            "Epoch: 0007 loss_train: 1.7381 acc_train: 0.2969 loss_val: 1.7019 acc_val: 0.4054\n",
            "Epoch: 0008 loss_train: 1.7107 acc_train: 0.3796 loss_val: 1.6705 acc_val: 0.4745\n",
            "Epoch: 0009 loss_train: 1.6795 acc_train: 0.4687 loss_val: 1.6371 acc_val: 0.5180\n",
            "Epoch: 0010 loss_train: 1.6443 acc_train: 0.5238 loss_val: 1.6021 acc_val: 0.5450\n",
            "Epoch: 0011 loss_train: 1.6093 acc_train: 0.5633 loss_val: 1.5659 acc_val: 0.5631\n",
            "Epoch: 0012 loss_train: 1.5712 acc_train: 0.5779 loss_val: 1.5283 acc_val: 0.5751\n",
            "Epoch: 0013 loss_train: 1.5338 acc_train: 0.5939 loss_val: 1.4895 acc_val: 0.5856\n",
            "Epoch: 0014 loss_train: 1.4936 acc_train: 0.6134 loss_val: 1.4499 acc_val: 0.6006\n",
            "Epoch: 0015 loss_train: 1.4514 acc_train: 0.6254 loss_val: 1.4097 acc_val: 0.6126\n",
            "Epoch: 0016 loss_train: 1.4086 acc_train: 0.6304 loss_val: 1.3691 acc_val: 0.6201\n",
            "Epoch: 0017 loss_train: 1.3665 acc_train: 0.6485 loss_val: 1.3284 acc_val: 0.6276\n",
            "Epoch: 0018 loss_train: 1.3224 acc_train: 0.6550 loss_val: 1.2880 acc_val: 0.6396\n",
            "Epoch: 0019 loss_train: 1.2805 acc_train: 0.6660 loss_val: 1.2483 acc_val: 0.6456\n",
            "Epoch: 0020 loss_train: 1.2373 acc_train: 0.6730 loss_val: 1.2094 acc_val: 0.6562\n",
            "Epoch: 0021 loss_train: 1.1942 acc_train: 0.6790 loss_val: 1.1717 acc_val: 0.6682\n",
            "Epoch: 0022 loss_train: 1.1514 acc_train: 0.6960 loss_val: 1.1356 acc_val: 0.6892\n",
            "Epoch: 0023 loss_train: 1.1101 acc_train: 0.7031 loss_val: 1.1012 acc_val: 0.6922\n",
            "Epoch: 0024 loss_train: 1.0701 acc_train: 0.7111 loss_val: 1.0689 acc_val: 0.6937\n",
            "Epoch: 0025 loss_train: 1.0313 acc_train: 0.7171 loss_val: 1.0388 acc_val: 0.6982\n",
            "Epoch: 0026 loss_train: 0.9938 acc_train: 0.7266 loss_val: 1.0108 acc_val: 0.7042\n",
            "Epoch: 0027 loss_train: 0.9575 acc_train: 0.7276 loss_val: 0.9850 acc_val: 0.7042\n",
            "Epoch: 0028 loss_train: 0.9233 acc_train: 0.7336 loss_val: 0.9612 acc_val: 0.7132\n",
            "Epoch: 0029 loss_train: 0.8901 acc_train: 0.7391 loss_val: 0.9397 acc_val: 0.7162\n",
            "Epoch: 0030 loss_train: 0.8563 acc_train: 0.7456 loss_val: 0.9205 acc_val: 0.7192\n",
            "Epoch: 0031 loss_train: 0.8228 acc_train: 0.7531 loss_val: 0.9037 acc_val: 0.7207\n",
            "Epoch: 0032 loss_train: 0.7948 acc_train: 0.7616 loss_val: 0.8892 acc_val: 0.7192\n",
            "Epoch: 0033 loss_train: 0.7652 acc_train: 0.7682 loss_val: 0.8765 acc_val: 0.7207\n",
            "Epoch: 0034 loss_train: 0.7386 acc_train: 0.7722 loss_val: 0.8653 acc_val: 0.7207\n",
            "Epoch: 0035 loss_train: 0.7101 acc_train: 0.7777 loss_val: 0.8560 acc_val: 0.7237\n",
            "Epoch: 0036 loss_train: 0.6839 acc_train: 0.7837 loss_val: 0.8485 acc_val: 0.7267\n",
            "Epoch: 0037 loss_train: 0.6595 acc_train: 0.7867 loss_val: 0.8427 acc_val: 0.7237\n",
            "Epoch: 0038 loss_train: 0.6345 acc_train: 0.7972 loss_val: 0.8381 acc_val: 0.7357\n",
            "Epoch: 0039 loss_train: 0.6103 acc_train: 0.8067 loss_val: 0.8349 acc_val: 0.7372\n",
            "Epoch: 0040 loss_train: 0.5898 acc_train: 0.8137 loss_val: 0.8334 acc_val: 0.7462\n",
            "Epoch: 0041 loss_train: 0.5698 acc_train: 0.8167 loss_val: 0.8337 acc_val: 0.7462\n",
            "Epoch: 0042 loss_train: 0.5483 acc_train: 0.8212 loss_val: 0.8358 acc_val: 0.7477\n",
            "Epoch: 0043 loss_train: 0.5265 acc_train: 0.8247 loss_val: 0.8397 acc_val: 0.7508\n",
            "Epoch: 0044 loss_train: 0.5079 acc_train: 0.8292 loss_val: 0.8456 acc_val: 0.7492\n",
            "Epoch: 0045 loss_train: 0.4877 acc_train: 0.8322 loss_val: 0.8525 acc_val: 0.7447\n",
            "Epoch: 0046 loss_train: 0.4702 acc_train: 0.8388 loss_val: 0.8603 acc_val: 0.7477\n",
            "Epoch: 0047 loss_train: 0.4494 acc_train: 0.8478 loss_val: 0.8698 acc_val: 0.7462\n",
            "Epoch: 0048 loss_train: 0.4337 acc_train: 0.8533 loss_val: 0.8809 acc_val: 0.7417\n",
            "Epoch: 0049 loss_train: 0.4156 acc_train: 0.8578 loss_val: 0.8936 acc_val: 0.7402\n",
            "Epoch: 0050 loss_train: 0.3995 acc_train: 0.8653 loss_val: 0.9074 acc_val: 0.7417\n",
            "Epoch: 0051 loss_train: 0.3842 acc_train: 0.8643 loss_val: 0.9222 acc_val: 0.7372\n",
            "Epoch: 0052 loss_train: 0.3657 acc_train: 0.8773 loss_val: 0.9390 acc_val: 0.7357\n",
            "Epoch: 0053 loss_train: 0.3496 acc_train: 0.8808 loss_val: 0.9587 acc_val: 0.7372\n",
            "Epoch: 0054 loss_train: 0.3342 acc_train: 0.8848 loss_val: 0.9803 acc_val: 0.7312\n",
            "Epoch: 0055 loss_train: 0.3183 acc_train: 0.8918 loss_val: 1.0022 acc_val: 0.7282\n",
            "Epoch: 0056 loss_train: 0.2997 acc_train: 0.9004 loss_val: 1.0245 acc_val: 0.7297\n",
            "Epoch: 0057 loss_train: 0.2826 acc_train: 0.8998 loss_val: 1.0471 acc_val: 0.7282\n",
            "Epoch: 0058 loss_train: 0.2705 acc_train: 0.9014 loss_val: 1.0700 acc_val: 0.7312\n",
            "Epoch: 0059 loss_train: 0.2493 acc_train: 0.9119 loss_val: 1.0930 acc_val: 0.7342\n",
            "Epoch: 0060 loss_train: 0.2338 acc_train: 0.9204 loss_val: 1.1163 acc_val: 0.7342\n",
            "Epoch: 0061 loss_train: 0.2192 acc_train: 0.9274 loss_val: 1.1416 acc_val: 0.7357\n",
            "Epoch: 0062 loss_train: 0.2034 acc_train: 0.9359 loss_val: 1.1693 acc_val: 0.7387\n",
            "Epoch: 0063 loss_train: 0.1880 acc_train: 0.9439 loss_val: 1.1976 acc_val: 0.7432\n",
            "Epoch: 0064 loss_train: 0.1752 acc_train: 0.9464 loss_val: 1.2253 acc_val: 0.7387\n",
            "Epoch: 0065 loss_train: 0.1605 acc_train: 0.9549 loss_val: 1.2564 acc_val: 0.7402\n",
            "Epoch: 0066 loss_train: 0.1493 acc_train: 0.9514 loss_val: 1.2885 acc_val: 0.7387\n",
            "Epoch: 0067 loss_train: 0.1369 acc_train: 0.9629 loss_val: 1.3184 acc_val: 0.7402\n",
            "Epoch: 0068 loss_train: 0.1237 acc_train: 0.9619 loss_val: 1.3475 acc_val: 0.7387\n",
            "Epoch: 0069 loss_train: 0.1141 acc_train: 0.9670 loss_val: 1.3774 acc_val: 0.7372\n",
            "Epoch: 0070 loss_train: 0.1046 acc_train: 0.9685 loss_val: 1.4064 acc_val: 0.7372\n",
            "Epoch: 0071 loss_train: 0.0957 acc_train: 0.9710 loss_val: 1.4324 acc_val: 0.7327\n",
            "Epoch: 0072 loss_train: 0.0864 acc_train: 0.9725 loss_val: 1.4580 acc_val: 0.7282\n",
            "Epoch: 0073 loss_train: 0.0794 acc_train: 0.9745 loss_val: 1.4830 acc_val: 0.7297\n",
            "Epoch: 0074 loss_train: 0.0719 acc_train: 0.9775 loss_val: 1.5067 acc_val: 0.7282\n",
            "Epoch: 0075 loss_train: 0.0650 acc_train: 0.9810 loss_val: 1.5305 acc_val: 0.7297\n",
            "Epoch: 0076 loss_train: 0.0593 acc_train: 0.9825 loss_val: 1.5557 acc_val: 0.7282\n",
            "Epoch: 0077 loss_train: 0.0555 acc_train: 0.9840 loss_val: 1.5821 acc_val: 0.7252\n",
            "Epoch: 0078 loss_train: 0.0480 acc_train: 0.9870 loss_val: 1.6092 acc_val: 0.7267\n",
            "Epoch: 0079 loss_train: 0.0456 acc_train: 0.9875 loss_val: 1.6336 acc_val: 0.7282\n",
            "Epoch: 0080 loss_train: 0.0405 acc_train: 0.9875 loss_val: 1.6592 acc_val: 0.7267\n",
            "Epoch: 0081 loss_train: 0.0359 acc_train: 0.9900 loss_val: 1.6857 acc_val: 0.7282\n",
            "Epoch: 0082 loss_train: 0.0342 acc_train: 0.9925 loss_val: 1.7128 acc_val: 0.7267\n",
            "Epoch: 0083 loss_train: 0.0307 acc_train: 0.9915 loss_val: 1.7375 acc_val: 0.7252\n",
            "Epoch: 0084 loss_train: 0.0291 acc_train: 0.9915 loss_val: 1.7580 acc_val: 0.7237\n",
            "Epoch: 0085 loss_train: 0.0253 acc_train: 0.9950 loss_val: 1.7781 acc_val: 0.7252\n",
            "Epoch: 0086 loss_train: 0.0240 acc_train: 0.9915 loss_val: 1.8004 acc_val: 0.7267\n",
            "Epoch: 0087 loss_train: 0.0223 acc_train: 0.9945 loss_val: 1.8231 acc_val: 0.7222\n",
            "Epoch: 0088 loss_train: 0.0203 acc_train: 0.9950 loss_val: 1.8434 acc_val: 0.7237\n",
            "Epoch: 0089 loss_train: 0.0180 acc_train: 0.9960 loss_val: 1.8600 acc_val: 0.7237\n",
            "Epoch: 0090 loss_train: 0.0178 acc_train: 0.9945 loss_val: 1.8726 acc_val: 0.7267\n",
            "Epoch: 0091 loss_train: 0.0173 acc_train: 0.9950 loss_val: 1.8876 acc_val: 0.7297\n",
            "Epoch: 0092 loss_train: 0.0151 acc_train: 0.9950 loss_val: 1.9051 acc_val: 0.7282\n",
            "Epoch: 0093 loss_train: 0.0131 acc_train: 0.9965 loss_val: 1.9223 acc_val: 0.7282\n",
            "Optimization Finished!\n",
            "Train cost: 30.0862s\n",
            "Loading 43th epoch\n",
            "Test set results: loss= 0.7103 accuracy= 0.7831\n",
            "  NumNodes: 3327\n",
            "  NumEdges: 9228\n",
            "  NumFeats: 3703\n",
            "  NumClasses: 6\n",
            "  NumTrainingSamples: 120\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:354: DGLWarning: DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\n",
            "  dgl_warning(\"DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\")\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=3743, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=6, bias=True)\n",
            ")\n",
            "total params: 4154632\n",
            "Epoch: 0001 loss_train: 1.8171 acc_train: 0.1407 loss_val: 1.8039 acc_val: 0.1622\n",
            "Epoch: 0002 loss_train: 1.8108 acc_train: 0.1412 loss_val: 1.7937 acc_val: 0.1712\n",
            "Epoch: 0003 loss_train: 1.8024 acc_train: 0.1637 loss_val: 1.7788 acc_val: 0.1997\n",
            "Epoch: 0004 loss_train: 1.7859 acc_train: 0.1848 loss_val: 1.7596 acc_val: 0.2237\n",
            "Epoch: 0005 loss_train: 1.7700 acc_train: 0.2193 loss_val: 1.7364 acc_val: 0.2613\n",
            "Epoch: 0006 loss_train: 1.7468 acc_train: 0.2824 loss_val: 1.7100 acc_val: 0.3348\n",
            "Epoch: 0007 loss_train: 1.7196 acc_train: 0.3395 loss_val: 1.6808 acc_val: 0.4459\n",
            "Epoch: 0008 loss_train: 1.6933 acc_train: 0.4171 loss_val: 1.6494 acc_val: 0.4895\n",
            "Epoch: 0009 loss_train: 1.6637 acc_train: 0.4587 loss_val: 1.6161 acc_val: 0.4970\n",
            "Epoch: 0010 loss_train: 1.6295 acc_train: 0.5068 loss_val: 1.5812 acc_val: 0.5255\n",
            "Epoch: 0011 loss_train: 1.5961 acc_train: 0.5363 loss_val: 1.5450 acc_val: 0.5465\n",
            "Epoch: 0012 loss_train: 1.5581 acc_train: 0.5643 loss_val: 1.5079 acc_val: 0.5736\n",
            "Epoch: 0013 loss_train: 1.5229 acc_train: 0.5839 loss_val: 1.4698 acc_val: 0.5856\n",
            "Epoch: 0014 loss_train: 1.4846 acc_train: 0.5869 loss_val: 1.4311 acc_val: 0.5976\n",
            "Epoch: 0015 loss_train: 1.4440 acc_train: 0.6104 loss_val: 1.3916 acc_val: 0.6081\n",
            "Epoch: 0016 loss_train: 1.4039 acc_train: 0.6169 loss_val: 1.3516 acc_val: 0.6246\n",
            "Epoch: 0017 loss_train: 1.3628 acc_train: 0.6324 loss_val: 1.3116 acc_val: 0.6276\n",
            "Epoch: 0018 loss_train: 1.3242 acc_train: 0.6405 loss_val: 1.2719 acc_val: 0.6411\n",
            "Epoch: 0019 loss_train: 1.2807 acc_train: 0.6510 loss_val: 1.2330 acc_val: 0.6441\n",
            "Epoch: 0020 loss_train: 1.2395 acc_train: 0.6540 loss_val: 1.1953 acc_val: 0.6532\n",
            "Epoch: 0021 loss_train: 1.1992 acc_train: 0.6670 loss_val: 1.1590 acc_val: 0.6577\n",
            "Epoch: 0022 loss_train: 1.1559 acc_train: 0.6775 loss_val: 1.1243 acc_val: 0.6637\n",
            "Epoch: 0023 loss_train: 1.1186 acc_train: 0.6820 loss_val: 1.0917 acc_val: 0.6817\n",
            "Epoch: 0024 loss_train: 1.0806 acc_train: 0.6885 loss_val: 1.0614 acc_val: 0.6892\n",
            "Epoch: 0025 loss_train: 1.0435 acc_train: 0.6960 loss_val: 1.0331 acc_val: 0.6997\n",
            "Epoch: 0026 loss_train: 1.0072 acc_train: 0.7061 loss_val: 1.0068 acc_val: 0.6997\n",
            "Epoch: 0027 loss_train: 0.9742 acc_train: 0.7181 loss_val: 0.9829 acc_val: 0.7042\n",
            "Epoch: 0028 loss_train: 0.9395 acc_train: 0.7276 loss_val: 0.9613 acc_val: 0.7042\n",
            "Epoch: 0029 loss_train: 0.9077 acc_train: 0.7366 loss_val: 0.9422 acc_val: 0.7102\n",
            "Epoch: 0030 loss_train: 0.8758 acc_train: 0.7406 loss_val: 0.9254 acc_val: 0.7132\n",
            "Epoch: 0031 loss_train: 0.8419 acc_train: 0.7521 loss_val: 0.9105 acc_val: 0.7132\n",
            "Epoch: 0032 loss_train: 0.8151 acc_train: 0.7586 loss_val: 0.8973 acc_val: 0.7102\n",
            "Epoch: 0033 loss_train: 0.7830 acc_train: 0.7631 loss_val: 0.8853 acc_val: 0.7147\n",
            "Epoch: 0034 loss_train: 0.7545 acc_train: 0.7752 loss_val: 0.8745 acc_val: 0.7102\n",
            "Epoch: 0035 loss_train: 0.7290 acc_train: 0.7757 loss_val: 0.8650 acc_val: 0.7132\n",
            "Epoch: 0036 loss_train: 0.6987 acc_train: 0.7837 loss_val: 0.8574 acc_val: 0.7162\n",
            "Epoch: 0037 loss_train: 0.6723 acc_train: 0.7927 loss_val: 0.8523 acc_val: 0.7207\n",
            "Epoch: 0038 loss_train: 0.6465 acc_train: 0.7967 loss_val: 0.8497 acc_val: 0.7222\n",
            "Epoch: 0039 loss_train: 0.6219 acc_train: 0.8057 loss_val: 0.8486 acc_val: 0.7252\n",
            "Epoch: 0040 loss_train: 0.5970 acc_train: 0.8112 loss_val: 0.8486 acc_val: 0.7282\n",
            "Epoch: 0041 loss_train: 0.5742 acc_train: 0.8187 loss_val: 0.8485 acc_val: 0.7237\n",
            "Epoch: 0042 loss_train: 0.5505 acc_train: 0.8202 loss_val: 0.8478 acc_val: 0.7237\n",
            "Epoch: 0043 loss_train: 0.5260 acc_train: 0.8292 loss_val: 0.8471 acc_val: 0.7252\n",
            "Epoch: 0044 loss_train: 0.5044 acc_train: 0.8322 loss_val: 0.8480 acc_val: 0.7342\n",
            "Epoch: 0045 loss_train: 0.4802 acc_train: 0.8433 loss_val: 0.8511 acc_val: 0.7417\n",
            "Epoch: 0046 loss_train: 0.4568 acc_train: 0.8518 loss_val: 0.8566 acc_val: 0.7417\n",
            "Epoch: 0047 loss_train: 0.4356 acc_train: 0.8618 loss_val: 0.8645 acc_val: 0.7477\n",
            "Epoch: 0048 loss_train: 0.4166 acc_train: 0.8668 loss_val: 0.8756 acc_val: 0.7447\n",
            "Epoch: 0049 loss_train: 0.3962 acc_train: 0.8708 loss_val: 0.8895 acc_val: 0.7508\n",
            "Epoch: 0050 loss_train: 0.3771 acc_train: 0.8733 loss_val: 0.9057 acc_val: 0.7447\n",
            "Epoch: 0051 loss_train: 0.3593 acc_train: 0.8743 loss_val: 0.9237 acc_val: 0.7432\n",
            "Epoch: 0052 loss_train: 0.3398 acc_train: 0.8818 loss_val: 0.9430 acc_val: 0.7402\n",
            "Epoch: 0053 loss_train: 0.3220 acc_train: 0.8873 loss_val: 0.9656 acc_val: 0.7342\n",
            "Epoch: 0054 loss_train: 0.3061 acc_train: 0.8973 loss_val: 0.9901 acc_val: 0.7327\n",
            "Epoch: 0055 loss_train: 0.2863 acc_train: 0.9024 loss_val: 1.0147 acc_val: 0.7297\n",
            "Epoch: 0056 loss_train: 0.2664 acc_train: 0.9094 loss_val: 1.0390 acc_val: 0.7237\n",
            "Epoch: 0057 loss_train: 0.2466 acc_train: 0.9134 loss_val: 1.0654 acc_val: 0.7222\n",
            "Epoch: 0058 loss_train: 0.2325 acc_train: 0.9239 loss_val: 1.0946 acc_val: 0.7252\n",
            "Epoch: 0059 loss_train: 0.2128 acc_train: 0.9299 loss_val: 1.1232 acc_val: 0.7282\n",
            "Epoch: 0060 loss_train: 0.1994 acc_train: 0.9384 loss_val: 1.1503 acc_val: 0.7252\n",
            "Epoch: 0061 loss_train: 0.1856 acc_train: 0.9444 loss_val: 1.1774 acc_val: 0.7237\n",
            "Epoch: 0062 loss_train: 0.1713 acc_train: 0.9499 loss_val: 1.2049 acc_val: 0.7297\n",
            "Epoch: 0063 loss_train: 0.1607 acc_train: 0.9499 loss_val: 1.2319 acc_val: 0.7252\n",
            "Epoch: 0064 loss_train: 0.1481 acc_train: 0.9549 loss_val: 1.2570 acc_val: 0.7237\n",
            "Epoch: 0065 loss_train: 0.1341 acc_train: 0.9574 loss_val: 1.2843 acc_val: 0.7327\n",
            "Epoch: 0066 loss_train: 0.1232 acc_train: 0.9644 loss_val: 1.3121 acc_val: 0.7312\n",
            "Epoch: 0067 loss_train: 0.1122 acc_train: 0.9685 loss_val: 1.3372 acc_val: 0.7342\n",
            "Epoch: 0068 loss_train: 0.1030 acc_train: 0.9715 loss_val: 1.3640 acc_val: 0.7372\n",
            "Epoch: 0069 loss_train: 0.0918 acc_train: 0.9715 loss_val: 1.3925 acc_val: 0.7357\n",
            "Epoch: 0070 loss_train: 0.0830 acc_train: 0.9765 loss_val: 1.4201 acc_val: 0.7342\n",
            "Epoch: 0071 loss_train: 0.0753 acc_train: 0.9790 loss_val: 1.4436 acc_val: 0.7372\n",
            "Epoch: 0072 loss_train: 0.0686 acc_train: 0.9775 loss_val: 1.4682 acc_val: 0.7327\n",
            "Epoch: 0073 loss_train: 0.0614 acc_train: 0.9855 loss_val: 1.4919 acc_val: 0.7357\n",
            "Epoch: 0074 loss_train: 0.0558 acc_train: 0.9855 loss_val: 1.5135 acc_val: 0.7312\n",
            "Epoch: 0075 loss_train: 0.0506 acc_train: 0.9855 loss_val: 1.5379 acc_val: 0.7327\n",
            "Epoch: 0076 loss_train: 0.0468 acc_train: 0.9865 loss_val: 1.5638 acc_val: 0.7327\n",
            "Epoch: 0077 loss_train: 0.0433 acc_train: 0.9860 loss_val: 1.5900 acc_val: 0.7342\n",
            "Epoch: 0078 loss_train: 0.0381 acc_train: 0.9910 loss_val: 1.6131 acc_val: 0.7282\n",
            "Epoch: 0079 loss_train: 0.0360 acc_train: 0.9905 loss_val: 1.6358 acc_val: 0.7327\n",
            "Epoch: 0080 loss_train: 0.0319 acc_train: 0.9910 loss_val: 1.6574 acc_val: 0.7312\n",
            "Epoch: 0081 loss_train: 0.0285 acc_train: 0.9930 loss_val: 1.6792 acc_val: 0.7312\n",
            "Epoch: 0082 loss_train: 0.0261 acc_train: 0.9925 loss_val: 1.6998 acc_val: 0.7327\n",
            "Epoch: 0083 loss_train: 0.0238 acc_train: 0.9950 loss_val: 1.7180 acc_val: 0.7327\n",
            "Epoch: 0084 loss_train: 0.0220 acc_train: 0.9960 loss_val: 1.7340 acc_val: 0.7327\n",
            "Epoch: 0085 loss_train: 0.0200 acc_train: 0.9970 loss_val: 1.7529 acc_val: 0.7312\n",
            "Epoch: 0086 loss_train: 0.0186 acc_train: 0.9960 loss_val: 1.7742 acc_val: 0.7297\n",
            "Epoch: 0087 loss_train: 0.0176 acc_train: 0.9950 loss_val: 1.7948 acc_val: 0.7282\n",
            "Epoch: 0088 loss_train: 0.0157 acc_train: 0.9960 loss_val: 1.8132 acc_val: 0.7267\n",
            "Epoch: 0089 loss_train: 0.0144 acc_train: 0.9975 loss_val: 1.8299 acc_val: 0.7297\n",
            "Epoch: 0090 loss_train: 0.0140 acc_train: 0.9955 loss_val: 1.8447 acc_val: 0.7282\n",
            "Epoch: 0091 loss_train: 0.0132 acc_train: 0.9965 loss_val: 1.8568 acc_val: 0.7312\n",
            "Epoch: 0092 loss_train: 0.0122 acc_train: 0.9965 loss_val: 1.8693 acc_val: 0.7297\n",
            "Epoch: 0093 loss_train: 0.0107 acc_train: 0.9975 loss_val: 1.8823 acc_val: 0.7297\n",
            "Epoch: 0094 loss_train: 0.0106 acc_train: 0.9975 loss_val: 1.8928 acc_val: 0.7297\n",
            "Epoch: 0095 loss_train: 0.0092 acc_train: 0.9985 loss_val: 1.9040 acc_val: 0.7297\n",
            "Epoch: 0096 loss_train: 0.0087 acc_train: 0.9975 loss_val: 1.9142 acc_val: 0.7282\n",
            "Epoch: 0097 loss_train: 0.0083 acc_train: 0.9990 loss_val: 1.9235 acc_val: 0.7312\n",
            "Epoch: 0098 loss_train: 0.0084 acc_train: 0.9980 loss_val: 1.9311 acc_val: 0.7312\n",
            "Epoch: 0099 loss_train: 0.0078 acc_train: 0.9980 loss_val: 1.9443 acc_val: 0.7297\n",
            "Optimization Finished!\n",
            "Train cost: 32.0026s\n",
            "Loading 49th epoch\n",
            "Test set results: loss= 0.7295 accuracy= 0.7861\n",
            "  NumNodes: 3327\n",
            "  NumEdges: 9228\n",
            "  NumFeats: 3703\n",
            "  NumClasses: 6\n",
            "  NumTrainingSamples: 120\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:354: DGLWarning: DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\n",
            "  dgl_warning(\"DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\")\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=3743, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=6, bias=True)\n",
            ")\n",
            "total params: 4154632\n",
            "Epoch: 0001 loss_train: 1.8374 acc_train: 0.1147 loss_val: 1.8326 acc_val: 0.1351\n",
            "Epoch: 0002 loss_train: 1.8304 acc_train: 0.1202 loss_val: 1.8225 acc_val: 0.1366\n",
            "Epoch: 0003 loss_train: 1.8218 acc_train: 0.1357 loss_val: 1.8077 acc_val: 0.1441\n",
            "Epoch: 0004 loss_train: 1.8044 acc_train: 0.1482 loss_val: 1.7886 acc_val: 0.1727\n",
            "Epoch: 0005 loss_train: 1.7865 acc_train: 0.1928 loss_val: 1.7658 acc_val: 0.2012\n",
            "Epoch: 0006 loss_train: 1.7620 acc_train: 0.2454 loss_val: 1.7399 acc_val: 0.2402\n",
            "Epoch: 0007 loss_train: 1.7355 acc_train: 0.2954 loss_val: 1.7114 acc_val: 0.3033\n",
            "Epoch: 0008 loss_train: 1.7068 acc_train: 0.3465 loss_val: 1.6812 acc_val: 0.4114\n",
            "Epoch: 0009 loss_train: 1.6753 acc_train: 0.4141 loss_val: 1.6496 acc_val: 0.4640\n",
            "Epoch: 0010 loss_train: 1.6415 acc_train: 0.4647 loss_val: 1.6170 acc_val: 0.4790\n",
            "Epoch: 0011 loss_train: 1.6108 acc_train: 0.4892 loss_val: 1.5837 acc_val: 0.4970\n",
            "Epoch: 0012 loss_train: 1.5732 acc_train: 0.5118 loss_val: 1.5498 acc_val: 0.5045\n",
            "Epoch: 0013 loss_train: 1.5422 acc_train: 0.5143 loss_val: 1.5151 acc_val: 0.5165\n",
            "Epoch: 0014 loss_train: 1.5052 acc_train: 0.5303 loss_val: 1.4793 acc_val: 0.5390\n",
            "Epoch: 0015 loss_train: 1.4694 acc_train: 0.5378 loss_val: 1.4427 acc_val: 0.5526\n",
            "Epoch: 0016 loss_train: 1.4326 acc_train: 0.5498 loss_val: 1.4052 acc_val: 0.5691\n",
            "Epoch: 0017 loss_train: 1.3937 acc_train: 0.5638 loss_val: 1.3676 acc_val: 0.5841\n",
            "Epoch: 0018 loss_train: 1.3563 acc_train: 0.5789 loss_val: 1.3303 acc_val: 0.6021\n",
            "Epoch: 0019 loss_train: 1.3164 acc_train: 0.5994 loss_val: 1.2934 acc_val: 0.6126\n",
            "Epoch: 0020 loss_train: 1.2799 acc_train: 0.6154 loss_val: 1.2579 acc_val: 0.6246\n",
            "Epoch: 0021 loss_train: 1.2432 acc_train: 0.6289 loss_val: 1.2242 acc_val: 0.6261\n",
            "Epoch: 0022 loss_train: 1.2052 acc_train: 0.6385 loss_val: 1.1931 acc_val: 0.6291\n",
            "Epoch: 0023 loss_train: 1.1723 acc_train: 0.6450 loss_val: 1.1646 acc_val: 0.6321\n",
            "Epoch: 0024 loss_train: 1.1360 acc_train: 0.6485 loss_val: 1.1385 acc_val: 0.6351\n",
            "Epoch: 0025 loss_train: 1.1022 acc_train: 0.6580 loss_val: 1.1144 acc_val: 0.6411\n",
            "Epoch: 0026 loss_train: 1.0665 acc_train: 0.6770 loss_val: 1.0914 acc_val: 0.6592\n",
            "Epoch: 0027 loss_train: 1.0331 acc_train: 0.6895 loss_val: 1.0688 acc_val: 0.6727\n",
            "Epoch: 0028 loss_train: 0.9947 acc_train: 0.7036 loss_val: 1.0458 acc_val: 0.6787\n",
            "Epoch: 0029 loss_train: 0.9612 acc_train: 0.7191 loss_val: 1.0233 acc_val: 0.6787\n",
            "Epoch: 0030 loss_train: 0.9249 acc_train: 0.7326 loss_val: 1.0016 acc_val: 0.6847\n",
            "Epoch: 0031 loss_train: 0.8863 acc_train: 0.7486 loss_val: 0.9798 acc_val: 0.6862\n",
            "Epoch: 0032 loss_train: 0.8523 acc_train: 0.7581 loss_val: 0.9591 acc_val: 0.7012\n",
            "Epoch: 0033 loss_train: 0.8157 acc_train: 0.7666 loss_val: 0.9402 acc_val: 0.7072\n",
            "Epoch: 0034 loss_train: 0.7767 acc_train: 0.7772 loss_val: 0.9227 acc_val: 0.7087\n",
            "Epoch: 0035 loss_train: 0.7482 acc_train: 0.7802 loss_val: 0.9066 acc_val: 0.7072\n",
            "Epoch: 0036 loss_train: 0.7064 acc_train: 0.7867 loss_val: 0.8927 acc_val: 0.7087\n",
            "Epoch: 0037 loss_train: 0.6749 acc_train: 0.7997 loss_val: 0.8806 acc_val: 0.7132\n",
            "Epoch: 0038 loss_train: 0.6394 acc_train: 0.8147 loss_val: 0.8692 acc_val: 0.7192\n",
            "Epoch: 0039 loss_train: 0.6096 acc_train: 0.8207 loss_val: 0.8590 acc_val: 0.7252\n",
            "Epoch: 0040 loss_train: 0.5807 acc_train: 0.8287 loss_val: 0.8514 acc_val: 0.7282\n",
            "Epoch: 0041 loss_train: 0.5534 acc_train: 0.8307 loss_val: 0.8461 acc_val: 0.7312\n",
            "Epoch: 0042 loss_train: 0.5257 acc_train: 0.8383 loss_val: 0.8425 acc_val: 0.7342\n",
            "Epoch: 0043 loss_train: 0.4956 acc_train: 0.8438 loss_val: 0.8410 acc_val: 0.7327\n",
            "Epoch: 0044 loss_train: 0.4714 acc_train: 0.8553 loss_val: 0.8423 acc_val: 0.7312\n",
            "Epoch: 0045 loss_train: 0.4438 acc_train: 0.8658 loss_val: 0.8459 acc_val: 0.7327\n",
            "Epoch: 0046 loss_train: 0.4196 acc_train: 0.8688 loss_val: 0.8493 acc_val: 0.7342\n",
            "Epoch: 0047 loss_train: 0.3948 acc_train: 0.8818 loss_val: 0.8537 acc_val: 0.7432\n",
            "Epoch: 0048 loss_train: 0.3727 acc_train: 0.8888 loss_val: 0.8612 acc_val: 0.7402\n",
            "Epoch: 0049 loss_train: 0.3491 acc_train: 0.8963 loss_val: 0.8717 acc_val: 0.7372\n",
            "Epoch: 0050 loss_train: 0.3300 acc_train: 0.8963 loss_val: 0.8855 acc_val: 0.7402\n",
            "Epoch: 0051 loss_train: 0.3074 acc_train: 0.9039 loss_val: 0.9006 acc_val: 0.7432\n",
            "Epoch: 0052 loss_train: 0.2847 acc_train: 0.9129 loss_val: 0.9169 acc_val: 0.7432\n",
            "Epoch: 0053 loss_train: 0.2653 acc_train: 0.9179 loss_val: 0.9363 acc_val: 0.7447\n",
            "Epoch: 0054 loss_train: 0.2472 acc_train: 0.9239 loss_val: 0.9596 acc_val: 0.7508\n",
            "Epoch: 0055 loss_train: 0.2299 acc_train: 0.9289 loss_val: 0.9846 acc_val: 0.7523\n",
            "Epoch: 0056 loss_train: 0.2111 acc_train: 0.9369 loss_val: 1.0118 acc_val: 0.7462\n",
            "Epoch: 0057 loss_train: 0.1935 acc_train: 0.9399 loss_val: 1.0396 acc_val: 0.7447\n",
            "Epoch: 0058 loss_train: 0.1803 acc_train: 0.9464 loss_val: 1.0687 acc_val: 0.7417\n",
            "Epoch: 0059 loss_train: 0.1640 acc_train: 0.9539 loss_val: 1.0964 acc_val: 0.7462\n",
            "Epoch: 0060 loss_train: 0.1491 acc_train: 0.9619 loss_val: 1.1248 acc_val: 0.7462\n",
            "Epoch: 0061 loss_train: 0.1346 acc_train: 0.9644 loss_val: 1.1549 acc_val: 0.7432\n",
            "Epoch: 0062 loss_train: 0.1224 acc_train: 0.9695 loss_val: 1.1845 acc_val: 0.7447\n",
            "Epoch: 0063 loss_train: 0.1162 acc_train: 0.9705 loss_val: 1.2120 acc_val: 0.7417\n",
            "Epoch: 0064 loss_train: 0.1036 acc_train: 0.9735 loss_val: 1.2375 acc_val: 0.7402\n",
            "Epoch: 0065 loss_train: 0.0946 acc_train: 0.9750 loss_val: 1.2656 acc_val: 0.7402\n",
            "Epoch: 0066 loss_train: 0.0855 acc_train: 0.9770 loss_val: 1.2971 acc_val: 0.7357\n",
            "Epoch: 0067 loss_train: 0.0776 acc_train: 0.9775 loss_val: 1.3268 acc_val: 0.7387\n",
            "Epoch: 0068 loss_train: 0.0691 acc_train: 0.9850 loss_val: 1.3520 acc_val: 0.7432\n",
            "Epoch: 0069 loss_train: 0.0635 acc_train: 0.9850 loss_val: 1.3774 acc_val: 0.7417\n",
            "Epoch: 0070 loss_train: 0.0589 acc_train: 0.9850 loss_val: 1.4078 acc_val: 0.7372\n",
            "Epoch: 0071 loss_train: 0.0536 acc_train: 0.9880 loss_val: 1.4385 acc_val: 0.7312\n",
            "Epoch: 0072 loss_train: 0.0497 acc_train: 0.9900 loss_val: 1.4675 acc_val: 0.7297\n",
            "Epoch: 0073 loss_train: 0.0469 acc_train: 0.9875 loss_val: 1.4935 acc_val: 0.7282\n",
            "Epoch: 0074 loss_train: 0.0418 acc_train: 0.9895 loss_val: 1.5168 acc_val: 0.7297\n",
            "Epoch: 0075 loss_train: 0.0386 acc_train: 0.9910 loss_val: 1.5399 acc_val: 0.7282\n",
            "Epoch: 0076 loss_train: 0.0362 acc_train: 0.9910 loss_val: 1.5645 acc_val: 0.7282\n",
            "Epoch: 0077 loss_train: 0.0337 acc_train: 0.9905 loss_val: 1.5887 acc_val: 0.7297\n",
            "Epoch: 0078 loss_train: 0.0299 acc_train: 0.9945 loss_val: 1.6104 acc_val: 0.7297\n",
            "Epoch: 0079 loss_train: 0.0284 acc_train: 0.9930 loss_val: 1.6291 acc_val: 0.7282\n",
            "Epoch: 0080 loss_train: 0.0246 acc_train: 0.9950 loss_val: 1.6488 acc_val: 0.7282\n",
            "Epoch: 0081 loss_train: 0.0224 acc_train: 0.9950 loss_val: 1.6695 acc_val: 0.7222\n",
            "Epoch: 0082 loss_train: 0.0207 acc_train: 0.9945 loss_val: 1.6876 acc_val: 0.7207\n",
            "Epoch: 0083 loss_train: 0.0192 acc_train: 0.9945 loss_val: 1.7043 acc_val: 0.7237\n",
            "Epoch: 0084 loss_train: 0.0176 acc_train: 0.9960 loss_val: 1.7202 acc_val: 0.7222\n",
            "Epoch: 0085 loss_train: 0.0159 acc_train: 0.9970 loss_val: 1.7360 acc_val: 0.7222\n",
            "Epoch: 0086 loss_train: 0.0140 acc_train: 0.9970 loss_val: 1.7522 acc_val: 0.7267\n",
            "Epoch: 0087 loss_train: 0.0137 acc_train: 0.9980 loss_val: 1.7677 acc_val: 0.7267\n",
            "Epoch: 0088 loss_train: 0.0124 acc_train: 0.9965 loss_val: 1.7806 acc_val: 0.7282\n",
            "Epoch: 0089 loss_train: 0.0107 acc_train: 0.9975 loss_val: 1.7929 acc_val: 0.7282\n",
            "Epoch: 0090 loss_train: 0.0101 acc_train: 0.9985 loss_val: 1.8071 acc_val: 0.7282\n",
            "Epoch: 0091 loss_train: 0.0097 acc_train: 0.9975 loss_val: 1.8238 acc_val: 0.7267\n",
            "Epoch: 0092 loss_train: 0.0092 acc_train: 0.9980 loss_val: 1.8389 acc_val: 0.7252\n",
            "Epoch: 0093 loss_train: 0.0084 acc_train: 0.9980 loss_val: 1.8524 acc_val: 0.7252\n",
            "Epoch: 0094 loss_train: 0.0082 acc_train: 0.9980 loss_val: 1.8620 acc_val: 0.7252\n",
            "Epoch: 0095 loss_train: 0.0072 acc_train: 0.9985 loss_val: 1.8750 acc_val: 0.7237\n",
            "Epoch: 0096 loss_train: 0.0069 acc_train: 0.9985 loss_val: 1.8894 acc_val: 0.7237\n",
            "Epoch: 0097 loss_train: 0.0060 acc_train: 0.9990 loss_val: 1.9034 acc_val: 0.7267\n",
            "Epoch: 0098 loss_train: 0.0061 acc_train: 0.9985 loss_val: 1.9175 acc_val: 0.7237\n",
            "Epoch: 0099 loss_train: 0.0053 acc_train: 0.9985 loss_val: 1.9292 acc_val: 0.7252\n",
            "Epoch: 0100 loss_train: 0.0052 acc_train: 0.9990 loss_val: 1.9377 acc_val: 0.7267\n",
            "Epoch: 0101 loss_train: 0.0047 acc_train: 0.9995 loss_val: 1.9439 acc_val: 0.7237\n",
            "Epoch: 0102 loss_train: 0.0046 acc_train: 0.9990 loss_val: 1.9492 acc_val: 0.7297\n",
            "Epoch: 0103 loss_train: 0.0044 acc_train: 0.9990 loss_val: 1.9548 acc_val: 0.7267\n",
            "Epoch: 0104 loss_train: 0.0043 acc_train: 0.9985 loss_val: 1.9605 acc_val: 0.7282\n",
            "Epoch: 0105 loss_train: 0.0039 acc_train: 0.9990 loss_val: 1.9676 acc_val: 0.7282\n",
            "Optimization Finished!\n",
            "Train cost: 34.4501s\n",
            "Loading 55th epoch\n",
            "Test set results: loss= 0.8149 accuracy= 0.7771\n",
            "  NumNodes: 3327\n",
            "  NumEdges: 9228\n",
            "  NumFeats: 3703\n",
            "  NumClasses: 6\n",
            "  NumTrainingSamples: 120\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:354: DGLWarning: DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\n",
            "  dgl_warning(\"DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\")\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=3743, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=6, bias=True)\n",
            ")\n",
            "total params: 4154632\n",
            "Epoch: 0001 loss_train: 1.8282 acc_train: 0.1077 loss_val: 1.8249 acc_val: 0.1111\n",
            "Epoch: 0002 loss_train: 1.8219 acc_train: 0.1047 loss_val: 1.8150 acc_val: 0.1246\n",
            "Epoch: 0003 loss_train: 1.8123 acc_train: 0.1177 loss_val: 1.8005 acc_val: 0.1456\n",
            "Epoch: 0004 loss_train: 1.7959 acc_train: 0.1307 loss_val: 1.7819 acc_val: 0.1982\n",
            "Epoch: 0005 loss_train: 1.7770 acc_train: 0.1693 loss_val: 1.7600 acc_val: 0.2447\n",
            "Epoch: 0006 loss_train: 1.7547 acc_train: 0.2554 loss_val: 1.7354 acc_val: 0.3273\n",
            "Epoch: 0007 loss_train: 1.7298 acc_train: 0.3400 loss_val: 1.7087 acc_val: 0.3919\n",
            "Epoch: 0008 loss_train: 1.7016 acc_train: 0.3961 loss_val: 1.6805 acc_val: 0.4309\n",
            "Epoch: 0009 loss_train: 1.6731 acc_train: 0.4296 loss_val: 1.6516 acc_val: 0.4655\n",
            "Epoch: 0010 loss_train: 1.6425 acc_train: 0.4582 loss_val: 1.6219 acc_val: 0.4730\n",
            "Epoch: 0011 loss_train: 1.6117 acc_train: 0.4757 loss_val: 1.5917 acc_val: 0.4760\n",
            "Epoch: 0012 loss_train: 1.5802 acc_train: 0.4887 loss_val: 1.5606 acc_val: 0.4805\n",
            "Epoch: 0013 loss_train: 1.5496 acc_train: 0.4937 loss_val: 1.5287 acc_val: 0.5015\n",
            "Epoch: 0014 loss_train: 1.5163 acc_train: 0.5058 loss_val: 1.4960 acc_val: 0.5165\n",
            "Epoch: 0015 loss_train: 1.4832 acc_train: 0.5153 loss_val: 1.4624 acc_val: 0.5405\n",
            "Epoch: 0016 loss_train: 1.4487 acc_train: 0.5323 loss_val: 1.4284 acc_val: 0.5465\n",
            "Epoch: 0017 loss_train: 1.4142 acc_train: 0.5468 loss_val: 1.3942 acc_val: 0.5601\n",
            "Epoch: 0018 loss_train: 1.3803 acc_train: 0.5643 loss_val: 1.3601 acc_val: 0.5736\n",
            "Epoch: 0019 loss_train: 1.3430 acc_train: 0.5769 loss_val: 1.3262 acc_val: 0.5721\n",
            "Epoch: 0020 loss_train: 1.3101 acc_train: 0.5799 loss_val: 1.2929 acc_val: 0.5811\n",
            "Epoch: 0021 loss_train: 1.2775 acc_train: 0.5864 loss_val: 1.2606 acc_val: 0.5946\n",
            "Epoch: 0022 loss_train: 1.2433 acc_train: 0.5999 loss_val: 1.2294 acc_val: 0.6006\n",
            "Epoch: 0023 loss_train: 1.2103 acc_train: 0.6084 loss_val: 1.2000 acc_val: 0.6051\n",
            "Epoch: 0024 loss_train: 1.1758 acc_train: 0.6244 loss_val: 1.1725 acc_val: 0.6126\n",
            "Epoch: 0025 loss_train: 1.1413 acc_train: 0.6415 loss_val: 1.1467 acc_val: 0.6276\n",
            "Epoch: 0026 loss_train: 1.1064 acc_train: 0.6645 loss_val: 1.1210 acc_val: 0.6336\n",
            "Epoch: 0027 loss_train: 1.0702 acc_train: 0.6870 loss_val: 1.0943 acc_val: 0.6471\n",
            "Epoch: 0028 loss_train: 1.0299 acc_train: 0.6960 loss_val: 1.0661 acc_val: 0.6592\n",
            "Epoch: 0029 loss_train: 0.9911 acc_train: 0.7141 loss_val: 1.0377 acc_val: 0.6787\n",
            "Epoch: 0030 loss_train: 0.9510 acc_train: 0.7336 loss_val: 1.0115 acc_val: 0.6892\n",
            "Epoch: 0031 loss_train: 0.9070 acc_train: 0.7496 loss_val: 0.9893 acc_val: 0.6937\n",
            "Epoch: 0032 loss_train: 0.8662 acc_train: 0.7551 loss_val: 0.9707 acc_val: 0.6952\n",
            "Epoch: 0033 loss_train: 0.8234 acc_train: 0.7707 loss_val: 0.9519 acc_val: 0.6997\n",
            "Epoch: 0034 loss_train: 0.7830 acc_train: 0.7807 loss_val: 0.9330 acc_val: 0.7042\n",
            "Epoch: 0035 loss_train: 0.7503 acc_train: 0.7742 loss_val: 0.9157 acc_val: 0.7027\n",
            "Epoch: 0036 loss_train: 0.7075 acc_train: 0.7912 loss_val: 0.9004 acc_val: 0.7087\n",
            "Epoch: 0037 loss_train: 0.6716 acc_train: 0.8012 loss_val: 0.8880 acc_val: 0.7087\n",
            "Epoch: 0038 loss_train: 0.6344 acc_train: 0.8122 loss_val: 0.8791 acc_val: 0.7132\n",
            "Epoch: 0039 loss_train: 0.6028 acc_train: 0.8227 loss_val: 0.8711 acc_val: 0.7162\n",
            "Epoch: 0040 loss_train: 0.5725 acc_train: 0.8292 loss_val: 0.8653 acc_val: 0.7162\n",
            "Epoch: 0041 loss_train: 0.5436 acc_train: 0.8368 loss_val: 0.8609 acc_val: 0.7267\n",
            "Epoch: 0042 loss_train: 0.5133 acc_train: 0.8458 loss_val: 0.8585 acc_val: 0.7327\n",
            "Epoch: 0043 loss_train: 0.4854 acc_train: 0.8558 loss_val: 0.8565 acc_val: 0.7297\n",
            "Epoch: 0044 loss_train: 0.4596 acc_train: 0.8658 loss_val: 0.8585 acc_val: 0.7282\n",
            "Epoch: 0045 loss_train: 0.4336 acc_train: 0.8708 loss_val: 0.8638 acc_val: 0.7327\n",
            "Epoch: 0046 loss_train: 0.4084 acc_train: 0.8783 loss_val: 0.8707 acc_val: 0.7372\n",
            "Epoch: 0047 loss_train: 0.3839 acc_train: 0.8853 loss_val: 0.8794 acc_val: 0.7402\n",
            "Epoch: 0048 loss_train: 0.3594 acc_train: 0.8963 loss_val: 0.8888 acc_val: 0.7387\n",
            "Epoch: 0049 loss_train: 0.3358 acc_train: 0.8988 loss_val: 0.8980 acc_val: 0.7372\n",
            "Epoch: 0050 loss_train: 0.3146 acc_train: 0.9064 loss_val: 0.9099 acc_val: 0.7387\n",
            "Epoch: 0051 loss_train: 0.2917 acc_train: 0.9129 loss_val: 0.9243 acc_val: 0.7402\n",
            "Epoch: 0052 loss_train: 0.2723 acc_train: 0.9194 loss_val: 0.9420 acc_val: 0.7447\n",
            "Epoch: 0053 loss_train: 0.2527 acc_train: 0.9229 loss_val: 0.9627 acc_val: 0.7417\n",
            "Epoch: 0054 loss_train: 0.2350 acc_train: 0.9294 loss_val: 0.9843 acc_val: 0.7402\n",
            "Epoch: 0055 loss_train: 0.2174 acc_train: 0.9339 loss_val: 1.0052 acc_val: 0.7387\n",
            "Epoch: 0056 loss_train: 0.2010 acc_train: 0.9399 loss_val: 1.0268 acc_val: 0.7372\n",
            "Epoch: 0057 loss_train: 0.1829 acc_train: 0.9469 loss_val: 1.0503 acc_val: 0.7342\n",
            "Epoch: 0058 loss_train: 0.1730 acc_train: 0.9499 loss_val: 1.0764 acc_val: 0.7342\n",
            "Epoch: 0059 loss_train: 0.1569 acc_train: 0.9574 loss_val: 1.1029 acc_val: 0.7312\n",
            "Epoch: 0060 loss_train: 0.1432 acc_train: 0.9629 loss_val: 1.1311 acc_val: 0.7342\n",
            "Epoch: 0061 loss_train: 0.1314 acc_train: 0.9664 loss_val: 1.1600 acc_val: 0.7327\n",
            "Epoch: 0062 loss_train: 0.1202 acc_train: 0.9700 loss_val: 1.1864 acc_val: 0.7357\n",
            "Epoch: 0063 loss_train: 0.1121 acc_train: 0.9685 loss_val: 1.2116 acc_val: 0.7357\n",
            "Epoch: 0064 loss_train: 0.1011 acc_train: 0.9710 loss_val: 1.2373 acc_val: 0.7357\n",
            "Epoch: 0065 loss_train: 0.0918 acc_train: 0.9765 loss_val: 1.2656 acc_val: 0.7342\n",
            "Epoch: 0066 loss_train: 0.0838 acc_train: 0.9780 loss_val: 1.2950 acc_val: 0.7327\n",
            "Epoch: 0067 loss_train: 0.0766 acc_train: 0.9740 loss_val: 1.3224 acc_val: 0.7357\n",
            "Epoch: 0068 loss_train: 0.0677 acc_train: 0.9800 loss_val: 1.3450 acc_val: 0.7387\n",
            "Epoch: 0069 loss_train: 0.0608 acc_train: 0.9855 loss_val: 1.3665 acc_val: 0.7402\n",
            "Epoch: 0070 loss_train: 0.0569 acc_train: 0.9850 loss_val: 1.3913 acc_val: 0.7447\n",
            "Epoch: 0071 loss_train: 0.0528 acc_train: 0.9865 loss_val: 1.4186 acc_val: 0.7492\n",
            "Epoch: 0072 loss_train: 0.0479 acc_train: 0.9915 loss_val: 1.4486 acc_val: 0.7447\n",
            "Epoch: 0073 loss_train: 0.0463 acc_train: 0.9900 loss_val: 1.4791 acc_val: 0.7417\n",
            "Epoch: 0074 loss_train: 0.0415 acc_train: 0.9890 loss_val: 1.5063 acc_val: 0.7432\n",
            "Epoch: 0075 loss_train: 0.0384 acc_train: 0.9910 loss_val: 1.5304 acc_val: 0.7417\n",
            "Epoch: 0076 loss_train: 0.0356 acc_train: 0.9910 loss_val: 1.5532 acc_val: 0.7447\n",
            "Epoch: 0077 loss_train: 0.0326 acc_train: 0.9905 loss_val: 1.5760 acc_val: 0.7417\n",
            "Epoch: 0078 loss_train: 0.0289 acc_train: 0.9915 loss_val: 1.5995 acc_val: 0.7417\n",
            "Epoch: 0079 loss_train: 0.0284 acc_train: 0.9930 loss_val: 1.6219 acc_val: 0.7417\n",
            "Epoch: 0080 loss_train: 0.0249 acc_train: 0.9965 loss_val: 1.6419 acc_val: 0.7357\n",
            "Epoch: 0081 loss_train: 0.0243 acc_train: 0.9950 loss_val: 1.6575 acc_val: 0.7387\n",
            "Epoch: 0082 loss_train: 0.0221 acc_train: 0.9950 loss_val: 1.6696 acc_val: 0.7387\n",
            "Epoch: 0083 loss_train: 0.0210 acc_train: 0.9935 loss_val: 1.6832 acc_val: 0.7372\n",
            "Epoch: 0084 loss_train: 0.0187 acc_train: 0.9960 loss_val: 1.7000 acc_val: 0.7387\n",
            "Epoch: 0085 loss_train: 0.0172 acc_train: 0.9970 loss_val: 1.7173 acc_val: 0.7387\n",
            "Epoch: 0086 loss_train: 0.0153 acc_train: 0.9965 loss_val: 1.7330 acc_val: 0.7387\n",
            "Epoch: 0087 loss_train: 0.0144 acc_train: 0.9975 loss_val: 1.7446 acc_val: 0.7387\n",
            "Epoch: 0088 loss_train: 0.0139 acc_train: 0.9965 loss_val: 1.7560 acc_val: 0.7402\n",
            "Epoch: 0089 loss_train: 0.0119 acc_train: 0.9970 loss_val: 1.7697 acc_val: 0.7402\n",
            "Epoch: 0090 loss_train: 0.0115 acc_train: 0.9975 loss_val: 1.7882 acc_val: 0.7387\n",
            "Epoch: 0091 loss_train: 0.0104 acc_train: 0.9985 loss_val: 1.8084 acc_val: 0.7357\n",
            "Epoch: 0092 loss_train: 0.0099 acc_train: 0.9990 loss_val: 1.8270 acc_val: 0.7342\n",
            "Epoch: 0093 loss_train: 0.0092 acc_train: 0.9985 loss_val: 1.8396 acc_val: 0.7312\n",
            "Epoch: 0094 loss_train: 0.0086 acc_train: 0.9990 loss_val: 1.8473 acc_val: 0.7312\n",
            "Epoch: 0095 loss_train: 0.0083 acc_train: 0.9990 loss_val: 1.8540 acc_val: 0.7297\n",
            "Epoch: 0096 loss_train: 0.0078 acc_train: 0.9985 loss_val: 1.8626 acc_val: 0.7342\n",
            "Epoch: 0097 loss_train: 0.0073 acc_train: 0.9990 loss_val: 1.8733 acc_val: 0.7357\n",
            "Epoch: 0098 loss_train: 0.0067 acc_train: 0.9990 loss_val: 1.8860 acc_val: 0.7357\n",
            "Epoch: 0099 loss_train: 0.0057 acc_train: 1.0000 loss_val: 1.8982 acc_val: 0.7372\n",
            "Epoch: 0100 loss_train: 0.0056 acc_train: 0.9990 loss_val: 1.9079 acc_val: 0.7372\n",
            "Epoch: 0101 loss_train: 0.0051 acc_train: 0.9995 loss_val: 1.9157 acc_val: 0.7372\n",
            "Epoch: 0102 loss_train: 0.0050 acc_train: 1.0000 loss_val: 1.9212 acc_val: 0.7372\n",
            "Epoch: 0103 loss_train: 0.0047 acc_train: 0.9995 loss_val: 1.9277 acc_val: 0.7372\n",
            "Epoch: 0104 loss_train: 0.0045 acc_train: 0.9995 loss_val: 1.9342 acc_val: 0.7372\n",
            "Epoch: 0105 loss_train: 0.0043 acc_train: 0.9995 loss_val: 1.9427 acc_val: 0.7372\n",
            "Epoch: 0106 loss_train: 0.0038 acc_train: 0.9990 loss_val: 1.9525 acc_val: 0.7357\n",
            "Epoch: 0107 loss_train: 0.0036 acc_train: 0.9995 loss_val: 1.9618 acc_val: 0.7372\n",
            "Epoch: 0108 loss_train: 0.0032 acc_train: 1.0000 loss_val: 1.9703 acc_val: 0.7357\n",
            "Epoch: 0109 loss_train: 0.0032 acc_train: 1.0000 loss_val: 1.9766 acc_val: 0.7342\n",
            "Epoch: 0110 loss_train: 0.0029 acc_train: 1.0000 loss_val: 1.9810 acc_val: 0.7342\n",
            "Epoch: 0111 loss_train: 0.0026 acc_train: 1.0000 loss_val: 1.9865 acc_val: 0.7342\n",
            "Epoch: 0112 loss_train: 0.0024 acc_train: 1.0000 loss_val: 1.9945 acc_val: 0.7342\n",
            "Epoch: 0113 loss_train: 0.0024 acc_train: 1.0000 loss_val: 2.0029 acc_val: 0.7342\n",
            "Epoch: 0114 loss_train: 0.0020 acc_train: 1.0000 loss_val: 2.0101 acc_val: 0.7342\n",
            "Epoch: 0115 loss_train: 0.0019 acc_train: 1.0000 loss_val: 2.0163 acc_val: 0.7342\n",
            "Epoch: 0116 loss_train: 0.0018 acc_train: 1.0000 loss_val: 2.0212 acc_val: 0.7342\n",
            "Epoch: 0117 loss_train: 0.0018 acc_train: 1.0000 loss_val: 2.0240 acc_val: 0.7342\n",
            "Epoch: 0118 loss_train: 0.0016 acc_train: 1.0000 loss_val: 2.0260 acc_val: 0.7372\n",
            "Epoch: 0119 loss_train: 0.0016 acc_train: 1.0000 loss_val: 2.0280 acc_val: 0.7387\n",
            "Epoch: 0120 loss_train: 0.0016 acc_train: 1.0000 loss_val: 2.0308 acc_val: 0.7372\n",
            "Epoch: 0121 loss_train: 0.0015 acc_train: 1.0000 loss_val: 2.0343 acc_val: 0.7357\n",
            "Optimization Finished!\n",
            "Train cost: 39.4568s\n",
            "Loading 71th epoch\n",
            "Test set results: loss= 1.1137 accuracy= 0.7666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6AJ6r4BTaNL2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pubmed"
      ],
      "metadata": {
        "id": "ZTeF1tLvaKgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --dataset pubmed --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 7  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack none --attperc 20 --TrainTest train --weight_decay=1e-05\n",
        "\n",
        "!python train.py --dataset pubmed --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 7  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack dice --attperc 10 --TrainTest train --weight_decay=1e-05\n",
        "\n",
        "!python train.py --dataset pubmed --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 7  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack dice --attperc 40 --TrainTest train --weight_decay=1e-05\n"
      ],
      "metadata": {
        "id": "9rNjlHp-rnyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --dataset pubmed --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 7  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack dice --attperc 60 --TrainTest train --weight_decay=1e-05\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkePHFjpZwgz",
        "outputId": "0bddd7bc-c9ca-4ac5-b690-a27c1b58ace4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/train.py': [Errno 107] Transport endpoint is not connected\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --dataset pubmed --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 7  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack l2_weak --attperc 10 --TrainTest train --weight_decay=1e-05\n",
        "\n",
        "!python train.py --dataset pubmed --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 7  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack l2_weak --attperc 40 --TrainTest train --weight_decay=1e-05\n",
        "\n",
        "!python train.py --dataset pubmed --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 7  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack l2_weak --attperc 60 --TrainTest train --weight_decay=1e-05\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_nYoSdxariz",
        "outputId": "a0e4fbc0-464a-4cb2-f4cf-11987fc78ca1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n",
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n",
            "Downloading /root/.dgl/pubmed.zip from https://data.dgl.ai/dataset/pubmed.zip...\n",
            "Extracting file to /root/.dgl/pubmed\n",
            "Finished data loading and preprocessing.\n",
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done saving data into cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:354: DGLWarning: DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\n",
            "  dgl_warning(\"DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\")\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=515, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=3, bias=True)\n",
            ")\n",
            "total params: 2501125\n",
            "Epoch: 0001 loss_train: 6.4541 acc_train: 0.4269 loss_val: 1.9895 acc_val: 0.5920\n",
            "Epoch: 0002 loss_train: 5.5849 acc_train: 0.6452 loss_val: 1.6297 acc_val: 0.6848\n",
            "Epoch: 0003 loss_train: 4.5642 acc_train: 0.7033 loss_val: 1.3014 acc_val: 0.7515\n",
            "Epoch: 0004 loss_train: 3.6853 acc_train: 0.7670 loss_val: 1.1068 acc_val: 0.7842\n",
            "Epoch: 0005 loss_train: 3.2232 acc_train: 0.7882 loss_val: 1.0260 acc_val: 0.7977\n",
            "Epoch: 0006 loss_train: 2.9591 acc_train: 0.7998 loss_val: 0.9305 acc_val: 0.8215\n",
            "Epoch: 0007 loss_train: 2.6816 acc_train: 0.8231 loss_val: 0.8531 acc_val: 0.8339\n",
            "Epoch: 0008 loss_train: 2.4217 acc_train: 0.8399 loss_val: 0.7822 acc_val: 0.8509\n",
            "Epoch: 0009 loss_train: 2.2013 acc_train: 0.8547 loss_val: 0.7494 acc_val: 0.8608\n",
            "Epoch: 0010 loss_train: 2.0429 acc_train: 0.8635 loss_val: 0.7086 acc_val: 0.8692\n",
            "Epoch: 0011 loss_train: 1.8785 acc_train: 0.8754 loss_val: 0.6752 acc_val: 0.8722\n",
            "Epoch: 0012 loss_train: 1.7319 acc_train: 0.8893 loss_val: 0.6640 acc_val: 0.8768\n",
            "Epoch: 0013 loss_train: 1.6005 acc_train: 0.8985 loss_val: 0.6472 acc_val: 0.8826\n",
            "Epoch: 0014 loss_train: 1.5224 acc_train: 0.9048 loss_val: 0.6528 acc_val: 0.8844\n",
            "Epoch: 0015 loss_train: 1.5214 acc_train: 0.9014 loss_val: 0.7041 acc_val: 0.8742\n",
            "Epoch: 0016 loss_train: 1.4393 acc_train: 0.9066 loss_val: 0.6359 acc_val: 0.8841\n",
            "Epoch: 0017 loss_train: 1.3206 acc_train: 0.9170 loss_val: 0.6133 acc_val: 0.8877\n",
            "Epoch: 0018 loss_train: 1.1863 acc_train: 0.9269 loss_val: 0.6587 acc_val: 0.8846\n",
            "Epoch: 0019 loss_train: 1.1557 acc_train: 0.9262 loss_val: 0.6676 acc_val: 0.8831\n",
            "Epoch: 0020 loss_train: 1.0563 acc_train: 0.9342 loss_val: 0.6800 acc_val: 0.8851\n",
            "Epoch: 0021 loss_train: 0.9631 acc_train: 0.9432 loss_val: 0.6842 acc_val: 0.8900\n",
            "Epoch: 0022 loss_train: 0.9375 acc_train: 0.9409 loss_val: 0.7017 acc_val: 0.8910\n",
            "Epoch: 0023 loss_train: 0.8967 acc_train: 0.9448 loss_val: 0.7113 acc_val: 0.8872\n",
            "Epoch: 0024 loss_train: 1.0552 acc_train: 0.9325 loss_val: 0.6880 acc_val: 0.8872\n",
            "Epoch: 0025 loss_train: 0.9134 acc_train: 0.9416 loss_val: 0.7344 acc_val: 0.8808\n",
            "Epoch: 0026 loss_train: 0.7897 acc_train: 0.9500 loss_val: 0.7336 acc_val: 0.8862\n",
            "Epoch: 0027 loss_train: 0.6379 acc_train: 0.9639 loss_val: 0.7793 acc_val: 0.8862\n",
            "Epoch: 0028 loss_train: 0.5527 acc_train: 0.9675 loss_val: 0.8403 acc_val: 0.8821\n",
            "Epoch: 0029 loss_train: 0.4766 acc_train: 0.9742 loss_val: 0.8603 acc_val: 0.8864\n",
            "Epoch: 0030 loss_train: 0.4146 acc_train: 0.9772 loss_val: 0.9487 acc_val: 0.8844\n",
            "Epoch: 0031 loss_train: 0.3544 acc_train: 0.9805 loss_val: 1.0001 acc_val: 0.8811\n",
            "Epoch: 0032 loss_train: 0.2882 acc_train: 0.9835 loss_val: 1.0792 acc_val: 0.8775\n",
            "Epoch: 0033 loss_train: 0.2920 acc_train: 0.9835 loss_val: 1.1571 acc_val: 0.8765\n",
            "Epoch: 0034 loss_train: 0.4328 acc_train: 0.9721 loss_val: 1.2567 acc_val: 0.8732\n",
            "Epoch: 0035 loss_train: 0.4426 acc_train: 0.9735 loss_val: 1.1263 acc_val: 0.8844\n",
            "Epoch: 0036 loss_train: 0.3413 acc_train: 0.9788 loss_val: 1.1305 acc_val: 0.8811\n",
            "Epoch: 0037 loss_train: 0.2355 acc_train: 0.9858 loss_val: 1.0992 acc_val: 0.8788\n",
            "Epoch: 0038 loss_train: 0.1636 acc_train: 0.9915 loss_val: 1.2547 acc_val: 0.8806\n",
            "Epoch: 0039 loss_train: 0.1506 acc_train: 0.9911 loss_val: 1.3046 acc_val: 0.8780\n",
            "Epoch: 0040 loss_train: 0.1387 acc_train: 0.9915 loss_val: 1.3813 acc_val: 0.8806\n",
            "Epoch: 0041 loss_train: 0.1333 acc_train: 0.9935 loss_val: 1.3895 acc_val: 0.8770\n",
            "Epoch: 0042 loss_train: 0.1142 acc_train: 0.9939 loss_val: 1.4463 acc_val: 0.8834\n",
            "Epoch: 0043 loss_train: 0.1107 acc_train: 0.9927 loss_val: 1.5184 acc_val: 0.8763\n",
            "Epoch: 0044 loss_train: 0.0951 acc_train: 0.9950 loss_val: 1.5492 acc_val: 0.8753\n",
            "Epoch: 0045 loss_train: 0.0927 acc_train: 0.9950 loss_val: 1.5712 acc_val: 0.8783\n",
            "Epoch: 0046 loss_train: 0.1260 acc_train: 0.9924 loss_val: 1.5838 acc_val: 0.8763\n",
            "Epoch: 0047 loss_train: 0.1548 acc_train: 0.9919 loss_val: 1.7073 acc_val: 0.8656\n",
            "Epoch: 0048 loss_train: 0.2139 acc_train: 0.9872 loss_val: 1.5707 acc_val: 0.8725\n",
            "Epoch: 0049 loss_train: 0.2478 acc_train: 0.9866 loss_val: 1.5039 acc_val: 0.8775\n",
            "Epoch: 0050 loss_train: 0.2309 acc_train: 0.9861 loss_val: 1.3982 acc_val: 0.8780\n",
            "Epoch: 0051 loss_train: 0.2149 acc_train: 0.9880 loss_val: 1.3446 acc_val: 0.8712\n",
            "Epoch: 0052 loss_train: 0.1672 acc_train: 0.9902 loss_val: 1.4230 acc_val: 0.8740\n",
            "Epoch: 0053 loss_train: 0.1465 acc_train: 0.9921 loss_val: 1.3990 acc_val: 0.8803\n",
            "Epoch: 0054 loss_train: 0.1034 acc_train: 0.9943 loss_val: 1.4059 acc_val: 0.8791\n",
            "Epoch: 0055 loss_train: 0.0760 acc_train: 0.9961 loss_val: 1.4440 acc_val: 0.8778\n",
            "Epoch: 0056 loss_train: 0.0586 acc_train: 0.9970 loss_val: 1.6051 acc_val: 0.8750\n",
            "Epoch: 0057 loss_train: 0.0729 acc_train: 0.9955 loss_val: 1.6664 acc_val: 0.8775\n",
            "Epoch: 0058 loss_train: 0.1116 acc_train: 0.9930 loss_val: 1.6312 acc_val: 0.8745\n",
            "Epoch: 0059 loss_train: 0.2178 acc_train: 0.9881 loss_val: 1.9535 acc_val: 0.8451\n",
            "Epoch: 0060 loss_train: 0.4945 acc_train: 0.9735 loss_val: 1.6471 acc_val: 0.8636\n",
            "Epoch: 0061 loss_train: 0.3522 acc_train: 0.9788 loss_val: 1.1348 acc_val: 0.8732\n",
            "Epoch: 0062 loss_train: 0.3408 acc_train: 0.9807 loss_val: 1.1913 acc_val: 0.8818\n",
            "Epoch: 0063 loss_train: 0.2537 acc_train: 0.9848 loss_val: 1.1722 acc_val: 0.8725\n",
            "Epoch: 0064 loss_train: 0.1675 acc_train: 0.9908 loss_val: 1.1968 acc_val: 0.8834\n",
            "Epoch: 0065 loss_train: 0.0957 acc_train: 0.9954 loss_val: 1.2707 acc_val: 0.8813\n",
            "Epoch: 0066 loss_train: 0.0746 acc_train: 0.9964 loss_val: 1.4198 acc_val: 0.8811\n",
            "Epoch: 0067 loss_train: 0.0550 acc_train: 0.9964 loss_val: 1.4792 acc_val: 0.8778\n",
            "Epoch: 0068 loss_train: 0.0414 acc_train: 0.9977 loss_val: 1.5503 acc_val: 0.8854\n",
            "Epoch: 0069 loss_train: 0.0393 acc_train: 0.9980 loss_val: 1.5880 acc_val: 0.8765\n",
            "Epoch: 0070 loss_train: 0.0252 acc_train: 0.9988 loss_val: 1.5868 acc_val: 0.8851\n",
            "Epoch: 0071 loss_train: 0.0212 acc_train: 0.9990 loss_val: 1.6438 acc_val: 0.8824\n",
            "Epoch: 0072 loss_train: 0.0308 acc_train: 0.9984 loss_val: 1.8025 acc_val: 0.8717\n",
            "Optimization Finished!\n",
            "Train cost: 57.9187s\n",
            "Loading 22th epoch\n",
            "Test set results: loss= 0.7247 accuracy= 0.8841\n",
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:354: DGLWarning: DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\n",
            "  dgl_warning(\"DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\")\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=515, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=3, bias=True)\n",
            ")\n",
            "total params: 2501125\n",
            "Epoch: 0001 loss_train: 6.4406 acc_train: 0.4287 loss_val: 1.9885 acc_val: 0.5892\n",
            "Epoch: 0002 loss_train: 5.6149 acc_train: 0.6323 loss_val: 1.6443 acc_val: 0.6767\n",
            "Epoch: 0003 loss_train: 4.6597 acc_train: 0.6860 loss_val: 1.3244 acc_val: 0.7459\n",
            "Epoch: 0004 loss_train: 3.8341 acc_train: 0.7515 loss_val: 1.1463 acc_val: 0.7754\n",
            "Epoch: 0005 loss_train: 3.3929 acc_train: 0.7735 loss_val: 1.0611 acc_val: 0.7893\n",
            "Epoch: 0006 loss_train: 3.1099 acc_train: 0.7896 loss_val: 0.9666 acc_val: 0.8106\n",
            "Epoch: 0007 loss_train: 2.8246 acc_train: 0.8119 loss_val: 0.8885 acc_val: 0.8235\n",
            "Epoch: 0008 loss_train: 2.5580 acc_train: 0.8292 loss_val: 0.8178 acc_val: 0.8413\n",
            "Epoch: 0009 loss_train: 2.3220 acc_train: 0.8447 loss_val: 0.7651 acc_val: 0.8555\n",
            "Epoch: 0010 loss_train: 2.1406 acc_train: 0.8585 loss_val: 0.7289 acc_val: 0.8618\n",
            "Epoch: 0011 loss_train: 1.9611 acc_train: 0.8709 loss_val: 0.6913 acc_val: 0.8707\n",
            "Epoch: 0012 loss_train: 1.8167 acc_train: 0.8819 loss_val: 0.6735 acc_val: 0.8755\n",
            "Epoch: 0013 loss_train: 1.6737 acc_train: 0.8942 loss_val: 0.6613 acc_val: 0.8801\n",
            "Epoch: 0014 loss_train: 1.5667 acc_train: 0.9004 loss_val: 0.6620 acc_val: 0.8854\n",
            "Epoch: 0015 loss_train: 1.4752 acc_train: 0.9079 loss_val: 0.6371 acc_val: 0.8846\n",
            "Epoch: 0016 loss_train: 1.3863 acc_train: 0.9123 loss_val: 0.6500 acc_val: 0.8826\n",
            "Epoch: 0017 loss_train: 1.2990 acc_train: 0.9210 loss_val: 0.7068 acc_val: 0.8722\n",
            "Epoch: 0018 loss_train: 1.3910 acc_train: 0.9085 loss_val: 0.6809 acc_val: 0.8763\n",
            "Epoch: 0019 loss_train: 1.3450 acc_train: 0.9157 loss_val: 0.6689 acc_val: 0.8737\n",
            "Epoch: 0020 loss_train: 1.1988 acc_train: 0.9222 loss_val: 0.6534 acc_val: 0.8915\n",
            "Epoch: 0021 loss_train: 1.0823 acc_train: 0.9317 loss_val: 0.7230 acc_val: 0.8829\n",
            "Epoch: 0022 loss_train: 0.9991 acc_train: 0.9385 loss_val: 0.6924 acc_val: 0.8867\n",
            "Epoch: 0023 loss_train: 0.9064 acc_train: 0.9460 loss_val: 0.7059 acc_val: 0.8877\n",
            "Epoch: 0024 loss_train: 0.9205 acc_train: 0.9411 loss_val: 0.7509 acc_val: 0.8816\n",
            "Epoch: 0025 loss_train: 0.7504 acc_train: 0.9557 loss_val: 0.7492 acc_val: 0.8907\n",
            "Epoch: 0026 loss_train: 0.6335 acc_train: 0.9664 loss_val: 0.7851 acc_val: 0.8887\n",
            "Epoch: 0027 loss_train: 0.5496 acc_train: 0.9693 loss_val: 0.8479 acc_val: 0.8895\n",
            "Epoch: 0028 loss_train: 0.4719 acc_train: 0.9746 loss_val: 0.9471 acc_val: 0.8841\n",
            "Epoch: 0029 loss_train: 0.4674 acc_train: 0.9735 loss_val: 0.9378 acc_val: 0.8806\n",
            "Epoch: 0030 loss_train: 0.5571 acc_train: 0.9647 loss_val: 1.3244 acc_val: 0.8438\n",
            "Epoch: 0031 loss_train: 1.5416 acc_train: 0.9075 loss_val: 0.8898 acc_val: 0.8702\n",
            "Epoch: 0032 loss_train: 1.0276 acc_train: 0.9381 loss_val: 0.8546 acc_val: 0.8684\n",
            "Epoch: 0033 loss_train: 0.7790 acc_train: 0.9508 loss_val: 0.8513 acc_val: 0.8824\n",
            "Epoch: 0034 loss_train: 0.6046 acc_train: 0.9651 loss_val: 0.8170 acc_val: 0.8869\n",
            "Epoch: 0035 loss_train: 0.4240 acc_train: 0.9773 loss_val: 0.9151 acc_val: 0.8831\n",
            "Epoch: 0036 loss_train: 0.3078 acc_train: 0.9829 loss_val: 1.0144 acc_val: 0.8849\n",
            "Epoch: 0037 loss_train: 0.2113 acc_train: 0.9894 loss_val: 1.0881 acc_val: 0.8839\n",
            "Epoch: 0038 loss_train: 0.1856 acc_train: 0.9907 loss_val: 1.1737 acc_val: 0.8803\n",
            "Epoch: 0039 loss_train: 0.1712 acc_train: 0.9919 loss_val: 1.2500 acc_val: 0.8778\n",
            "Epoch: 0040 loss_train: 0.1401 acc_train: 0.9925 loss_val: 1.2707 acc_val: 0.8824\n",
            "Epoch: 0041 loss_train: 0.1342 acc_train: 0.9925 loss_val: 1.3495 acc_val: 0.8826\n",
            "Epoch: 0042 loss_train: 0.1021 acc_train: 0.9943 loss_val: 1.3954 acc_val: 0.8798\n",
            "Epoch: 0043 loss_train: 0.0876 acc_train: 0.9950 loss_val: 1.4366 acc_val: 0.8780\n",
            "Epoch: 0044 loss_train: 0.0750 acc_train: 0.9951 loss_val: 1.4803 acc_val: 0.8722\n",
            "Epoch: 0045 loss_train: 0.0761 acc_train: 0.9964 loss_val: 1.5631 acc_val: 0.8760\n",
            "Epoch: 0046 loss_train: 0.0659 acc_train: 0.9963 loss_val: 1.5505 acc_val: 0.8785\n",
            "Epoch: 0047 loss_train: 0.0706 acc_train: 0.9959 loss_val: 1.6459 acc_val: 0.8808\n",
            "Epoch: 0048 loss_train: 0.1193 acc_train: 0.9941 loss_val: 1.7435 acc_val: 0.8684\n",
            "Epoch: 0049 loss_train: 0.1788 acc_train: 0.9896 loss_val: 1.6557 acc_val: 0.8656\n",
            "Epoch: 0050 loss_train: 0.2048 acc_train: 0.9878 loss_val: 1.5470 acc_val: 0.8737\n",
            "Epoch: 0051 loss_train: 0.2190 acc_train: 0.9867 loss_val: 1.5409 acc_val: 0.8656\n",
            "Epoch: 0052 loss_train: 0.2026 acc_train: 0.9880 loss_val: 1.3996 acc_val: 0.8659\n",
            "Epoch: 0053 loss_train: 0.1851 acc_train: 0.9891 loss_val: 1.5478 acc_val: 0.8740\n",
            "Epoch: 0054 loss_train: 0.1589 acc_train: 0.9908 loss_val: 1.5006 acc_val: 0.8697\n",
            "Epoch: 0055 loss_train: 0.2432 acc_train: 0.9864 loss_val: 1.5211 acc_val: 0.8727\n",
            "Epoch: 0056 loss_train: 0.2309 acc_train: 0.9865 loss_val: 1.3800 acc_val: 0.8727\n",
            "Epoch: 0057 loss_train: 0.1824 acc_train: 0.9896 loss_val: 1.4773 acc_val: 0.8676\n",
            "Epoch: 0058 loss_train: 0.1893 acc_train: 0.9890 loss_val: 1.3975 acc_val: 0.8737\n",
            "Epoch: 0059 loss_train: 0.1490 acc_train: 0.9920 loss_val: 1.4555 acc_val: 0.8798\n",
            "Epoch: 0060 loss_train: 0.0989 acc_train: 0.9947 loss_val: 1.4341 acc_val: 0.8737\n",
            "Epoch: 0061 loss_train: 0.0593 acc_train: 0.9971 loss_val: 1.5361 acc_val: 0.8768\n",
            "Epoch: 0062 loss_train: 0.0647 acc_train: 0.9972 loss_val: 1.6024 acc_val: 0.8687\n",
            "Epoch: 0063 loss_train: 0.0347 acc_train: 0.9984 loss_val: 1.6840 acc_val: 0.8704\n",
            "Epoch: 0064 loss_train: 0.0299 acc_train: 0.9988 loss_val: 1.7686 acc_val: 0.8755\n",
            "Epoch: 0065 loss_train: 0.0317 acc_train: 0.9982 loss_val: 1.7636 acc_val: 0.8765\n",
            "Epoch: 0066 loss_train: 0.0323 acc_train: 0.9983 loss_val: 1.9067 acc_val: 0.8755\n",
            "Epoch: 0067 loss_train: 0.0454 acc_train: 0.9975 loss_val: 1.8713 acc_val: 0.8725\n",
            "Epoch: 0068 loss_train: 0.0406 acc_train: 0.9979 loss_val: 1.9212 acc_val: 0.8684\n",
            "Epoch: 0069 loss_train: 0.0463 acc_train: 0.9971 loss_val: 1.9371 acc_val: 0.8699\n",
            "Epoch: 0070 loss_train: 0.0588 acc_train: 0.9972 loss_val: 1.8549 acc_val: 0.8760\n",
            "Optimization Finished!\n",
            "Train cost: 52.2059s\n",
            "Loading 20th epoch\n",
            "Test set results: loss= 0.6964 accuracy= 0.8805\n",
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:354: DGLWarning: DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\n",
            "  dgl_warning(\"DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\")\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=515, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=3, bias=True)\n",
            ")\n",
            "total params: 2501125\n",
            "Epoch: 0001 loss_train: 6.4417 acc_train: 0.4259 loss_val: 1.9890 acc_val: 0.6075\n",
            "Epoch: 0002 loss_train: 5.6272 acc_train: 0.6318 loss_val: 1.6520 acc_val: 0.6717\n",
            "Epoch: 0003 loss_train: 4.7118 acc_train: 0.6775 loss_val: 1.3394 acc_val: 0.7391\n",
            "Epoch: 0004 loss_train: 3.9044 acc_train: 0.7432 loss_val: 1.1601 acc_val: 0.7723\n",
            "Epoch: 0005 loss_train: 3.4605 acc_train: 0.7685 loss_val: 1.0742 acc_val: 0.7858\n",
            "Epoch: 0006 loss_train: 3.1755 acc_train: 0.7837 loss_val: 0.9821 acc_val: 0.8038\n",
            "Epoch: 0007 loss_train: 2.8977 acc_train: 0.8067 loss_val: 0.9024 acc_val: 0.8210\n",
            "Epoch: 0008 loss_train: 2.6179 acc_train: 0.8249 loss_val: 0.8305 acc_val: 0.8372\n",
            "Epoch: 0009 loss_train: 2.3692 acc_train: 0.8393 loss_val: 0.7737 acc_val: 0.8547\n",
            "Epoch: 0010 loss_train: 2.1781 acc_train: 0.8552 loss_val: 0.7377 acc_val: 0.8608\n",
            "Epoch: 0011 loss_train: 1.9740 acc_train: 0.8675 loss_val: 0.6991 acc_val: 0.8689\n",
            "Epoch: 0012 loss_train: 1.8384 acc_train: 0.8810 loss_val: 0.6858 acc_val: 0.8720\n",
            "Epoch: 0013 loss_train: 1.6818 acc_train: 0.8922 loss_val: 0.6627 acc_val: 0.8801\n",
            "Epoch: 0014 loss_train: 1.5782 acc_train: 0.9018 loss_val: 0.6708 acc_val: 0.8806\n",
            "Epoch: 0015 loss_train: 1.5347 acc_train: 0.8997 loss_val: 0.6717 acc_val: 0.8783\n",
            "Epoch: 0016 loss_train: 1.4419 acc_train: 0.9070 loss_val: 0.6371 acc_val: 0.8836\n",
            "Epoch: 0017 loss_train: 1.3051 acc_train: 0.9193 loss_val: 0.6514 acc_val: 0.8829\n",
            "Epoch: 0018 loss_train: 1.2726 acc_train: 0.9202 loss_val: 0.7487 acc_val: 0.8669\n",
            "Epoch: 0019 loss_train: 1.2901 acc_train: 0.9156 loss_val: 0.7216 acc_val: 0.8740\n",
            "Epoch: 0020 loss_train: 1.1848 acc_train: 0.9273 loss_val: 0.6975 acc_val: 0.8758\n",
            "Epoch: 0021 loss_train: 1.1341 acc_train: 0.9276 loss_val: 0.6960 acc_val: 0.8844\n",
            "Epoch: 0022 loss_train: 1.0309 acc_train: 0.9347 loss_val: 0.6921 acc_val: 0.8882\n",
            "Epoch: 0023 loss_train: 0.9499 acc_train: 0.9439 loss_val: 0.7158 acc_val: 0.8818\n",
            "Epoch: 0024 loss_train: 0.8538 acc_train: 0.9496 loss_val: 0.7248 acc_val: 0.8849\n",
            "Epoch: 0025 loss_train: 0.7351 acc_train: 0.9581 loss_val: 0.7631 acc_val: 0.8798\n",
            "Epoch: 0026 loss_train: 0.6172 acc_train: 0.9677 loss_val: 0.8216 acc_val: 0.8831\n",
            "Epoch: 0027 loss_train: 0.5615 acc_train: 0.9691 loss_val: 0.8991 acc_val: 0.8818\n",
            "Epoch: 0028 loss_train: 0.4852 acc_train: 0.9735 loss_val: 0.9509 acc_val: 0.8780\n",
            "Epoch: 0029 loss_train: 0.4331 acc_train: 0.9745 loss_val: 1.0207 acc_val: 0.8778\n",
            "Epoch: 0030 loss_train: 0.4475 acc_train: 0.9745 loss_val: 1.0186 acc_val: 0.8773\n",
            "Epoch: 0031 loss_train: 0.3916 acc_train: 0.9780 loss_val: 1.1614 acc_val: 0.8755\n",
            "Epoch: 0032 loss_train: 0.3870 acc_train: 0.9741 loss_val: 1.4577 acc_val: 0.8611\n",
            "Epoch: 0033 loss_train: 1.0781 acc_train: 0.9378 loss_val: 1.1499 acc_val: 0.8684\n",
            "Epoch: 0034 loss_train: 0.6947 acc_train: 0.9594 loss_val: 0.9311 acc_val: 0.8656\n",
            "Epoch: 0035 loss_train: 0.5278 acc_train: 0.9665 loss_val: 1.0336 acc_val: 0.8687\n",
            "Epoch: 0036 loss_train: 0.4250 acc_train: 0.9765 loss_val: 1.0608 acc_val: 0.8735\n",
            "Epoch: 0037 loss_train: 0.2996 acc_train: 0.9832 loss_val: 1.1202 acc_val: 0.8737\n",
            "Epoch: 0038 loss_train: 0.2217 acc_train: 0.9882 loss_val: 1.2450 acc_val: 0.8692\n",
            "Epoch: 0039 loss_train: 0.1865 acc_train: 0.9894 loss_val: 1.3008 acc_val: 0.8709\n",
            "Epoch: 0040 loss_train: 0.1721 acc_train: 0.9914 loss_val: 1.3493 acc_val: 0.8742\n",
            "Epoch: 0041 loss_train: 0.1433 acc_train: 0.9927 loss_val: 1.5077 acc_val: 0.8730\n",
            "Epoch: 0042 loss_train: 0.1414 acc_train: 0.9920 loss_val: 1.5194 acc_val: 0.8699\n",
            "Epoch: 0043 loss_train: 0.1383 acc_train: 0.9921 loss_val: 1.5798 acc_val: 0.8742\n",
            "Epoch: 0044 loss_train: 0.1376 acc_train: 0.9918 loss_val: 1.6024 acc_val: 0.8763\n",
            "Epoch: 0045 loss_train: 0.1484 acc_train: 0.9921 loss_val: 1.5817 acc_val: 0.8631\n",
            "Epoch: 0046 loss_train: 0.1328 acc_train: 0.9922 loss_val: 1.5749 acc_val: 0.8760\n",
            "Epoch: 0047 loss_train: 0.1270 acc_train: 0.9923 loss_val: 1.5784 acc_val: 0.8715\n",
            "Epoch: 0048 loss_train: 0.1068 acc_train: 0.9939 loss_val: 1.6249 acc_val: 0.8715\n",
            "Epoch: 0049 loss_train: 0.1132 acc_train: 0.9941 loss_val: 1.7187 acc_val: 0.8758\n",
            "Epoch: 0050 loss_train: 0.1048 acc_train: 0.9939 loss_val: 1.7130 acc_val: 0.8715\n",
            "Epoch: 0051 loss_train: 0.0796 acc_train: 0.9948 loss_val: 1.7050 acc_val: 0.8715\n",
            "Epoch: 0052 loss_train: 0.0782 acc_train: 0.9951 loss_val: 1.6908 acc_val: 0.8682\n",
            "Epoch: 0053 loss_train: 0.0780 acc_train: 0.9955 loss_val: 1.7922 acc_val: 0.8758\n",
            "Epoch: 0054 loss_train: 0.1090 acc_train: 0.9943 loss_val: 1.9216 acc_val: 0.8735\n",
            "Epoch: 0055 loss_train: 0.1562 acc_train: 0.9911 loss_val: 2.1557 acc_val: 0.8438\n",
            "Epoch: 0056 loss_train: 6.1454 acc_train: 0.8131 loss_val: 1.1749 acc_val: 0.7875\n",
            "Epoch: 0057 loss_train: 3.0493 acc_train: 0.8292 loss_val: 1.7827 acc_val: 0.7234\n",
            "Epoch: 0058 loss_train: 3.1142 acc_train: 0.8128 loss_val: 0.7884 acc_val: 0.8545\n",
            "Epoch: 0059 loss_train: 2.2006 acc_train: 0.8580 loss_val: 0.8578 acc_val: 0.8463\n",
            "Epoch: 0060 loss_train: 2.0337 acc_train: 0.8728 loss_val: 0.7436 acc_val: 0.8651\n",
            "Epoch: 0061 loss_train: 1.8408 acc_train: 0.8844 loss_val: 0.7254 acc_val: 0.8618\n",
            "Epoch: 0062 loss_train: 1.7569 acc_train: 0.8900 loss_val: 0.6703 acc_val: 0.8737\n",
            "Epoch: 0063 loss_train: 1.6010 acc_train: 0.9019 loss_val: 0.6877 acc_val: 0.8760\n",
            "Epoch: 0064 loss_train: 1.4899 acc_train: 0.9098 loss_val: 0.6483 acc_val: 0.8829\n",
            "Epoch: 0065 loss_train: 1.3990 acc_train: 0.9161 loss_val: 0.6507 acc_val: 0.8824\n",
            "Epoch: 0066 loss_train: 1.3086 acc_train: 0.9196 loss_val: 0.6570 acc_val: 0.8851\n",
            "Epoch: 0067 loss_train: 1.2320 acc_train: 0.9244 loss_val: 0.6740 acc_val: 0.8851\n",
            "Epoch: 0068 loss_train: 1.1792 acc_train: 0.9286 loss_val: 0.7206 acc_val: 0.8750\n",
            "Epoch: 0069 loss_train: 1.1334 acc_train: 0.9331 loss_val: 0.7133 acc_val: 0.8818\n",
            "Epoch: 0070 loss_train: 1.0730 acc_train: 0.9351 loss_val: 0.7102 acc_val: 0.8824\n",
            "Epoch: 0071 loss_train: 0.9536 acc_train: 0.9445 loss_val: 0.7357 acc_val: 0.8811\n",
            "Epoch: 0072 loss_train: 0.9017 acc_train: 0.9484 loss_val: 0.8360 acc_val: 0.8753\n",
            "Optimization Finished!\n",
            "Train cost: 52.4891s\n",
            "Loading 22th epoch\n",
            "Test set results: loss= 0.7415 accuracy= 0.8783\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --dataset pubmed --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 7  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack l2_strong --attperc 10 --TrainTest train --weight_decay=1e-05\n",
        "\n",
        "!python train.py --dataset pubmed --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 7  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack l2_strong --attperc 40 --TrainTest train --weight_decay=1e-05\n",
        "\n",
        "!python train.py --dataset pubmed --epoch 200 --batch_size 2000 --dropout 0.1 --hidden_dim 512 \\\n",
        "          --hops 7  --n_heads 8 --n_layers 1 --pe_dim 15 --peak_lr 0.001  --attack dil2_strongce --attperc 60 --TrainTest train --weight_decay=1e-05\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYgsAAJ2asKI",
        "outputId": "bbd6c853-5326-490f-ea4c-afb172452354"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:354: DGLWarning: DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\n",
            "  dgl_warning(\"DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\")\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=515, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=3, bias=True)\n",
            ")\n",
            "total params: 2501125\n",
            "Epoch: 0001 loss_train: 6.5303 acc_train: 0.3875 loss_val: 2.0254 acc_val: 0.5271\n",
            "Epoch: 0002 loss_train: 5.6630 acc_train: 0.6292 loss_val: 1.6799 acc_val: 0.6516\n",
            "Epoch: 0003 loss_train: 4.5763 acc_train: 0.7084 loss_val: 1.3146 acc_val: 0.7619\n",
            "Epoch: 0004 loss_train: 3.5211 acc_train: 0.7901 loss_val: 1.1118 acc_val: 0.7964\n",
            "Epoch: 0005 loss_train: 3.0029 acc_train: 0.8117 loss_val: 1.0319 acc_val: 0.8073\n",
            "Epoch: 0006 loss_train: 2.7620 acc_train: 0.8243 loss_val: 0.9386 acc_val: 0.8162\n",
            "Epoch: 0007 loss_train: 2.4906 acc_train: 0.8405 loss_val: 0.8448 acc_val: 0.8360\n",
            "Epoch: 0008 loss_train: 2.2433 acc_train: 0.8531 loss_val: 0.7972 acc_val: 0.8486\n",
            "Epoch: 0009 loss_train: 2.0581 acc_train: 0.8675 loss_val: 0.7470 acc_val: 0.8613\n",
            "Epoch: 0010 loss_train: 1.9095 acc_train: 0.8756 loss_val: 0.7204 acc_val: 0.8656\n",
            "Epoch: 0011 loss_train: 1.7516 acc_train: 0.8853 loss_val: 0.6823 acc_val: 0.8737\n",
            "Epoch: 0012 loss_train: 1.6091 acc_train: 0.8958 loss_val: 0.6766 acc_val: 0.8747\n",
            "Epoch: 0013 loss_train: 1.4963 acc_train: 0.9057 loss_val: 0.6610 acc_val: 0.8783\n",
            "Epoch: 0014 loss_train: 1.4062 acc_train: 0.9126 loss_val: 0.6802 acc_val: 0.8816\n",
            "Epoch: 0015 loss_train: 1.3630 acc_train: 0.9115 loss_val: 0.7180 acc_val: 0.8745\n",
            "Epoch: 0016 loss_train: 1.2854 acc_train: 0.9183 loss_val: 0.6528 acc_val: 0.8806\n",
            "Epoch: 0017 loss_train: 1.1827 acc_train: 0.9271 loss_val: 0.6779 acc_val: 0.8813\n",
            "Epoch: 0018 loss_train: 1.1113 acc_train: 0.9309 loss_val: 0.7223 acc_val: 0.8768\n",
            "Epoch: 0019 loss_train: 1.0452 acc_train: 0.9339 loss_val: 0.6961 acc_val: 0.8808\n",
            "Epoch: 0020 loss_train: 0.9918 acc_train: 0.9380 loss_val: 0.7167 acc_val: 0.8816\n",
            "Epoch: 0021 loss_train: 0.8772 acc_train: 0.9471 loss_val: 0.7380 acc_val: 0.8849\n",
            "Epoch: 0022 loss_train: 0.8502 acc_train: 0.9466 loss_val: 0.7526 acc_val: 0.8811\n",
            "Epoch: 0023 loss_train: 0.7397 acc_train: 0.9547 loss_val: 0.7562 acc_val: 0.8851\n",
            "Epoch: 0024 loss_train: 0.6887 acc_train: 0.9581 loss_val: 0.7921 acc_val: 0.8834\n",
            "Epoch: 0025 loss_train: 0.7350 acc_train: 0.9563 loss_val: 1.2370 acc_val: 0.8306\n",
            "Epoch: 0026 loss_train: 1.1624 acc_train: 0.9260 loss_val: 0.8184 acc_val: 0.8651\n",
            "Epoch: 0027 loss_train: 0.8542 acc_train: 0.9431 loss_val: 0.7431 acc_val: 0.8765\n",
            "Epoch: 0028 loss_train: 0.7137 acc_train: 0.9543 loss_val: 0.7906 acc_val: 0.8788\n",
            "Epoch: 0029 loss_train: 0.5731 acc_train: 0.9667 loss_val: 0.7792 acc_val: 0.8869\n",
            "Epoch: 0030 loss_train: 0.4609 acc_train: 0.9753 loss_val: 0.9006 acc_val: 0.8831\n",
            "Epoch: 0031 loss_train: 0.3788 acc_train: 0.9788 loss_val: 0.9793 acc_val: 0.8826\n",
            "Epoch: 0032 loss_train: 0.3058 acc_train: 0.9844 loss_val: 0.9930 acc_val: 0.8818\n",
            "Epoch: 0033 loss_train: 0.2456 acc_train: 0.9875 loss_val: 1.0986 acc_val: 0.8775\n",
            "Epoch: 0034 loss_train: 0.2505 acc_train: 0.9853 loss_val: 1.1637 acc_val: 0.8798\n",
            "Epoch: 0035 loss_train: 0.2733 acc_train: 0.9835 loss_val: 1.1403 acc_val: 0.8773\n",
            "Epoch: 0036 loss_train: 0.2702 acc_train: 0.9844 loss_val: 1.1749 acc_val: 0.8778\n",
            "Epoch: 0037 loss_train: 0.2666 acc_train: 0.9825 loss_val: 1.2219 acc_val: 0.8709\n",
            "Epoch: 0038 loss_train: 0.2061 acc_train: 0.9878 loss_val: 1.2484 acc_val: 0.8768\n",
            "Epoch: 0039 loss_train: 0.2088 acc_train: 0.9874 loss_val: 1.3385 acc_val: 0.8699\n",
            "Epoch: 0040 loss_train: 0.1781 acc_train: 0.9890 loss_val: 1.2860 acc_val: 0.8778\n",
            "Epoch: 0041 loss_train: 0.1409 acc_train: 0.9915 loss_val: 1.3734 acc_val: 0.8765\n",
            "Epoch: 0042 loss_train: 0.1206 acc_train: 0.9928 loss_val: 1.3797 acc_val: 0.8839\n",
            "Epoch: 0043 loss_train: 0.1033 acc_train: 0.9933 loss_val: 1.4117 acc_val: 0.8773\n",
            "Epoch: 0044 loss_train: 0.0838 acc_train: 0.9953 loss_val: 1.5142 acc_val: 0.8796\n",
            "Epoch: 0045 loss_train: 0.0964 acc_train: 0.9942 loss_val: 1.5348 acc_val: 0.8793\n",
            "Epoch: 0046 loss_train: 0.0775 acc_train: 0.9957 loss_val: 1.5597 acc_val: 0.8755\n",
            "Epoch: 0047 loss_train: 0.0924 acc_train: 0.9941 loss_val: 1.6042 acc_val: 0.8788\n",
            "Epoch: 0048 loss_train: 0.1434 acc_train: 0.9919 loss_val: 1.4984 acc_val: 0.8770\n",
            "Epoch: 0049 loss_train: 0.2155 acc_train: 0.9873 loss_val: 1.7099 acc_val: 0.8580\n",
            "Epoch: 0050 loss_train: 0.3499 acc_train: 0.9801 loss_val: 1.3990 acc_val: 0.8755\n",
            "Epoch: 0051 loss_train: 0.2946 acc_train: 0.9822 loss_val: 1.3465 acc_val: 0.8509\n",
            "Epoch: 0052 loss_train: 0.3301 acc_train: 0.9811 loss_val: 1.3338 acc_val: 0.8770\n",
            "Epoch: 0053 loss_train: 0.2896 acc_train: 0.9824 loss_val: 1.2720 acc_val: 0.8646\n",
            "Epoch: 0054 loss_train: 0.2209 acc_train: 0.9866 loss_val: 1.1764 acc_val: 0.8798\n",
            "Epoch: 0055 loss_train: 0.1574 acc_train: 0.9905 loss_val: 1.2959 acc_val: 0.8780\n",
            "Epoch: 0056 loss_train: 0.1271 acc_train: 0.9927 loss_val: 1.4403 acc_val: 0.8788\n",
            "Epoch: 0057 loss_train: 0.0884 acc_train: 0.9948 loss_val: 1.4144 acc_val: 0.8851\n",
            "Epoch: 0058 loss_train: 0.0588 acc_train: 0.9965 loss_val: 1.4775 acc_val: 0.8775\n",
            "Epoch: 0059 loss_train: 0.0473 acc_train: 0.9975 loss_val: 1.5737 acc_val: 0.8839\n",
            "Epoch: 0060 loss_train: 0.0301 acc_train: 0.9983 loss_val: 1.6047 acc_val: 0.8803\n",
            "Epoch: 0061 loss_train: 0.0243 acc_train: 0.9991 loss_val: 1.6485 acc_val: 0.8839\n",
            "Epoch: 0062 loss_train: 0.0288 acc_train: 0.9984 loss_val: 1.7709 acc_val: 0.8763\n",
            "Epoch: 0063 loss_train: 0.0615 acc_train: 0.9972 loss_val: 1.8441 acc_val: 0.8727\n",
            "Epoch: 0064 loss_train: 0.0687 acc_train: 0.9973 loss_val: 1.7682 acc_val: 0.8770\n",
            "Epoch: 0065 loss_train: 0.0843 acc_train: 0.9962 loss_val: 1.6808 acc_val: 0.8813\n",
            "Epoch: 0066 loss_train: 0.0637 acc_train: 0.9964 loss_val: 1.6250 acc_val: 0.8798\n",
            "Epoch: 0067 loss_train: 0.0630 acc_train: 0.9961 loss_val: 1.6604 acc_val: 0.8732\n",
            "Epoch: 0068 loss_train: 0.0620 acc_train: 0.9969 loss_val: 1.5881 acc_val: 0.8801\n",
            "Epoch: 0069 loss_train: 0.0606 acc_train: 0.9967 loss_val: 1.5353 acc_val: 0.8796\n",
            "Epoch: 0070 loss_train: 0.0524 acc_train: 0.9971 loss_val: 1.6288 acc_val: 0.8755\n",
            "Epoch: 0071 loss_train: 0.0536 acc_train: 0.9973 loss_val: 1.6654 acc_val: 0.8725\n",
            "Epoch: 0072 loss_train: 0.0450 acc_train: 0.9974 loss_val: 1.6425 acc_val: 0.8798\n",
            "Epoch: 0073 loss_train: 0.0244 acc_train: 0.9987 loss_val: 1.6806 acc_val: 0.8829\n",
            "Epoch: 0074 loss_train: 0.0214 acc_train: 0.9989 loss_val: 1.7455 acc_val: 0.8796\n",
            "Epoch: 0075 loss_train: 0.0244 acc_train: 0.9987 loss_val: 1.8743 acc_val: 0.8803\n",
            "Epoch: 0076 loss_train: 0.0342 acc_train: 0.9983 loss_val: 1.8110 acc_val: 0.8818\n",
            "Epoch: 0077 loss_train: 0.0283 acc_train: 0.9986 loss_val: 1.9097 acc_val: 0.8811\n",
            "Epoch: 0078 loss_train: 0.0359 acc_train: 0.9978 loss_val: 1.9340 acc_val: 0.8798\n",
            "Epoch: 0079 loss_train: 0.0331 acc_train: 0.9982 loss_val: 1.9287 acc_val: 0.8791\n",
            "Optimization Finished!\n",
            "Train cost: 59.9604s\n",
            "Loading 29th epoch\n",
            "Test set results: loss= 0.8090 accuracy= 0.8838\n",
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:354: DGLWarning: DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\n",
            "  dgl_warning(\"DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\")\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=515, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=3, bias=True)\n",
            ")\n",
            "total params: 2501125\n",
            "Epoch: 0001 loss_train: 6.3610 acc_train: 0.4285 loss_val: 1.9850 acc_val: 0.6263\n",
            "Epoch: 0002 loss_train: 5.4764 acc_train: 0.6991 loss_val: 1.6840 acc_val: 0.6678\n",
            "Epoch: 0003 loss_train: 4.4177 acc_train: 0.7110 loss_val: 1.4165 acc_val: 0.6803\n",
            "Epoch: 0004 loss_train: 3.5678 acc_train: 0.7658 loss_val: 1.2476 acc_val: 0.7852\n",
            "Epoch: 0005 loss_train: 2.9295 acc_train: 0.8199 loss_val: 1.0361 acc_val: 0.8068\n",
            "Epoch: 0006 loss_train: 2.4760 acc_train: 0.8490 loss_val: 0.9668 acc_val: 0.8149\n",
            "Epoch: 0007 loss_train: 2.2355 acc_train: 0.8649 loss_val: 0.9261 acc_val: 0.8223\n",
            "Epoch: 0008 loss_train: 2.0435 acc_train: 0.8722 loss_val: 0.8522 acc_val: 0.8390\n",
            "Epoch: 0009 loss_train: 1.8445 acc_train: 0.8823 loss_val: 0.7992 acc_val: 0.8484\n",
            "Epoch: 0010 loss_train: 1.7038 acc_train: 0.8912 loss_val: 0.7749 acc_val: 0.8534\n",
            "Epoch: 0011 loss_train: 1.5753 acc_train: 0.8999 loss_val: 0.7563 acc_val: 0.8600\n",
            "Epoch: 0012 loss_train: 1.4752 acc_train: 0.9066 loss_val: 0.7401 acc_val: 0.8664\n",
            "Epoch: 0013 loss_train: 1.3672 acc_train: 0.9160 loss_val: 0.6874 acc_val: 0.8750\n",
            "Epoch: 0014 loss_train: 1.2395 acc_train: 0.9230 loss_val: 0.6894 acc_val: 0.8778\n",
            "Epoch: 0015 loss_train: 1.1575 acc_train: 0.9273 loss_val: 0.7968 acc_val: 0.8661\n",
            "Epoch: 0016 loss_train: 1.1910 acc_train: 0.9216 loss_val: 0.6944 acc_val: 0.8760\n",
            "Epoch: 0017 loss_train: 1.0551 acc_train: 0.9339 loss_val: 0.7123 acc_val: 0.8755\n",
            "Epoch: 0018 loss_train: 0.9762 acc_train: 0.9377 loss_val: 0.7091 acc_val: 0.8758\n",
            "Epoch: 0019 loss_train: 0.8808 acc_train: 0.9455 loss_val: 0.7093 acc_val: 0.8808\n",
            "Epoch: 0020 loss_train: 0.8104 acc_train: 0.9506 loss_val: 0.7439 acc_val: 0.8801\n",
            "Epoch: 0021 loss_train: 0.7588 acc_train: 0.9532 loss_val: 0.7922 acc_val: 0.8770\n",
            "Epoch: 0022 loss_train: 0.7360 acc_train: 0.9554 loss_val: 0.8561 acc_val: 0.8666\n",
            "Epoch: 0023 loss_train: 0.7258 acc_train: 0.9538 loss_val: 0.8059 acc_val: 0.8742\n",
            "Epoch: 0024 loss_train: 0.6222 acc_train: 0.9623 loss_val: 0.8213 acc_val: 0.8778\n",
            "Epoch: 0025 loss_train: 0.5760 acc_train: 0.9644 loss_val: 0.8821 acc_val: 0.8709\n",
            "Epoch: 0026 loss_train: 0.5059 acc_train: 0.9697 loss_val: 0.8830 acc_val: 0.8798\n",
            "Epoch: 0027 loss_train: 0.4360 acc_train: 0.9743 loss_val: 0.9486 acc_val: 0.8780\n",
            "Epoch: 0028 loss_train: 0.3864 acc_train: 0.9764 loss_val: 0.9831 acc_val: 0.8775\n",
            "Epoch: 0029 loss_train: 0.3156 acc_train: 0.9817 loss_val: 1.0288 acc_val: 0.8778\n",
            "Epoch: 0030 loss_train: 0.2923 acc_train: 0.9821 loss_val: 1.2727 acc_val: 0.8621\n",
            "Epoch: 0031 loss_train: 0.3622 acc_train: 0.9766 loss_val: 1.1583 acc_val: 0.8628\n",
            "Epoch: 0032 loss_train: 0.4382 acc_train: 0.9720 loss_val: 1.1038 acc_val: 0.8669\n",
            "Epoch: 0033 loss_train: 0.3199 acc_train: 0.9809 loss_val: 1.1490 acc_val: 0.8796\n",
            "Epoch: 0034 loss_train: 0.2165 acc_train: 0.9870 loss_val: 1.1983 acc_val: 0.8735\n",
            "Epoch: 0035 loss_train: 0.1771 acc_train: 0.9899 loss_val: 1.1998 acc_val: 0.8770\n",
            "Epoch: 0036 loss_train: 0.1945 acc_train: 0.9884 loss_val: 1.3510 acc_val: 0.8758\n",
            "Epoch: 0037 loss_train: 0.3156 acc_train: 0.9815 loss_val: 1.4140 acc_val: 0.8707\n",
            "Epoch: 0038 loss_train: 0.4489 acc_train: 0.9757 loss_val: 1.2345 acc_val: 0.8697\n",
            "Epoch: 0039 loss_train: 0.3174 acc_train: 0.9785 loss_val: 1.2685 acc_val: 0.8656\n",
            "Epoch: 0040 loss_train: 0.2226 acc_train: 0.9873 loss_val: 1.1903 acc_val: 0.8715\n",
            "Epoch: 0041 loss_train: 0.1482 acc_train: 0.9912 loss_val: 1.2400 acc_val: 0.8740\n",
            "Epoch: 0042 loss_train: 0.0933 acc_train: 0.9952 loss_val: 1.3790 acc_val: 0.8788\n",
            "Epoch: 0043 loss_train: 0.0702 acc_train: 0.9962 loss_val: 1.4165 acc_val: 0.8793\n",
            "Epoch: 0044 loss_train: 0.0506 acc_train: 0.9976 loss_val: 1.4996 acc_val: 0.8780\n",
            "Epoch: 0045 loss_train: 0.0431 acc_train: 0.9980 loss_val: 1.5369 acc_val: 0.8765\n",
            "Epoch: 0046 loss_train: 0.0302 acc_train: 0.9988 loss_val: 1.6066 acc_val: 0.8760\n",
            "Epoch: 0047 loss_train: 0.0310 acc_train: 0.9989 loss_val: 1.5882 acc_val: 0.8765\n",
            "Epoch: 0048 loss_train: 0.0264 acc_train: 0.9987 loss_val: 1.7053 acc_val: 0.8791\n",
            "Epoch: 0049 loss_train: 0.0360 acc_train: 0.9987 loss_val: 1.7393 acc_val: 0.8712\n",
            "Epoch: 0050 loss_train: 0.0319 acc_train: 0.9986 loss_val: 1.7988 acc_val: 0.8773\n",
            "Epoch: 0051 loss_train: 0.0594 acc_train: 0.9969 loss_val: 1.8093 acc_val: 0.8709\n",
            "Epoch: 0052 loss_train: 0.2383 acc_train: 0.9891 loss_val: 3.5193 acc_val: 0.7896\n",
            "Epoch: 0053 loss_train: 4.6818 acc_train: 0.7839 loss_val: 1.2880 acc_val: 0.7974\n",
            "Epoch: 0054 loss_train: 3.4260 acc_train: 0.8120 loss_val: 0.9128 acc_val: 0.8327\n",
            "Epoch: 0055 loss_train: 2.0484 acc_train: 0.8887 loss_val: 0.9514 acc_val: 0.8626\n",
            "Epoch: 0056 loss_train: 1.7329 acc_train: 0.9024 loss_val: 0.7372 acc_val: 0.8674\n",
            "Epoch: 0057 loss_train: 1.5378 acc_train: 0.9064 loss_val: 0.7044 acc_val: 0.8699\n",
            "Epoch: 0058 loss_train: 1.4291 acc_train: 0.9144 loss_val: 0.7565 acc_val: 0.8623\n",
            "Epoch: 0059 loss_train: 1.4614 acc_train: 0.9110 loss_val: 0.6987 acc_val: 0.8770\n",
            "Epoch: 0060 loss_train: 1.3259 acc_train: 0.9185 loss_val: 0.7490 acc_val: 0.8659\n",
            "Epoch: 0061 loss_train: 1.2446 acc_train: 0.9232 loss_val: 0.6719 acc_val: 0.8793\n",
            "Epoch: 0062 loss_train: 1.1261 acc_train: 0.9341 loss_val: 0.6891 acc_val: 0.8803\n",
            "Epoch: 0063 loss_train: 1.0606 acc_train: 0.9344 loss_val: 0.7319 acc_val: 0.8763\n",
            "Epoch: 0064 loss_train: 1.0535 acc_train: 0.9374 loss_val: 0.6983 acc_val: 0.8796\n",
            "Epoch: 0065 loss_train: 0.9735 acc_train: 0.9410 loss_val: 0.7057 acc_val: 0.8758\n",
            "Epoch: 0066 loss_train: 0.9098 acc_train: 0.9450 loss_val: 0.7422 acc_val: 0.8816\n",
            "Epoch: 0067 loss_train: 0.8733 acc_train: 0.9478 loss_val: 0.7247 acc_val: 0.8839\n",
            "Epoch: 0068 loss_train: 0.7866 acc_train: 0.9553 loss_val: 0.7515 acc_val: 0.8826\n",
            "Epoch: 0069 loss_train: 0.7304 acc_train: 0.9593 loss_val: 0.7890 acc_val: 0.8806\n",
            "Epoch: 0070 loss_train: 0.6633 acc_train: 0.9630 loss_val: 0.8187 acc_val: 0.8770\n",
            "Epoch: 0071 loss_train: 0.6190 acc_train: 0.9661 loss_val: 0.8306 acc_val: 0.8801\n",
            "Epoch: 0072 loss_train: 0.5569 acc_train: 0.9702 loss_val: 0.8822 acc_val: 0.8750\n",
            "Epoch: 0073 loss_train: 0.5330 acc_train: 0.9723 loss_val: 0.9311 acc_val: 0.8755\n",
            "Epoch: 0074 loss_train: 0.5342 acc_train: 0.9718 loss_val: 0.9794 acc_val: 0.8712\n",
            "Epoch: 0075 loss_train: 0.5264 acc_train: 0.9719 loss_val: 0.9884 acc_val: 0.8727\n",
            "Epoch: 0076 loss_train: 0.5035 acc_train: 0.9730 loss_val: 0.9323 acc_val: 0.8737\n",
            "Epoch: 0077 loss_train: 0.6168 acc_train: 0.9652 loss_val: 0.8941 acc_val: 0.8770\n",
            "Epoch: 0078 loss_train: 0.5859 acc_train: 0.9669 loss_val: 0.9081 acc_val: 0.8725\n",
            "Epoch: 0079 loss_train: 0.5157 acc_train: 0.9726 loss_val: 0.8759 acc_val: 0.8780\n",
            "Epoch: 0080 loss_train: 0.4479 acc_train: 0.9762 loss_val: 0.9939 acc_val: 0.8712\n",
            "Epoch: 0081 loss_train: 0.4332 acc_train: 0.9761 loss_val: 1.0480 acc_val: 0.8753\n",
            "Epoch: 0082 loss_train: 0.3611 acc_train: 0.9821 loss_val: 1.0691 acc_val: 0.8737\n",
            "Epoch: 0083 loss_train: 0.3461 acc_train: 0.9822 loss_val: 1.0768 acc_val: 0.8725\n",
            "Epoch: 0084 loss_train: 0.3200 acc_train: 0.9852 loss_val: 1.0986 acc_val: 0.8684\n",
            "Epoch: 0085 loss_train: 0.3206 acc_train: 0.9839 loss_val: 1.1044 acc_val: 0.8717\n",
            "Epoch: 0086 loss_train: 0.2928 acc_train: 0.9870 loss_val: 1.1263 acc_val: 0.8747\n",
            "Epoch: 0087 loss_train: 0.2714 acc_train: 0.9864 loss_val: 1.1484 acc_val: 0.8753\n",
            "Epoch: 0088 loss_train: 0.2449 acc_train: 0.9890 loss_val: 1.2049 acc_val: 0.8745\n",
            "Epoch: 0089 loss_train: 0.2244 acc_train: 0.9910 loss_val: 1.2259 acc_val: 0.8715\n",
            "Epoch: 0090 loss_train: 0.2117 acc_train: 0.9907 loss_val: 1.2474 acc_val: 0.8674\n",
            "Epoch: 0091 loss_train: 0.2388 acc_train: 0.9888 loss_val: 1.2407 acc_val: 0.8768\n",
            "Epoch: 0092 loss_train: 0.2256 acc_train: 0.9896 loss_val: 1.2463 acc_val: 0.8694\n",
            "Epoch: 0093 loss_train: 0.2771 acc_train: 0.9861 loss_val: 1.2535 acc_val: 0.8742\n",
            "Epoch: 0094 loss_train: 0.2231 acc_train: 0.9897 loss_val: 1.2104 acc_val: 0.8727\n",
            "Epoch: 0095 loss_train: 0.2238 acc_train: 0.9888 loss_val: 1.2063 acc_val: 0.8727\n",
            "Epoch: 0096 loss_train: 0.2036 acc_train: 0.9900 loss_val: 1.2410 acc_val: 0.8735\n",
            "Epoch: 0097 loss_train: 0.1783 acc_train: 0.9920 loss_val: 1.2641 acc_val: 0.8692\n",
            "Epoch: 0098 loss_train: 0.1757 acc_train: 0.9921 loss_val: 1.2941 acc_val: 0.8730\n",
            "Epoch: 0099 loss_train: 0.1769 acc_train: 0.9921 loss_val: 1.3249 acc_val: 0.8732\n",
            "Epoch: 0100 loss_train: 0.1727 acc_train: 0.9913 loss_val: 1.3466 acc_val: 0.8737\n",
            "Epoch: 0101 loss_train: 0.1474 acc_train: 0.9932 loss_val: 1.3654 acc_val: 0.8755\n",
            "Epoch: 0102 loss_train: 0.1598 acc_train: 0.9929 loss_val: 1.4368 acc_val: 0.8720\n",
            "Epoch: 0103 loss_train: 0.1487 acc_train: 0.9935 loss_val: 1.4344 acc_val: 0.8737\n",
            "Epoch: 0104 loss_train: 0.1324 acc_train: 0.9939 loss_val: 1.4201 acc_val: 0.8765\n",
            "Epoch: 0105 loss_train: 0.1515 acc_train: 0.9932 loss_val: 1.4310 acc_val: 0.8694\n",
            "Epoch: 0106 loss_train: 0.1426 acc_train: 0.9943 loss_val: 1.3686 acc_val: 0.8788\n",
            "Epoch: 0107 loss_train: 0.1294 acc_train: 0.9948 loss_val: 1.4063 acc_val: 0.8750\n",
            "Epoch: 0108 loss_train: 0.1350 acc_train: 0.9937 loss_val: 1.4275 acc_val: 0.8770\n",
            "Epoch: 0109 loss_train: 0.1178 acc_train: 0.9954 loss_val: 1.4301 acc_val: 0.8737\n",
            "Epoch: 0110 loss_train: 0.1177 acc_train: 0.9950 loss_val: 1.4530 acc_val: 0.8725\n",
            "Epoch: 0111 loss_train: 0.1222 acc_train: 0.9945 loss_val: 1.4621 acc_val: 0.8722\n",
            "Epoch: 0112 loss_train: 0.1070 acc_train: 0.9959 loss_val: 1.4942 acc_val: 0.8745\n",
            "Epoch: 0113 loss_train: 0.1010 acc_train: 0.9956 loss_val: 1.5422 acc_val: 0.8636\n",
            "Epoch: 0114 loss_train: 0.1004 acc_train: 0.9955 loss_val: 1.5311 acc_val: 0.8692\n",
            "Epoch: 0115 loss_train: 0.1043 acc_train: 0.9951 loss_val: 1.5828 acc_val: 0.8707\n",
            "Epoch: 0116 loss_train: 0.1032 acc_train: 0.9947 loss_val: 1.5842 acc_val: 0.8740\n",
            "Epoch: 0117 loss_train: 0.0838 acc_train: 0.9964 loss_val: 1.6324 acc_val: 0.8712\n",
            "Optimization Finished!\n",
            "Train cost: 85.8290s\n",
            "Loading 67th epoch\n",
            "Test set results: loss= 0.7259 accuracy= 0.8828\n",
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
            "\n",
            "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
            "\n",
            "  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '\n",
            "/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
            "  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
            "TransformerModel(\n",
            "  (att_embeddings_nope): Linear(in_features=515, out_features=512, bias=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiHeadAttention(\n",
            "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn): FeedForwardNetwork(\n",
            "        (layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (layer2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      )\n",
            "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (Linear1): Linear(in_features=256, out_features=3, bias=True)\n",
            ")\n",
            "total params: 2501125\n",
            "Epoch: 0001 loss_train: 6.5268 acc_train: 0.3937 loss_val: 2.0050 acc_val: 0.5621\n",
            "Epoch: 0002 loss_train: 5.6107 acc_train: 0.6451 loss_val: 1.6404 acc_val: 0.6889\n",
            "Epoch: 0003 loss_train: 4.5798 acc_train: 0.7072 loss_val: 1.2995 acc_val: 0.7518\n",
            "Epoch: 0004 loss_train: 3.6574 acc_train: 0.7731 loss_val: 1.0907 acc_val: 0.7875\n",
            "Epoch: 0005 loss_train: 3.1752 acc_train: 0.7946 loss_val: 1.0121 acc_val: 0.8055\n",
            "Epoch: 0006 loss_train: 2.9188 acc_train: 0.8090 loss_val: 0.9214 acc_val: 0.8230\n",
            "Epoch: 0007 loss_train: 2.6477 acc_train: 0.8278 loss_val: 0.8484 acc_val: 0.8347\n",
            "Epoch: 0008 loss_train: 2.4033 acc_train: 0.8418 loss_val: 0.7762 acc_val: 0.8537\n",
            "Epoch: 0009 loss_train: 2.1739 acc_train: 0.8575 loss_val: 0.7444 acc_val: 0.8631\n",
            "Epoch: 0010 loss_train: 2.0221 acc_train: 0.8680 loss_val: 0.7093 acc_val: 0.8671\n",
            "Epoch: 0011 loss_train: 1.8280 acc_train: 0.8831 loss_val: 0.6645 acc_val: 0.8765\n",
            "Epoch: 0012 loss_train: 1.7502 acc_train: 0.8883 loss_val: 0.6576 acc_val: 0.8793\n",
            "Epoch: 0013 loss_train: 1.5969 acc_train: 0.8994 loss_val: 0.6499 acc_val: 0.8801\n",
            "Epoch: 0014 loss_train: 1.5122 acc_train: 0.9037 loss_val: 0.6885 acc_val: 0.8760\n",
            "Epoch: 0015 loss_train: 1.4774 acc_train: 0.9064 loss_val: 0.6343 acc_val: 0.8867\n",
            "Epoch: 0016 loss_train: 1.3783 acc_train: 0.9145 loss_val: 0.6252 acc_val: 0.8834\n",
            "Epoch: 0017 loss_train: 1.2856 acc_train: 0.9230 loss_val: 0.6251 acc_val: 0.8897\n",
            "Epoch: 0018 loss_train: 1.1536 acc_train: 0.9307 loss_val: 0.6529 acc_val: 0.8854\n",
            "Epoch: 0019 loss_train: 1.0949 acc_train: 0.9336 loss_val: 0.6890 acc_val: 0.8851\n",
            "Epoch: 0020 loss_train: 1.0392 acc_train: 0.9375 loss_val: 0.6600 acc_val: 0.8887\n",
            "Epoch: 0021 loss_train: 0.9177 acc_train: 0.9467 loss_val: 0.6957 acc_val: 0.8887\n",
            "Epoch: 0022 loss_train: 0.8809 acc_train: 0.9458 loss_val: 0.7379 acc_val: 0.8803\n",
            "Epoch: 0023 loss_train: 0.8144 acc_train: 0.9530 loss_val: 0.7723 acc_val: 0.8826\n",
            "Epoch: 0024 loss_train: 0.7890 acc_train: 0.9523 loss_val: 0.8411 acc_val: 0.8747\n",
            "Epoch: 0025 loss_train: 0.7780 acc_train: 0.9509 loss_val: 0.8303 acc_val: 0.8783\n",
            "Epoch: 0026 loss_train: 0.7116 acc_train: 0.9564 loss_val: 0.8051 acc_val: 0.8839\n",
            "Epoch: 0027 loss_train: 0.5610 acc_train: 0.9684 loss_val: 0.8101 acc_val: 0.8869\n",
            "Epoch: 0028 loss_train: 0.4670 acc_train: 0.9745 loss_val: 0.9474 acc_val: 0.8780\n",
            "Epoch: 0029 loss_train: 0.4295 acc_train: 0.9751 loss_val: 0.9985 acc_val: 0.8760\n",
            "Epoch: 0030 loss_train: 0.3867 acc_train: 0.9766 loss_val: 1.0297 acc_val: 0.8813\n",
            "Epoch: 0031 loss_train: 0.3107 acc_train: 0.9823 loss_val: 1.0278 acc_val: 0.8841\n",
            "Epoch: 0032 loss_train: 0.2900 acc_train: 0.9840 loss_val: 1.1981 acc_val: 0.8763\n",
            "Epoch: 0033 loss_train: 0.7144 acc_train: 0.9576 loss_val: 1.1541 acc_val: 0.8605\n",
            "Epoch: 0034 loss_train: 0.6196 acc_train: 0.9594 loss_val: 1.0990 acc_val: 0.8826\n",
            "Epoch: 0035 loss_train: 0.6583 acc_train: 0.9623 loss_val: 1.1192 acc_val: 0.8770\n",
            "Epoch: 0036 loss_train: 0.5769 acc_train: 0.9657 loss_val: 0.8821 acc_val: 0.8778\n",
            "Epoch: 0037 loss_train: 0.3560 acc_train: 0.9807 loss_val: 0.9838 acc_val: 0.8816\n",
            "Epoch: 0038 loss_train: 0.2502 acc_train: 0.9858 loss_val: 1.0691 acc_val: 0.8785\n",
            "Epoch: 0039 loss_train: 0.1834 acc_train: 0.9899 loss_val: 1.1508 acc_val: 0.8803\n",
            "Epoch: 0040 loss_train: 0.1176 acc_train: 0.9932 loss_val: 1.1723 acc_val: 0.8770\n",
            "Epoch: 0041 loss_train: 0.0921 acc_train: 0.9954 loss_val: 1.2731 acc_val: 0.8791\n",
            "Epoch: 0042 loss_train: 0.0806 acc_train: 0.9963 loss_val: 1.3150 acc_val: 0.8824\n",
            "Epoch: 0043 loss_train: 0.0630 acc_train: 0.9975 loss_val: 1.3461 acc_val: 0.8851\n",
            "Epoch: 0044 loss_train: 0.0423 acc_train: 0.9982 loss_val: 1.4321 acc_val: 0.8834\n",
            "Epoch: 0045 loss_train: 0.0405 acc_train: 0.9981 loss_val: 1.4822 acc_val: 0.8816\n",
            "Epoch: 0046 loss_train: 0.0403 acc_train: 0.9978 loss_val: 1.4735 acc_val: 0.8803\n",
            "Epoch: 0047 loss_train: 0.0452 acc_train: 0.9975 loss_val: 1.5169 acc_val: 0.8818\n",
            "Epoch: 0048 loss_train: 0.0573 acc_train: 0.9966 loss_val: 1.5403 acc_val: 0.8811\n",
            "Epoch: 0049 loss_train: 0.0507 acc_train: 0.9970 loss_val: 1.6027 acc_val: 0.8780\n",
            "Epoch: 0050 loss_train: 0.0522 acc_train: 0.9970 loss_val: 1.5824 acc_val: 0.8791\n",
            "Epoch: 0051 loss_train: 0.0622 acc_train: 0.9969 loss_val: 1.6464 acc_val: 0.8753\n",
            "Epoch: 0052 loss_train: 0.0543 acc_train: 0.9969 loss_val: 1.6753 acc_val: 0.8742\n",
            "Epoch: 0053 loss_train: 0.0756 acc_train: 0.9961 loss_val: 1.7133 acc_val: 0.8735\n",
            "Epoch: 0054 loss_train: 0.0723 acc_train: 0.9958 loss_val: 1.7299 acc_val: 0.8770\n",
            "Epoch: 0055 loss_train: 0.1142 acc_train: 0.9930 loss_val: 2.0479 acc_val: 0.8514\n",
            "Epoch: 0056 loss_train: 5.6391 acc_train: 0.7626 loss_val: 2.3017 acc_val: 0.5294\n",
            "Epoch: 0057 loss_train: 4.7769 acc_train: 0.6653 loss_val: 1.0550 acc_val: 0.7918\n",
            "Epoch: 0058 loss_train: 2.7487 acc_train: 0.8244 loss_val: 0.8637 acc_val: 0.8347\n",
            "Epoch: 0059 loss_train: 3.2688 acc_train: 0.7867 loss_val: 0.9494 acc_val: 0.8012\n",
            "Epoch: 0060 loss_train: 2.5691 acc_train: 0.8255 loss_val: 0.9026 acc_val: 0.8131\n",
            "Epoch: 0061 loss_train: 2.3066 acc_train: 0.8536 loss_val: 0.7375 acc_val: 0.8598\n",
            "Epoch: 0062 loss_train: 2.0242 acc_train: 0.8661 loss_val: 0.7294 acc_val: 0.8646\n",
            "Epoch: 0063 loss_train: 1.9044 acc_train: 0.8779 loss_val: 0.6774 acc_val: 0.8687\n",
            "Epoch: 0064 loss_train: 1.7796 acc_train: 0.8870 loss_val: 0.6624 acc_val: 0.8709\n",
            "Epoch: 0065 loss_train: 1.6959 acc_train: 0.8922 loss_val: 0.6309 acc_val: 0.8791\n",
            "Epoch: 0066 loss_train: 1.6078 acc_train: 0.8992 loss_val: 0.6236 acc_val: 0.8806\n",
            "Epoch: 0067 loss_train: 1.5173 acc_train: 0.9060 loss_val: 0.6224 acc_val: 0.8806\n",
            "Epoch: 0068 loss_train: 1.4677 acc_train: 0.9092 loss_val: 0.6067 acc_val: 0.8851\n",
            "Epoch: 0069 loss_train: 1.3689 acc_train: 0.9154 loss_val: 0.6076 acc_val: 0.8867\n",
            "Epoch: 0070 loss_train: 1.3036 acc_train: 0.9176 loss_val: 0.6205 acc_val: 0.8851\n",
            "Epoch: 0071 loss_train: 1.2713 acc_train: 0.9203 loss_val: 0.6351 acc_val: 0.8905\n",
            "Epoch: 0072 loss_train: 1.2381 acc_train: 0.9236 loss_val: 0.6851 acc_val: 0.8750\n",
            "Epoch: 0073 loss_train: 1.2760 acc_train: 0.9182 loss_val: 0.6240 acc_val: 0.8872\n",
            "Epoch: 0074 loss_train: 1.2092 acc_train: 0.9250 loss_val: 0.6453 acc_val: 0.8869\n",
            "Epoch: 0075 loss_train: 1.1753 acc_train: 0.9275 loss_val: 0.6733 acc_val: 0.8796\n",
            "Epoch: 0076 loss_train: 1.0985 acc_train: 0.9304 loss_val: 0.6581 acc_val: 0.8889\n",
            "Epoch: 0077 loss_train: 1.0217 acc_train: 0.9390 loss_val: 0.6515 acc_val: 0.8846\n",
            "Epoch: 0078 loss_train: 0.9535 acc_train: 0.9441 loss_val: 0.6804 acc_val: 0.8915\n",
            "Epoch: 0079 loss_train: 0.9203 acc_train: 0.9451 loss_val: 0.7042 acc_val: 0.8895\n",
            "Epoch: 0080 loss_train: 0.8812 acc_train: 0.9471 loss_val: 0.7121 acc_val: 0.8884\n",
            "Epoch: 0081 loss_train: 0.8589 acc_train: 0.9501 loss_val: 0.7313 acc_val: 0.8859\n",
            "Epoch: 0082 loss_train: 0.7780 acc_train: 0.9577 loss_val: 0.7680 acc_val: 0.8889\n",
            "Epoch: 0083 loss_train: 0.7578 acc_train: 0.9568 loss_val: 0.7519 acc_val: 0.8864\n",
            "Epoch: 0084 loss_train: 0.7270 acc_train: 0.9597 loss_val: 0.8329 acc_val: 0.8796\n",
            "Epoch: 0085 loss_train: 0.9022 acc_train: 0.9493 loss_val: 0.8528 acc_val: 0.8697\n",
            "Epoch: 0086 loss_train: 1.0906 acc_train: 0.9307 loss_val: 0.6755 acc_val: 0.8877\n",
            "Epoch: 0087 loss_train: 0.9863 acc_train: 0.9408 loss_val: 0.6651 acc_val: 0.8912\n",
            "Epoch: 0088 loss_train: 0.8798 acc_train: 0.9475 loss_val: 0.6891 acc_val: 0.8859\n",
            "Epoch: 0089 loss_train: 0.7700 acc_train: 0.9553 loss_val: 0.6914 acc_val: 0.8887\n",
            "Epoch: 0090 loss_train: 0.7212 acc_train: 0.9589 loss_val: 0.7566 acc_val: 0.8920\n",
            "Epoch: 0091 loss_train: 0.6481 acc_train: 0.9633 loss_val: 0.7884 acc_val: 0.8846\n",
            "Epoch: 0092 loss_train: 0.6038 acc_train: 0.9670 loss_val: 0.8251 acc_val: 0.8839\n",
            "Epoch: 0093 loss_train: 0.5660 acc_train: 0.9691 loss_val: 0.8529 acc_val: 0.8851\n",
            "Epoch: 0094 loss_train: 0.5659 acc_train: 0.9702 loss_val: 0.8750 acc_val: 0.8829\n",
            "Epoch: 0095 loss_train: 0.5375 acc_train: 0.9702 loss_val: 0.8799 acc_val: 0.8859\n",
            "Epoch: 0096 loss_train: 0.5349 acc_train: 0.9714 loss_val: 0.8961 acc_val: 0.8874\n",
            "Epoch: 0097 loss_train: 0.4617 acc_train: 0.9767 loss_val: 0.9232 acc_val: 0.8854\n",
            "Epoch: 0098 loss_train: 0.4605 acc_train: 0.9757 loss_val: 0.9660 acc_val: 0.8791\n",
            "Epoch: 0099 loss_train: 0.4736 acc_train: 0.9759 loss_val: 0.9793 acc_val: 0.8829\n",
            "Epoch: 0100 loss_train: 0.4557 acc_train: 0.9761 loss_val: 0.9576 acc_val: 0.8826\n",
            "Epoch: 0101 loss_train: 0.4262 acc_train: 0.9791 loss_val: 0.9954 acc_val: 0.8841\n",
            "Epoch: 0102 loss_train: 0.4439 acc_train: 0.9765 loss_val: 1.0609 acc_val: 0.8798\n",
            "Epoch: 0103 loss_train: 0.5062 acc_train: 0.9711 loss_val: 0.9898 acc_val: 0.8834\n",
            "Epoch: 0104 loss_train: 0.4448 acc_train: 0.9768 loss_val: 0.9441 acc_val: 0.8887\n",
            "Epoch: 0105 loss_train: 0.4457 acc_train: 0.9773 loss_val: 0.9701 acc_val: 0.8801\n",
            "Epoch: 0106 loss_train: 0.4389 acc_train: 0.9767 loss_val: 1.0631 acc_val: 0.8763\n",
            "Epoch: 0107 loss_train: 0.4121 acc_train: 0.9794 loss_val: 0.9949 acc_val: 0.8793\n",
            "Epoch: 0108 loss_train: 0.3538 acc_train: 0.9837 loss_val: 1.0254 acc_val: 0.8824\n",
            "Epoch: 0109 loss_train: 0.3436 acc_train: 0.9836 loss_val: 1.0270 acc_val: 0.8839\n",
            "Epoch: 0110 loss_train: 0.3398 acc_train: 0.9838 loss_val: 1.0582 acc_val: 0.8829\n",
            "Epoch: 0111 loss_train: 0.3228 acc_train: 0.9840 loss_val: 1.0969 acc_val: 0.8798\n",
            "Epoch: 0112 loss_train: 0.3675 acc_train: 0.9806 loss_val: 1.0711 acc_val: 0.8829\n",
            "Epoch: 0113 loss_train: 0.3062 acc_train: 0.9853 loss_val: 1.0910 acc_val: 0.8816\n",
            "Epoch: 0114 loss_train: 0.2863 acc_train: 0.9860 loss_val: 1.1241 acc_val: 0.8775\n",
            "Epoch: 0115 loss_train: 0.2794 acc_train: 0.9877 loss_val: 1.1985 acc_val: 0.8798\n",
            "Epoch: 0116 loss_train: 0.2628 acc_train: 0.9877 loss_val: 1.1814 acc_val: 0.8780\n",
            "Epoch: 0117 loss_train: 0.2706 acc_train: 0.9870 loss_val: 1.2116 acc_val: 0.8811\n",
            "Epoch: 0118 loss_train: 0.2647 acc_train: 0.9873 loss_val: 1.1897 acc_val: 0.8811\n",
            "Epoch: 0119 loss_train: 0.2516 acc_train: 0.9891 loss_val: 1.1783 acc_val: 0.8824\n",
            "Epoch: 0120 loss_train: 0.2204 acc_train: 0.9901 loss_val: 1.2201 acc_val: 0.8798\n",
            "Epoch: 0121 loss_train: 0.2276 acc_train: 0.9901 loss_val: 1.2666 acc_val: 0.8793\n",
            "Epoch: 0122 loss_train: 0.2196 acc_train: 0.9899 loss_val: 1.2593 acc_val: 0.8801\n",
            "Epoch: 0123 loss_train: 0.1977 acc_train: 0.9910 loss_val: 1.2743 acc_val: 0.8818\n",
            "Epoch: 0124 loss_train: 0.1905 acc_train: 0.9919 loss_val: 1.3226 acc_val: 0.8773\n",
            "Epoch: 0125 loss_train: 0.1880 acc_train: 0.9926 loss_val: 1.3028 acc_val: 0.8801\n",
            "Epoch: 0126 loss_train: 0.1949 acc_train: 0.9920 loss_val: 1.3192 acc_val: 0.8780\n",
            "Epoch: 0127 loss_train: 0.2209 acc_train: 0.9902 loss_val: 1.3070 acc_val: 0.8758\n",
            "Epoch: 0128 loss_train: 0.2622 acc_train: 0.9866 loss_val: 1.2770 acc_val: 0.8829\n",
            "Epoch: 0129 loss_train: 0.2576 acc_train: 0.9884 loss_val: 1.2361 acc_val: 0.8785\n",
            "Epoch: 0130 loss_train: 0.2407 acc_train: 0.9892 loss_val: 1.3049 acc_val: 0.8765\n",
            "Epoch: 0131 loss_train: 0.2296 acc_train: 0.9901 loss_val: 1.2518 acc_val: 0.8791\n",
            "Epoch: 0132 loss_train: 0.2209 acc_train: 0.9899 loss_val: 1.2675 acc_val: 0.8803\n",
            "Epoch: 0133 loss_train: 0.2049 acc_train: 0.9909 loss_val: 1.2856 acc_val: 0.8796\n",
            "Epoch: 0134 loss_train: 0.1963 acc_train: 0.9913 loss_val: 1.2263 acc_val: 0.8803\n",
            "Epoch: 0135 loss_train: 0.2195 acc_train: 0.9897 loss_val: 1.2534 acc_val: 0.8826\n",
            "Epoch: 0136 loss_train: 0.1919 acc_train: 0.9917 loss_val: 1.3379 acc_val: 0.8778\n",
            "Epoch: 0137 loss_train: 0.1735 acc_train: 0.9931 loss_val: 1.2871 acc_val: 0.8829\n",
            "Epoch: 0138 loss_train: 0.1707 acc_train: 0.9933 loss_val: 1.3179 acc_val: 0.8818\n",
            "Epoch: 0139 loss_train: 0.1558 acc_train: 0.9936 loss_val: 1.3507 acc_val: 0.8831\n",
            "Epoch: 0140 loss_train: 0.1588 acc_train: 0.9929 loss_val: 1.3458 acc_val: 0.8806\n",
            "Optimization Finished!\n",
            "Train cost: 103.2237s\n",
            "Loading 90th epoch\n",
            "Test set results: loss= 0.7947 accuracy= 0.8813\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fxcME5x7bf9G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}