  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:

	DGLGraph.adjacency_matrix(transpose, scipy_fmt="csr").

  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '
/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)
  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1448, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU(approximate='none')
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=7, bias=True)
)
total params: 2979849
Epoch: 0001 loss_train: 1.9727 acc_train: 0.1156 loss_val: 1.9686 acc_val: 0.1310
Epoch: 0002 loss_train: 1.9669 acc_train: 0.1279 loss_val: 1.9561 acc_val: 0.1384
Epoch: 0003 loss_train: 1.9547 acc_train: 0.1396 loss_val: 1.9375 acc_val: 0.1587
Epoch: 0004 loss_train: 1.9373 acc_train: 0.1525 loss_val: 1.9130 acc_val: 0.2122
Epoch: 0005 loss_train: 1.9160 acc_train: 0.1925 loss_val: 1.8830 acc_val: 0.3118
Epoch: 0006 loss_train: 1.8886 acc_train: 0.2768 loss_val: 1.8478 acc_val: 0.4428
Epoch: 0007 loss_train: 1.8561 acc_train: 0.3930 loss_val: 1.8078 acc_val: 0.4815
Epoch: 0008 loss_train: 1.8199 acc_train: 0.4736 loss_val: 1.7640 acc_val: 0.5055
Epoch: 0009 loss_train: 1.7805 acc_train: 0.4889 loss_val: 1.7170 acc_val: 0.5055
Epoch: 0010 loss_train: 1.7355 acc_train: 0.5092 loss_val: 1.6678 acc_val: 0.5166
Epoch: 0011 loss_train: 1.6880 acc_train: 0.5203 loss_val: 1.6169 acc_val: 0.5277
Epoch: 0012 loss_train: 1.6405 acc_train: 0.5295 loss_val: 1.5647 acc_val: 0.5461
Epoch: 0013 loss_train: 1.5898 acc_train: 0.5387 loss_val: 1.5112 acc_val: 0.5517
Epoch: 0014 loss_train: 1.5383 acc_train: 0.5541 loss_val: 1.4567 acc_val: 0.5627
Epoch: 0015 loss_train: 1.4837 acc_train: 0.5658 loss_val: 1.4014 acc_val: 0.5701
Epoch: 0016 loss_train: 1.4274 acc_train: 0.5824 loss_val: 1.3452 acc_val: 0.5849
Epoch: 0017 loss_train: 1.3709 acc_train: 0.5996 loss_val: 1.2880 acc_val: 0.6070
Epoch: 0018 loss_train: 1.3128 acc_train: 0.6193 loss_val: 1.2305 acc_val: 0.6494
Epoch: 0019 loss_train: 1.2546 acc_train: 0.6513 loss_val: 1.1740 acc_val: 0.6734
Epoch: 0020 loss_train: 1.1946 acc_train: 0.6845 loss_val: 1.1196 acc_val: 0.6882
Epoch: 0021 loss_train: 1.1373 acc_train: 0.7103 loss_val: 1.0686 acc_val: 0.6993
Epoch: 0022 loss_train: 1.0810 acc_train: 0.7269 loss_val: 1.0214 acc_val: 0.6974
Epoch: 0023 loss_train: 1.0270 acc_train: 0.7386 loss_val: 0.9779 acc_val: 0.7103
Epoch: 0024 loss_train: 0.9762 acc_train: 0.7491 loss_val: 0.9369 acc_val: 0.7214
Epoch: 0025 loss_train: 0.9308 acc_train: 0.7558 loss_val: 0.8978 acc_val: 0.7288
Epoch: 0026 loss_train: 0.8845 acc_train: 0.7706 loss_val: 0.8600 acc_val: 0.7306
Epoch: 0027 loss_train: 0.8421 acc_train: 0.7811 loss_val: 0.8239 acc_val: 0.7472
Epoch: 0028 loss_train: 0.8022 acc_train: 0.8007 loss_val: 0.7899 acc_val: 0.7509
Epoch: 0029 loss_train: 0.7630 acc_train: 0.8093 loss_val: 0.7580 acc_val: 0.7638
Epoch: 0030 loss_train: 0.7271 acc_train: 0.8167 loss_val: 0.7286 acc_val: 0.7768
Epoch: 0031 loss_train: 0.6939 acc_train: 0.8266 loss_val: 0.7010 acc_val: 0.7804
Epoch: 0032 loss_train: 0.6632 acc_train: 0.8284 loss_val: 0.6747 acc_val: 0.7823
Epoch: 0033 loss_train: 0.6327 acc_train: 0.8346 loss_val: 0.6496 acc_val: 0.7823
Epoch: 0034 loss_train: 0.6044 acc_train: 0.8432 loss_val: 0.6263 acc_val: 0.7878
Epoch: 0035 loss_train: 0.5759 acc_train: 0.8481 loss_val: 0.6049 acc_val: 0.8044
Epoch: 0036 loss_train: 0.5496 acc_train: 0.8524 loss_val: 0.5844 acc_val: 0.8081
Epoch: 0037 loss_train: 0.5268 acc_train: 0.8598 loss_val: 0.5635 acc_val: 0.8118
Epoch: 0038 loss_train: 0.5036 acc_train: 0.8598 loss_val: 0.5414 acc_val: 0.8192
Epoch: 0039 loss_train: 0.4787 acc_train: 0.8647 loss_val: 0.5187 acc_val: 0.8266
Epoch: 0040 loss_train: 0.4569 acc_train: 0.8684 loss_val: 0.4966 acc_val: 0.8266
Epoch: 0041 loss_train: 0.4365 acc_train: 0.8739 loss_val: 0.4760 acc_val: 0.8303
Epoch: 0042 loss_train: 0.4171 acc_train: 0.8733 loss_val: 0.4578 acc_val: 0.8413
Epoch: 0043 loss_train: 0.3960 acc_train: 0.8819 loss_val: 0.4426 acc_val: 0.8561
Epoch: 0044 loss_train: 0.3774 acc_train: 0.8887 loss_val: 0.4296 acc_val: 0.8635
Epoch: 0045 loss_train: 0.3595 acc_train: 0.8918 loss_val: 0.4170 acc_val: 0.8690
Epoch: 0046 loss_train: 0.3405 acc_train: 0.8998 loss_val: 0.4041 acc_val: 0.8727
Epoch: 0047 loss_train: 0.3229 acc_train: 0.9090 loss_val: 0.3923 acc_val: 0.8745
Epoch: 0048 loss_train: 0.3065 acc_train: 0.9157 loss_val: 0.3836 acc_val: 0.8838
Epoch: 0049 loss_train: 0.2879 acc_train: 0.9194 loss_val: 0.3782 acc_val: 0.8875
Epoch: 0050 loss_train: 0.2725 acc_train: 0.9274 loss_val: 0.3758 acc_val: 0.8893
Epoch: 0051 loss_train: 0.2565 acc_train: 0.9293 loss_val: 0.3755 acc_val: 0.8838
Epoch: 0052 loss_train: 0.2410 acc_train: 0.9342 loss_val: 0.3742 acc_val: 0.8819
Epoch: 0053 loss_train: 0.2288 acc_train: 0.9403 loss_val: 0.3732 acc_val: 0.8838
Epoch: 0054 loss_train: 0.2129 acc_train: 0.9434 loss_val: 0.3749 acc_val: 0.8819
Epoch: 0055 loss_train: 0.1947 acc_train: 0.9508 loss_val: 0.3805 acc_val: 0.8782
Epoch: 0056 loss_train: 0.1813 acc_train: 0.9533 loss_val: 0.3863 acc_val: 0.8801
Epoch: 0057 loss_train: 0.1677 acc_train: 0.9576 loss_val: 0.3874 acc_val: 0.8782
Epoch: 0058 loss_train: 0.1540 acc_train: 0.9619 loss_val: 0.3872 acc_val: 0.8782
Epoch: 0059 loss_train: 0.1432 acc_train: 0.9656 loss_val: 0.3902 acc_val: 0.8782
Epoch: 0060 loss_train: 0.1315 acc_train: 0.9686 loss_val: 0.3960 acc_val: 0.8801
Epoch: 0061 loss_train: 0.1190 acc_train: 0.9729 loss_val: 0.4023 acc_val: 0.8782
Epoch: 0062 loss_train: 0.1102 acc_train: 0.9760 loss_val: 0.4080 acc_val: 0.8764
Epoch: 0063 loss_train: 0.1000 acc_train: 0.9822 loss_val: 0.4138 acc_val: 0.8764
Epoch: 0064 loss_train: 0.0907 acc_train: 0.9859 loss_val: 0.4196 acc_val: 0.8764
Epoch: 0065 loss_train: 0.0822 acc_train: 0.9865 loss_val: 0.4272 acc_val: 0.8801
Epoch: 0066 loss_train: 0.0764 acc_train: 0.9852 loss_val: 0.4338 acc_val: 0.8745
Epoch: 0067 loss_train: 0.0687 acc_train: 0.9871 loss_val: 0.4419 acc_val: 0.8727
Epoch: 0068 loss_train: 0.0618 acc_train: 0.9895 loss_val: 0.4517 acc_val: 0.8708
Epoch: 0069 loss_train: 0.0564 acc_train: 0.9914 loss_val: 0.4612 acc_val: 0.8727
Epoch: 0070 loss_train: 0.0516 acc_train: 0.9908 loss_val: 0.4702 acc_val: 0.8708
Epoch: 0071 loss_train: 0.0470 acc_train: 0.9920 loss_val: 0.4785 acc_val: 0.8690
Epoch: 0072 loss_train: 0.0425 acc_train: 0.9926 loss_val: 0.4848 acc_val: 0.8690
Epoch: 0073 loss_train: 0.0387 acc_train: 0.9945 loss_val: 0.4902 acc_val: 0.8690
Epoch: 0074 loss_train: 0.0355 acc_train: 0.9938 loss_val: 0.4948 acc_val: 0.8672
Epoch: 0075 loss_train: 0.0312 acc_train: 0.9945 loss_val: 0.4993 acc_val: 0.8672
Epoch: 0076 loss_train: 0.0288 acc_train: 0.9963 loss_val: 0.5046 acc_val: 0.8690
Epoch: 0077 loss_train: 0.0262 acc_train: 0.9951 loss_val: 0.5107 acc_val: 0.8690
Epoch: 0078 loss_train: 0.0229 acc_train: 0.9988 loss_val: 0.5159 acc_val: 0.8708
Epoch: 0079 loss_train: 0.0213 acc_train: 0.9982 loss_val: 0.5216 acc_val: 0.8708
Epoch: 0080 loss_train: 0.0198 acc_train: 0.9975 loss_val: 0.5286 acc_val: 0.8708
Epoch: 0081 loss_train: 0.0173 acc_train: 0.9982 loss_val: 0.5363 acc_val: 0.8690
Epoch: 0082 loss_train: 0.0152 acc_train: 0.9994 loss_val: 0.5455 acc_val: 0.8708
Epoch: 0083 loss_train: 0.0139 acc_train: 0.9988 loss_val: 0.5557 acc_val: 0.8727
Epoch: 0084 loss_train: 0.0124 acc_train: 0.9994 loss_val: 0.5655 acc_val: 0.8708
Epoch: 0085 loss_train: 0.0114 acc_train: 0.9994 loss_val: 0.5740 acc_val: 0.8690
Epoch: 0086 loss_train: 0.0101 acc_train: 1.0000 loss_val: 0.5804 acc_val: 0.8690
Epoch: 0087 loss_train: 0.0090 acc_train: 1.0000 loss_val: 0.5846 acc_val: 0.8708
Epoch: 0088 loss_train: 0.0084 acc_train: 1.0000 loss_val: 0.5882 acc_val: 0.8690
Epoch: 0089 loss_train: 0.0082 acc_train: 0.9994 loss_val: 0.5925 acc_val: 0.8708
Epoch: 0090 loss_train: 0.0073 acc_train: 1.0000 loss_val: 0.5984 acc_val: 0.8708
Epoch: 0091 loss_train: 0.0067 acc_train: 1.0000 loss_val: 0.6056 acc_val: 0.8690
Epoch: 0092 loss_train: 0.0060 acc_train: 1.0000 loss_val: 0.6134 acc_val: 0.8690
Epoch: 0093 loss_train: 0.0053 acc_train: 1.0000 loss_val: 0.6213 acc_val: 0.8708
Epoch: 0094 loss_train: 0.0052 acc_train: 1.0000 loss_val: 0.6275 acc_val: 0.8690
Epoch: 0095 loss_train: 0.0047 acc_train: 1.0000 loss_val: 0.6331 acc_val: 0.8672
Epoch: 0096 loss_train: 0.0043 acc_train: 1.0000 loss_val: 0.6373 acc_val: 0.8672
Epoch: 0097 loss_train: 0.0041 acc_train: 1.0000 loss_val: 0.6404 acc_val: 0.8690
Epoch: 0098 loss_train: 0.0040 acc_train: 1.0000 loss_val: 0.6428 acc_val: 0.8672
Epoch: 0099 loss_train: 0.0035 acc_train: 1.0000 loss_val: 0.6451 acc_val: 0.8672
Epoch: 0100 loss_train: 0.0035 acc_train: 1.0000 loss_val: 0.6471 acc_val: 0.8635
Epoch: 0101 loss_train: 0.0032 acc_train: 1.0000 loss_val: 0.6496 acc_val: 0.8635
Epoch: 0102 loss_train: 0.0031 acc_train: 1.0000 loss_val: 0.6526 acc_val: 0.8635
Epoch: 0103 loss_train: 0.0029 acc_train: 1.0000 loss_val: 0.6555 acc_val: 0.8635
Optimization Finished!
Train cost: 14.0709s
Loading 50th epoch
Test set results: loss= 0.3476 accuracy= 0.9019
  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:354: DGLWarning: DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges
  dgl_warning("DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges")
/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:

	DGLGraph.adjacency_matrix(transpose, scipy_fmt="csr").

  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '
/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)
  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1448, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU(approximate='none')
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=7, bias=True)
)
total params: 2979849
Epoch: 0001 loss_train: 1.9910 acc_train: 0.0750 loss_val: 1.9826 acc_val: 0.0923
Epoch: 0002 loss_train: 1.9858 acc_train: 0.0818 loss_val: 1.9706 acc_val: 0.1033
Epoch: 0003 loss_train: 1.9733 acc_train: 0.0959 loss_val: 1.9529 acc_val: 0.1144
Epoch: 0004 loss_train: 1.9570 acc_train: 0.0984 loss_val: 1.9296 acc_val: 0.1439
Epoch: 0005 loss_train: 1.9363 acc_train: 0.1175 loss_val: 1.9011 acc_val: 0.2196
Epoch: 0006 loss_train: 1.9122 acc_train: 0.1882 loss_val: 1.8679 acc_val: 0.3044
Epoch: 0007 loss_train: 1.8811 acc_train: 0.2878 loss_val: 1.8305 acc_val: 0.4225
Epoch: 0008 loss_train: 1.8489 acc_train: 0.3893 loss_val: 1.7895 acc_val: 0.4705
Epoch: 0009 loss_train: 1.8117 acc_train: 0.4348 loss_val: 1.7455 acc_val: 0.4871
Epoch: 0010 loss_train: 1.7700 acc_train: 0.4520 loss_val: 1.6997 acc_val: 0.5000
Epoch: 0011 loss_train: 1.7270 acc_train: 0.4797 loss_val: 1.6528 acc_val: 0.5148
Epoch: 0012 loss_train: 1.6839 acc_train: 0.4803 loss_val: 1.6051 acc_val: 0.5295
Epoch: 0013 loss_train: 1.6401 acc_train: 0.4957 loss_val: 1.5565 acc_val: 0.5332
Epoch: 0014 loss_train: 1.5952 acc_train: 0.5055 loss_val: 1.5069 acc_val: 0.5295
Epoch: 0015 loss_train: 1.5476 acc_train: 0.5141 loss_val: 1.4555 acc_val: 0.5424
Epoch: 0016 loss_train: 1.4984 acc_train: 0.5320 loss_val: 1.4016 acc_val: 0.5517
Epoch: 0017 loss_train: 1.4451 acc_train: 0.5523 loss_val: 1.3453 acc_val: 0.5720
Epoch: 0018 loss_train: 1.3916 acc_train: 0.5738 loss_val: 1.2874 acc_val: 0.6144
Epoch: 0019 loss_train: 1.3356 acc_train: 0.6082 loss_val: 1.2294 acc_val: 0.6605
Epoch: 0020 loss_train: 1.2756 acc_train: 0.6593 loss_val: 1.1733 acc_val: 0.6863
Epoch: 0021 loss_train: 1.2185 acc_train: 0.6790 loss_val: 1.1205 acc_val: 0.7030
Epoch: 0022 loss_train: 1.1614 acc_train: 0.7048 loss_val: 1.0709 acc_val: 0.7159
Epoch: 0023 loss_train: 1.1063 acc_train: 0.7232 loss_val: 1.0234 acc_val: 0.7288
Epoch: 0024 loss_train: 1.0512 acc_train: 0.7399 loss_val: 0.9769 acc_val: 0.7325
Epoch: 0025 loss_train: 1.0001 acc_train: 0.7528 loss_val: 0.9318 acc_val: 0.7343
Epoch: 0026 loss_train: 0.9495 acc_train: 0.7632 loss_val: 0.8892 acc_val: 0.7380
Epoch: 0027 loss_train: 0.9009 acc_train: 0.7700 loss_val: 0.8498 acc_val: 0.7435
Epoch: 0028 loss_train: 0.8580 acc_train: 0.7774 loss_val: 0.8142 acc_val: 0.7528
Epoch: 0029 loss_train: 0.8160 acc_train: 0.7854 loss_val: 0.7819 acc_val: 0.7565
Epoch: 0030 loss_train: 0.7800 acc_train: 0.7897 loss_val: 0.7519 acc_val: 0.7694
Epoch: 0031 loss_train: 0.7423 acc_train: 0.8044 loss_val: 0.7238 acc_val: 0.7731
Epoch: 0032 loss_train: 0.7055 acc_train: 0.8118 loss_val: 0.6974 acc_val: 0.7804
Epoch: 0033 loss_train: 0.6715 acc_train: 0.8223 loss_val: 0.6721 acc_val: 0.7952
Epoch: 0034 loss_train: 0.6401 acc_train: 0.8370 loss_val: 0.6468 acc_val: 0.8026
Epoch: 0035 loss_train: 0.6091 acc_train: 0.8462 loss_val: 0.6213 acc_val: 0.8100
Epoch: 0036 loss_train: 0.5784 acc_train: 0.8481 loss_val: 0.5964 acc_val: 0.8081
Epoch: 0037 loss_train: 0.5502 acc_train: 0.8555 loss_val: 0.5724 acc_val: 0.8100
Epoch: 0038 loss_train: 0.5233 acc_train: 0.8573 loss_val: 0.5492 acc_val: 0.8192
Epoch: 0039 loss_train: 0.4969 acc_train: 0.8592 loss_val: 0.5271 acc_val: 0.8339
Epoch: 0040 loss_train: 0.4708 acc_train: 0.8684 loss_val: 0.5054 acc_val: 0.8432
Epoch: 0041 loss_train: 0.4484 acc_train: 0.8733 loss_val: 0.4841 acc_val: 0.8450
Epoch: 0042 loss_train: 0.4242 acc_train: 0.8795 loss_val: 0.4638 acc_val: 0.8524
Epoch: 0043 loss_train: 0.4002 acc_train: 0.8856 loss_val: 0.4461 acc_val: 0.8598
Epoch: 0044 loss_train: 0.3787 acc_train: 0.8905 loss_val: 0.4308 acc_val: 0.8672
Epoch: 0045 loss_train: 0.3580 acc_train: 0.8967 loss_val: 0.4171 acc_val: 0.8653
Epoch: 0046 loss_train: 0.3362 acc_train: 0.9053 loss_val: 0.4043 acc_val: 0.8690
Epoch: 0047 loss_train: 0.3183 acc_train: 0.9133 loss_val: 0.3921 acc_val: 0.8745
Epoch: 0048 loss_train: 0.2977 acc_train: 0.9176 loss_val: 0.3815 acc_val: 0.8782
Epoch: 0049 loss_train: 0.2789 acc_train: 0.9207 loss_val: 0.3739 acc_val: 0.8856
Epoch: 0050 loss_train: 0.2617 acc_train: 0.9311 loss_val: 0.3700 acc_val: 0.8819
Epoch: 0051 loss_train: 0.2444 acc_train: 0.9373 loss_val: 0.3696 acc_val: 0.8838
Epoch: 0052 loss_train: 0.2294 acc_train: 0.9397 loss_val: 0.3707 acc_val: 0.8819
Epoch: 0053 loss_train: 0.2158 acc_train: 0.9428 loss_val: 0.3716 acc_val: 0.8819
Epoch: 0054 loss_train: 0.2010 acc_train: 0.9459 loss_val: 0.3732 acc_val: 0.8838
Epoch: 0055 loss_train: 0.1848 acc_train: 0.9526 loss_val: 0.3771 acc_val: 0.8819
Epoch: 0056 loss_train: 0.1712 acc_train: 0.9569 loss_val: 0.3821 acc_val: 0.8856
Epoch: 0057 loss_train: 0.1569 acc_train: 0.9643 loss_val: 0.3865 acc_val: 0.8856
Epoch: 0058 loss_train: 0.1474 acc_train: 0.9674 loss_val: 0.3912 acc_val: 0.8856
Epoch: 0059 loss_train: 0.1345 acc_train: 0.9699 loss_val: 0.3972 acc_val: 0.8838
Epoch: 0060 loss_train: 0.1212 acc_train: 0.9729 loss_val: 0.4035 acc_val: 0.8801
Epoch: 0061 loss_train: 0.1102 acc_train: 0.9754 loss_val: 0.4078 acc_val: 0.8782
Epoch: 0062 loss_train: 0.1014 acc_train: 0.9797 loss_val: 0.4118 acc_val: 0.8801
Epoch: 0063 loss_train: 0.0929 acc_train: 0.9822 loss_val: 0.4172 acc_val: 0.8782
Epoch: 0064 loss_train: 0.0834 acc_train: 0.9840 loss_val: 0.4244 acc_val: 0.8745
Epoch: 0065 loss_train: 0.0758 acc_train: 0.9852 loss_val: 0.4340 acc_val: 0.8727
Epoch: 0066 loss_train: 0.0711 acc_train: 0.9846 loss_val: 0.4422 acc_val: 0.8690
Epoch: 0067 loss_train: 0.0637 acc_train: 0.9883 loss_val: 0.4495 acc_val: 0.8708
Epoch: 0068 loss_train: 0.0572 acc_train: 0.9902 loss_val: 0.4575 acc_val: 0.8690
Epoch: 0069 loss_train: 0.0531 acc_train: 0.9938 loss_val: 0.4664 acc_val: 0.8690
Epoch: 0070 loss_train: 0.0491 acc_train: 0.9932 loss_val: 0.4755 acc_val: 0.8690
Epoch: 0071 loss_train: 0.0443 acc_train: 0.9920 loss_val: 0.4841 acc_val: 0.8764
Epoch: 0072 loss_train: 0.0399 acc_train: 0.9938 loss_val: 0.4903 acc_val: 0.8708
Epoch: 0073 loss_train: 0.0371 acc_train: 0.9951 loss_val: 0.4955 acc_val: 0.8745
Epoch: 0074 loss_train: 0.0344 acc_train: 0.9945 loss_val: 0.5013 acc_val: 0.8727
Epoch: 0075 loss_train: 0.0307 acc_train: 0.9945 loss_val: 0.5078 acc_val: 0.8708
Epoch: 0076 loss_train: 0.0283 acc_train: 0.9963 loss_val: 0.5164 acc_val: 0.8708
Epoch: 0077 loss_train: 0.0248 acc_train: 0.9957 loss_val: 0.5260 acc_val: 0.8727
Epoch: 0078 loss_train: 0.0217 acc_train: 0.9982 loss_val: 0.5346 acc_val: 0.8727
Epoch: 0079 loss_train: 0.0189 acc_train: 0.9982 loss_val: 0.5433 acc_val: 0.8727
Epoch: 0080 loss_train: 0.0180 acc_train: 0.9982 loss_val: 0.5509 acc_val: 0.8745
Epoch: 0081 loss_train: 0.0156 acc_train: 0.9982 loss_val: 0.5577 acc_val: 0.8745
Epoch: 0082 loss_train: 0.0140 acc_train: 1.0000 loss_val: 0.5641 acc_val: 0.8745
Epoch: 0083 loss_train: 0.0131 acc_train: 0.9994 loss_val: 0.5710 acc_val: 0.8745
Epoch: 0084 loss_train: 0.0117 acc_train: 1.0000 loss_val: 0.5783 acc_val: 0.8764
Epoch: 0085 loss_train: 0.0107 acc_train: 0.9994 loss_val: 0.5850 acc_val: 0.8745
Epoch: 0086 loss_train: 0.0098 acc_train: 1.0000 loss_val: 0.5909 acc_val: 0.8727
Epoch: 0087 loss_train: 0.0089 acc_train: 0.9994 loss_val: 0.5954 acc_val: 0.8727
Epoch: 0088 loss_train: 0.0079 acc_train: 1.0000 loss_val: 0.5992 acc_val: 0.8727
Epoch: 0089 loss_train: 0.0075 acc_train: 1.0000 loss_val: 0.6041 acc_val: 0.8727
Epoch: 0090 loss_train: 0.0067 acc_train: 1.0000 loss_val: 0.6108 acc_val: 0.8708
Epoch: 0091 loss_train: 0.0065 acc_train: 0.9994 loss_val: 0.6187 acc_val: 0.8708
Epoch: 0092 loss_train: 0.0057 acc_train: 1.0000 loss_val: 0.6281 acc_val: 0.8690
Epoch: 0093 loss_train: 0.0052 acc_train: 1.0000 loss_val: 0.6378 acc_val: 0.8708
Epoch: 0094 loss_train: 0.0053 acc_train: 1.0000 loss_val: 0.6428 acc_val: 0.8708
Epoch: 0095 loss_train: 0.0045 acc_train: 1.0000 loss_val: 0.6467 acc_val: 0.8690
Epoch: 0096 loss_train: 0.0043 acc_train: 1.0000 loss_val: 0.6488 acc_val: 0.8708
Epoch: 0097 loss_train: 0.0041 acc_train: 1.0000 loss_val: 0.6487 acc_val: 0.8708
Epoch: 0098 loss_train: 0.0039 acc_train: 1.0000 loss_val: 0.6484 acc_val: 0.8708
Epoch: 0099 loss_train: 0.0035 acc_train: 1.0000 loss_val: 0.6485 acc_val: 0.8708
Epoch: 0100 loss_train: 0.0035 acc_train: 1.0000 loss_val: 0.6491 acc_val: 0.8690
Epoch: 0101 loss_train: 0.0033 acc_train: 1.0000 loss_val: 0.6511 acc_val: 0.8690
Epoch: 0102 loss_train: 0.0031 acc_train: 1.0000 loss_val: 0.6551 acc_val: 0.8672
Epoch: 0103 loss_train: 0.0029 acc_train: 1.0000 loss_val: 0.6593 acc_val: 0.8672
Epoch: 0104 loss_train: 0.0027 acc_train: 1.0000 loss_val: 0.6634 acc_val: 0.8672
Epoch: 0105 loss_train: 0.0025 acc_train: 1.0000 loss_val: 0.6675 acc_val: 0.8653
Epoch: 0106 loss_train: 0.0025 acc_train: 1.0000 loss_val: 0.6715 acc_val: 0.8672
Epoch: 0107 loss_train: 0.0024 acc_train: 1.0000 loss_val: 0.6748 acc_val: 0.8672
Epoch: 0108 loss_train: 0.0023 acc_train: 1.0000 loss_val: 0.6771 acc_val: 0.8690
Optimization Finished!
Train cost: 14.3984s
Loading 49th epoch
Test set results: loss= 0.3473 accuracy= 0.8981
  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:354: DGLWarning: DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges
  dgl_warning("DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges")
/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:

	DGLGraph.adjacency_matrix(transpose, scipy_fmt="csr").

  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '
/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)
  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1448, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU(approximate='none')
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=7, bias=True)
)
total params: 2979849
Epoch: 0001 loss_train: 1.9656 acc_train: 0.1371 loss_val: 1.9597 acc_val: 0.1587
Epoch: 0002 loss_train: 1.9608 acc_train: 0.1427 loss_val: 1.9482 acc_val: 0.1624
Epoch: 0003 loss_train: 1.9516 acc_train: 0.1427 loss_val: 1.9312 acc_val: 0.1734
Epoch: 0004 loss_train: 1.9362 acc_train: 0.1605 loss_val: 1.9090 acc_val: 0.2380
Epoch: 0005 loss_train: 1.9177 acc_train: 0.1986 loss_val: 1.8824 acc_val: 0.3266
Epoch: 0006 loss_train: 1.8954 acc_train: 0.2891 loss_val: 1.8521 acc_val: 0.3911
Epoch: 0007 loss_train: 1.8688 acc_train: 0.3678 loss_val: 1.8190 acc_val: 0.4151
Epoch: 0008 loss_train: 1.8410 acc_train: 0.3733 loss_val: 1.7844 acc_val: 0.4114
Epoch: 0009 loss_train: 1.8135 acc_train: 0.3715 loss_val: 1.7491 acc_val: 0.4170
Epoch: 0010 loss_train: 1.7814 acc_train: 0.3672 loss_val: 1.7136 acc_val: 0.4133
Epoch: 0011 loss_train: 1.7509 acc_train: 0.3795 loss_val: 1.6779 acc_val: 0.4225
Epoch: 0012 loss_train: 1.7213 acc_train: 0.3770 loss_val: 1.6415 acc_val: 0.4391
Epoch: 0013 loss_train: 1.6917 acc_train: 0.3887 loss_val: 1.6033 acc_val: 0.4520
Epoch: 0014 loss_train: 1.6598 acc_train: 0.3998 loss_val: 1.5620 acc_val: 0.4705
Epoch: 0015 loss_train: 1.6239 acc_train: 0.4102 loss_val: 1.5162 acc_val: 0.4908
Epoch: 0016 loss_train: 1.5879 acc_train: 0.4219 loss_val: 1.4653 acc_val: 0.5185
Epoch: 0017 loss_train: 1.5421 acc_train: 0.4502 loss_val: 1.4102 acc_val: 0.5498
Epoch: 0018 loss_train: 1.4935 acc_train: 0.4791 loss_val: 1.3533 acc_val: 0.5793
Epoch: 0019 loss_train: 1.4420 acc_train: 0.5228 loss_val: 1.2971 acc_val: 0.6089
Epoch: 0020 loss_train: 1.3879 acc_train: 0.5615 loss_val: 1.2426 acc_val: 0.6494
Epoch: 0021 loss_train: 1.3357 acc_train: 0.6058 loss_val: 1.1886 acc_val: 0.6845
Epoch: 0022 loss_train: 1.2787 acc_train: 0.6494 loss_val: 1.1332 acc_val: 0.6956
Epoch: 0023 loss_train: 1.2229 acc_train: 0.6679 loss_val: 1.0779 acc_val: 0.7048
Epoch: 0024 loss_train: 1.1627 acc_train: 0.6753 loss_val: 1.0259 acc_val: 0.7030
Epoch: 0025 loss_train: 1.1073 acc_train: 0.6765 loss_val: 0.9781 acc_val: 0.7085
Epoch: 0026 loss_train: 1.0522 acc_train: 0.6839 loss_val: 0.9335 acc_val: 0.7251
Epoch: 0027 loss_train: 0.9974 acc_train: 0.7036 loss_val: 0.8933 acc_val: 0.7306
Epoch: 0028 loss_train: 0.9438 acc_train: 0.7442 loss_val: 0.8583 acc_val: 0.7435
Epoch: 0029 loss_train: 0.8958 acc_train: 0.7761 loss_val: 0.8246 acc_val: 0.7509
Epoch: 0030 loss_train: 0.8458 acc_train: 0.7946 loss_val: 0.7900 acc_val: 0.7565
Epoch: 0031 loss_train: 0.7932 acc_train: 0.8081 loss_val: 0.7577 acc_val: 0.7657
Epoch: 0032 loss_train: 0.7496 acc_train: 0.8057 loss_val: 0.7282 acc_val: 0.7804
Epoch: 0033 loss_train: 0.7048 acc_train: 0.8216 loss_val: 0.7015 acc_val: 0.7878
Epoch: 0034 loss_train: 0.6652 acc_train: 0.8383 loss_val: 0.6736 acc_val: 0.7934
Epoch: 0035 loss_train: 0.6260 acc_train: 0.8487 loss_val: 0.6417 acc_val: 0.8118
Epoch: 0036 loss_train: 0.5868 acc_train: 0.8506 loss_val: 0.6112 acc_val: 0.8173
Epoch: 0037 loss_train: 0.5514 acc_train: 0.8604 loss_val: 0.5865 acc_val: 0.8247
Epoch: 0038 loss_train: 0.5197 acc_train: 0.8678 loss_val: 0.5669 acc_val: 0.8247
Epoch: 0039 loss_train: 0.4910 acc_train: 0.8776 loss_val: 0.5458 acc_val: 0.8284
Epoch: 0040 loss_train: 0.4594 acc_train: 0.8801 loss_val: 0.5260 acc_val: 0.8303
Epoch: 0041 loss_train: 0.4358 acc_train: 0.8825 loss_val: 0.5103 acc_val: 0.8358
Epoch: 0042 loss_train: 0.4063 acc_train: 0.8905 loss_val: 0.4956 acc_val: 0.8395
Epoch: 0043 loss_train: 0.3812 acc_train: 0.8930 loss_val: 0.4778 acc_val: 0.8395
Epoch: 0044 loss_train: 0.3580 acc_train: 0.8967 loss_val: 0.4608 acc_val: 0.8413
Epoch: 0045 loss_train: 0.3391 acc_train: 0.9010 loss_val: 0.4487 acc_val: 0.8432
Epoch: 0046 loss_train: 0.3161 acc_train: 0.9084 loss_val: 0.4398 acc_val: 0.8506
Epoch: 0047 loss_train: 0.2955 acc_train: 0.9207 loss_val: 0.4298 acc_val: 0.8506
Epoch: 0048 loss_train: 0.2769 acc_train: 0.9213 loss_val: 0.4174 acc_val: 0.8579
Epoch: 0049 loss_train: 0.2581 acc_train: 0.9280 loss_val: 0.4076 acc_val: 0.8616
Epoch: 0050 loss_train: 0.2422 acc_train: 0.9305 loss_val: 0.4026 acc_val: 0.8635
Epoch: 0051 loss_train: 0.2249 acc_train: 0.9342 loss_val: 0.3988 acc_val: 0.8708
Epoch: 0052 loss_train: 0.2095 acc_train: 0.9446 loss_val: 0.3907 acc_val: 0.8672
Epoch: 0053 loss_train: 0.1951 acc_train: 0.9459 loss_val: 0.3841 acc_val: 0.8690
Epoch: 0054 loss_train: 0.1784 acc_train: 0.9545 loss_val: 0.3833 acc_val: 0.8653
Epoch: 0055 loss_train: 0.1616 acc_train: 0.9649 loss_val: 0.3886 acc_val: 0.8653
Epoch: 0056 loss_train: 0.1501 acc_train: 0.9643 loss_val: 0.3948 acc_val: 0.8690
Epoch: 0057 loss_train: 0.1370 acc_train: 0.9711 loss_val: 0.3991 acc_val: 0.8690
Epoch: 0058 loss_train: 0.1295 acc_train: 0.9717 loss_val: 0.4048 acc_val: 0.8708
Epoch: 0059 loss_train: 0.1199 acc_train: 0.9736 loss_val: 0.4150 acc_val: 0.8708
Epoch: 0060 loss_train: 0.1079 acc_train: 0.9779 loss_val: 0.4294 acc_val: 0.8727
Epoch: 0061 loss_train: 0.0990 acc_train: 0.9822 loss_val: 0.4396 acc_val: 0.8764
Epoch: 0062 loss_train: 0.0903 acc_train: 0.9834 loss_val: 0.4450 acc_val: 0.8745
Epoch: 0063 loss_train: 0.0808 acc_train: 0.9859 loss_val: 0.4534 acc_val: 0.8708
Epoch: 0064 loss_train: 0.0720 acc_train: 0.9865 loss_val: 0.4658 acc_val: 0.8672
Epoch: 0065 loss_train: 0.0655 acc_train: 0.9871 loss_val: 0.4780 acc_val: 0.8635
Epoch: 0066 loss_train: 0.0612 acc_train: 0.9889 loss_val: 0.4867 acc_val: 0.8635
Epoch: 0067 loss_train: 0.0548 acc_train: 0.9908 loss_val: 0.4925 acc_val: 0.8672
Epoch: 0068 loss_train: 0.0485 acc_train: 0.9914 loss_val: 0.4987 acc_val: 0.8690
Epoch: 0069 loss_train: 0.0457 acc_train: 0.9920 loss_val: 0.5062 acc_val: 0.8672
Epoch: 0070 loss_train: 0.0425 acc_train: 0.9932 loss_val: 0.5170 acc_val: 0.8653
Epoch: 0071 loss_train: 0.0372 acc_train: 0.9938 loss_val: 0.5267 acc_val: 0.8653
Epoch: 0072 loss_train: 0.0339 acc_train: 0.9957 loss_val: 0.5326 acc_val: 0.8653
Epoch: 0073 loss_train: 0.0315 acc_train: 0.9975 loss_val: 0.5329 acc_val: 0.8690
Epoch: 0074 loss_train: 0.0287 acc_train: 0.9975 loss_val: 0.5345 acc_val: 0.8690
Epoch: 0075 loss_train: 0.0265 acc_train: 0.9975 loss_val: 0.5412 acc_val: 0.8690
Epoch: 0076 loss_train: 0.0248 acc_train: 0.9969 loss_val: 0.5533 acc_val: 0.8653
Epoch: 0077 loss_train: 0.0217 acc_train: 0.9969 loss_val: 0.5635 acc_val: 0.8635
Epoch: 0078 loss_train: 0.0199 acc_train: 0.9975 loss_val: 0.5671 acc_val: 0.8635
Epoch: 0079 loss_train: 0.0176 acc_train: 0.9988 loss_val: 0.5704 acc_val: 0.8672
Epoch: 0080 loss_train: 0.0164 acc_train: 0.9988 loss_val: 0.5744 acc_val: 0.8708
Epoch: 0081 loss_train: 0.0147 acc_train: 0.9988 loss_val: 0.5810 acc_val: 0.8690
Epoch: 0082 loss_train: 0.0132 acc_train: 0.9994 loss_val: 0.5903 acc_val: 0.8672
Epoch: 0083 loss_train: 0.0121 acc_train: 0.9988 loss_val: 0.6009 acc_val: 0.8672
Epoch: 0084 loss_train: 0.0102 acc_train: 0.9994 loss_val: 0.6117 acc_val: 0.8672
Epoch: 0085 loss_train: 0.0091 acc_train: 1.0000 loss_val: 0.6216 acc_val: 0.8672
Epoch: 0086 loss_train: 0.0076 acc_train: 1.0000 loss_val: 0.6306 acc_val: 0.8653
Epoch: 0087 loss_train: 0.0071 acc_train: 1.0000 loss_val: 0.6381 acc_val: 0.8635
Epoch: 0088 loss_train: 0.0065 acc_train: 1.0000 loss_val: 0.6416 acc_val: 0.8635
Epoch: 0089 loss_train: 0.0061 acc_train: 1.0000 loss_val: 0.6433 acc_val: 0.8635
Epoch: 0090 loss_train: 0.0057 acc_train: 1.0000 loss_val: 0.6442 acc_val: 0.8635
Epoch: 0091 loss_train: 0.0052 acc_train: 1.0000 loss_val: 0.6462 acc_val: 0.8690
Epoch: 0092 loss_train: 0.0048 acc_train: 1.0000 loss_val: 0.6500 acc_val: 0.8708
Epoch: 0093 loss_train: 0.0044 acc_train: 1.0000 loss_val: 0.6559 acc_val: 0.8653
Epoch: 0094 loss_train: 0.0046 acc_train: 1.0000 loss_val: 0.6639 acc_val: 0.8635
Epoch: 0095 loss_train: 0.0040 acc_train: 1.0000 loss_val: 0.6733 acc_val: 0.8653
Epoch: 0096 loss_train: 0.0037 acc_train: 1.0000 loss_val: 0.6816 acc_val: 0.8653
Epoch: 0097 loss_train: 0.0036 acc_train: 1.0000 loss_val: 0.6868 acc_val: 0.8653
Epoch: 0098 loss_train: 0.0033 acc_train: 1.0000 loss_val: 0.6896 acc_val: 0.8653
Epoch: 0099 loss_train: 0.0031 acc_train: 1.0000 loss_val: 0.6902 acc_val: 0.8653
Epoch: 0100 loss_train: 0.0030 acc_train: 1.0000 loss_val: 0.6893 acc_val: 0.8672
Epoch: 0101 loss_train: 0.0028 acc_train: 1.0000 loss_val: 0.6877 acc_val: 0.8653
Epoch: 0102 loss_train: 0.0027 acc_train: 1.0000 loss_val: 0.6872 acc_val: 0.8653
Epoch: 0103 loss_train: 0.0027 acc_train: 1.0000 loss_val: 0.6883 acc_val: 0.8653
Epoch: 0104 loss_train: 0.0025 acc_train: 1.0000 loss_val: 0.6909 acc_val: 0.8653
Epoch: 0105 loss_train: 0.0023 acc_train: 1.0000 loss_val: 0.6950 acc_val: 0.8635
Epoch: 0106 loss_train: 0.0022 acc_train: 1.0000 loss_val: 0.7001 acc_val: 0.8616
Epoch: 0107 loss_train: 0.0022 acc_train: 1.0000 loss_val: 0.7055 acc_val: 0.8635
Epoch: 0108 loss_train: 0.0021 acc_train: 1.0000 loss_val: 0.7103 acc_val: 0.8616
Epoch: 0109 loss_train: 0.0020 acc_train: 1.0000 loss_val: 0.7145 acc_val: 0.8616
Epoch: 0110 loss_train: 0.0019 acc_train: 1.0000 loss_val: 0.7185 acc_val: 0.8616
Epoch: 0111 loss_train: 0.0018 acc_train: 1.0000 loss_val: 0.7212 acc_val: 0.8616
Optimization Finished!
Train cost: 15.6863s
Loading 61th epoch
Test set results: loss= 0.3408 accuracy= 0.9037
  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:354: DGLWarning: DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges
  dgl_warning("DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges")
/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:

	DGLGraph.adjacency_matrix(transpose, scipy_fmt="csr").

  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '
/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)
  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1448, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU(approximate='none')
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=7, bias=True)
)
total params: 2979849
Epoch: 0001 loss_train: 1.9862 acc_train: 0.0843 loss_val: 1.9878 acc_val: 0.0756
Epoch: 0002 loss_train: 1.9809 acc_train: 0.0867 loss_val: 1.9766 acc_val: 0.0812
Epoch: 0003 loss_train: 1.9720 acc_train: 0.0978 loss_val: 1.9601 acc_val: 0.0904
Epoch: 0004 loss_train: 1.9573 acc_train: 0.1002 loss_val: 1.9387 acc_val: 0.1070
Epoch: 0005 loss_train: 1.9390 acc_train: 0.1125 loss_val: 1.9133 acc_val: 0.2159
Epoch: 0006 loss_train: 1.9183 acc_train: 0.2153 loss_val: 1.8849 acc_val: 0.2804
Epoch: 0007 loss_train: 1.8944 acc_train: 0.2835 loss_val: 1.8546 acc_val: 0.2915
Epoch: 0008 loss_train: 1.8698 acc_train: 0.2952 loss_val: 1.8240 acc_val: 0.3026
Epoch: 0009 loss_train: 1.8439 acc_train: 0.2970 loss_val: 1.7934 acc_val: 0.3026
Epoch: 0010 loss_train: 1.8169 acc_train: 0.2983 loss_val: 1.7635 acc_val: 0.3100
Epoch: 0011 loss_train: 1.7934 acc_train: 0.3038 loss_val: 1.7343 acc_val: 0.3118
Epoch: 0012 loss_train: 1.7706 acc_train: 0.3069 loss_val: 1.7055 acc_val: 0.3155
Epoch: 0013 loss_train: 1.7495 acc_train: 0.3081 loss_val: 1.6758 acc_val: 0.3303
Epoch: 0014 loss_train: 1.7282 acc_train: 0.3149 loss_val: 1.6445 acc_val: 0.3672
Epoch: 0015 loss_train: 1.7017 acc_train: 0.3303 loss_val: 1.6093 acc_val: 0.3930
Epoch: 0016 loss_train: 1.6772 acc_train: 0.3487 loss_val: 1.5687 acc_val: 0.4354
Epoch: 0017 loss_train: 1.6457 acc_train: 0.3733 loss_val: 1.5229 acc_val: 0.4668
Epoch: 0018 loss_train: 1.6101 acc_train: 0.3918 loss_val: 1.4739 acc_val: 0.5166
Epoch: 0019 loss_train: 1.5702 acc_train: 0.4336 loss_val: 1.4234 acc_val: 0.5683
Epoch: 0020 loss_train: 1.5273 acc_train: 0.4889 loss_val: 1.3715 acc_val: 0.6162
Epoch: 0021 loss_train: 1.4824 acc_train: 0.5369 loss_val: 1.3165 acc_val: 0.6642
Epoch: 0022 loss_train: 1.4271 acc_train: 0.5800 loss_val: 1.2573 acc_val: 0.6716
Epoch: 0023 loss_train: 1.3701 acc_train: 0.6058 loss_val: 1.1958 acc_val: 0.6771
Epoch: 0024 loss_train: 1.3066 acc_train: 0.6132 loss_val: 1.1358 acc_val: 0.6919
Epoch: 0025 loss_train: 1.2461 acc_train: 0.6193 loss_val: 1.0800 acc_val: 0.6956
Epoch: 0026 loss_train: 1.1849 acc_train: 0.6371 loss_val: 1.0294 acc_val: 0.7066
Epoch: 0027 loss_train: 1.1245 acc_train: 0.6667 loss_val: 0.9858 acc_val: 0.7232
Epoch: 0028 loss_train: 1.0665 acc_train: 0.7048 loss_val: 0.9475 acc_val: 0.7288
Epoch: 0029 loss_train: 1.0137 acc_train: 0.7472 loss_val: 0.9098 acc_val: 0.7269
Epoch: 0030 loss_train: 0.9542 acc_train: 0.7595 loss_val: 0.8729 acc_val: 0.7269
Epoch: 0031 loss_train: 0.9002 acc_train: 0.7589 loss_val: 0.8374 acc_val: 0.7288
Epoch: 0032 loss_train: 0.8458 acc_train: 0.7774 loss_val: 0.8027 acc_val: 0.7399
Epoch: 0033 loss_train: 0.7930 acc_train: 0.8026 loss_val: 0.7656 acc_val: 0.7565
Epoch: 0034 loss_train: 0.7428 acc_train: 0.8173 loss_val: 0.7250 acc_val: 0.7638
Epoch: 0035 loss_train: 0.6959 acc_train: 0.8216 loss_val: 0.6876 acc_val: 0.7823
Epoch: 0036 loss_train: 0.6489 acc_train: 0.8370 loss_val: 0.6549 acc_val: 0.7934
Epoch: 0037 loss_train: 0.6028 acc_train: 0.8518 loss_val: 0.6251 acc_val: 0.8100
Epoch: 0038 loss_train: 0.5669 acc_train: 0.8561 loss_val: 0.5967 acc_val: 0.8173
Epoch: 0039 loss_train: 0.5273 acc_train: 0.8678 loss_val: 0.5744 acc_val: 0.8247
Epoch: 0040 loss_train: 0.4914 acc_train: 0.8733 loss_val: 0.5547 acc_val: 0.8358
Epoch: 0041 loss_train: 0.4594 acc_train: 0.8887 loss_val: 0.5306 acc_val: 0.8339
Epoch: 0042 loss_train: 0.4272 acc_train: 0.8911 loss_val: 0.5114 acc_val: 0.8339
Epoch: 0043 loss_train: 0.3978 acc_train: 0.8942 loss_val: 0.5006 acc_val: 0.8376
Epoch: 0044 loss_train: 0.3715 acc_train: 0.9053 loss_val: 0.4866 acc_val: 0.8358
Epoch: 0045 loss_train: 0.3487 acc_train: 0.9102 loss_val: 0.4716 acc_val: 0.8487
Epoch: 0046 loss_train: 0.3216 acc_train: 0.9145 loss_val: 0.4599 acc_val: 0.8506
Epoch: 0047 loss_train: 0.3016 acc_train: 0.9225 loss_val: 0.4490 acc_val: 0.8524
Epoch: 0048 loss_train: 0.2809 acc_train: 0.9225 loss_val: 0.4387 acc_val: 0.8561
Epoch: 0049 loss_train: 0.2621 acc_train: 0.9280 loss_val: 0.4311 acc_val: 0.8524
Epoch: 0050 loss_train: 0.2457 acc_train: 0.9342 loss_val: 0.4253 acc_val: 0.8524
Epoch: 0051 loss_train: 0.2269 acc_train: 0.9397 loss_val: 0.4181 acc_val: 0.8561
Epoch: 0052 loss_train: 0.2146 acc_train: 0.9446 loss_val: 0.4080 acc_val: 0.8616
Epoch: 0053 loss_train: 0.1989 acc_train: 0.9465 loss_val: 0.4036 acc_val: 0.8598
Epoch: 0054 loss_train: 0.1852 acc_train: 0.9496 loss_val: 0.4052 acc_val: 0.8653
Epoch: 0055 loss_train: 0.1702 acc_train: 0.9576 loss_val: 0.4081 acc_val: 0.8616
Epoch: 0056 loss_train: 0.1588 acc_train: 0.9606 loss_val: 0.4041 acc_val: 0.8579
Epoch: 0057 loss_train: 0.1462 acc_train: 0.9637 loss_val: 0.4027 acc_val: 0.8561
Epoch: 0058 loss_train: 0.1362 acc_train: 0.9656 loss_val: 0.4085 acc_val: 0.8561
Epoch: 0059 loss_train: 0.1255 acc_train: 0.9680 loss_val: 0.4208 acc_val: 0.8598
Epoch: 0060 loss_train: 0.1143 acc_train: 0.9723 loss_val: 0.4292 acc_val: 0.8598
Epoch: 0061 loss_train: 0.1023 acc_train: 0.9766 loss_val: 0.4353 acc_val: 0.8598
Epoch: 0062 loss_train: 0.0940 acc_train: 0.9809 loss_val: 0.4437 acc_val: 0.8616
Epoch: 0063 loss_train: 0.0852 acc_train: 0.9822 loss_val: 0.4565 acc_val: 0.8561
Epoch: 0064 loss_train: 0.0760 acc_train: 0.9840 loss_val: 0.4718 acc_val: 0.8579
Epoch: 0065 loss_train: 0.0696 acc_train: 0.9859 loss_val: 0.4819 acc_val: 0.8579
Epoch: 0066 loss_train: 0.0641 acc_train: 0.9871 loss_val: 0.4904 acc_val: 0.8579
Epoch: 0067 loss_train: 0.0582 acc_train: 0.9889 loss_val: 0.5018 acc_val: 0.8561
Epoch: 0068 loss_train: 0.0508 acc_train: 0.9926 loss_val: 0.5154 acc_val: 0.8524
Epoch: 0069 loss_train: 0.0485 acc_train: 0.9908 loss_val: 0.5274 acc_val: 0.8487
Epoch: 0070 loss_train: 0.0442 acc_train: 0.9926 loss_val: 0.5352 acc_val: 0.8487
Epoch: 0071 loss_train: 0.0380 acc_train: 0.9951 loss_val: 0.5399 acc_val: 0.8524
Epoch: 0072 loss_train: 0.0347 acc_train: 0.9957 loss_val: 0.5422 acc_val: 0.8524
Epoch: 0073 loss_train: 0.0317 acc_train: 0.9969 loss_val: 0.5442 acc_val: 0.8579
Epoch: 0074 loss_train: 0.0281 acc_train: 0.9969 loss_val: 0.5499 acc_val: 0.8561
Epoch: 0075 loss_train: 0.0259 acc_train: 0.9969 loss_val: 0.5589 acc_val: 0.8524
Epoch: 0076 loss_train: 0.0248 acc_train: 0.9975 loss_val: 0.5690 acc_val: 0.8524
Epoch: 0077 loss_train: 0.0217 acc_train: 0.9988 loss_val: 0.5758 acc_val: 0.8524
Epoch: 0078 loss_train: 0.0198 acc_train: 0.9988 loss_val: 0.5791 acc_val: 0.8542
Epoch: 0079 loss_train: 0.0177 acc_train: 0.9988 loss_val: 0.5840 acc_val: 0.8561
Epoch: 0080 loss_train: 0.0170 acc_train: 0.9988 loss_val: 0.5897 acc_val: 0.8561
Epoch: 0081 loss_train: 0.0146 acc_train: 0.9994 loss_val: 0.5957 acc_val: 0.8542
Epoch: 0082 loss_train: 0.0131 acc_train: 0.9994 loss_val: 0.6027 acc_val: 0.8542
Epoch: 0083 loss_train: 0.0118 acc_train: 0.9994 loss_val: 0.6105 acc_val: 0.8524
Epoch: 0084 loss_train: 0.0100 acc_train: 0.9994 loss_val: 0.6176 acc_val: 0.8524
Epoch: 0085 loss_train: 0.0092 acc_train: 0.9994 loss_val: 0.6244 acc_val: 0.8524
Epoch: 0086 loss_train: 0.0079 acc_train: 1.0000 loss_val: 0.6321 acc_val: 0.8506
Epoch: 0087 loss_train: 0.0073 acc_train: 0.9994 loss_val: 0.6405 acc_val: 0.8487
Epoch: 0088 loss_train: 0.0066 acc_train: 1.0000 loss_val: 0.6470 acc_val: 0.8506
Epoch: 0089 loss_train: 0.0059 acc_train: 1.0000 loss_val: 0.6518 acc_val: 0.8506
Epoch: 0090 loss_train: 0.0057 acc_train: 1.0000 loss_val: 0.6557 acc_val: 0.8487
Epoch: 0091 loss_train: 0.0053 acc_train: 1.0000 loss_val: 0.6584 acc_val: 0.8487
Epoch: 0092 loss_train: 0.0048 acc_train: 1.0000 loss_val: 0.6611 acc_val: 0.8469
Epoch: 0093 loss_train: 0.0046 acc_train: 1.0000 loss_val: 0.6640 acc_val: 0.8450
Epoch: 0094 loss_train: 0.0043 acc_train: 1.0000 loss_val: 0.6678 acc_val: 0.8432
Epoch: 0095 loss_train: 0.0042 acc_train: 1.0000 loss_val: 0.6731 acc_val: 0.8432
Epoch: 0096 loss_train: 0.0038 acc_train: 1.0000 loss_val: 0.6786 acc_val: 0.8487
Epoch: 0097 loss_train: 0.0035 acc_train: 1.0000 loss_val: 0.6838 acc_val: 0.8487
Epoch: 0098 loss_train: 0.0034 acc_train: 1.0000 loss_val: 0.6894 acc_val: 0.8469
Epoch: 0099 loss_train: 0.0031 acc_train: 1.0000 loss_val: 0.6934 acc_val: 0.8469
Epoch: 0100 loss_train: 0.0030 acc_train: 1.0000 loss_val: 0.6962 acc_val: 0.8487
Epoch: 0101 loss_train: 0.0028 acc_train: 1.0000 loss_val: 0.6977 acc_val: 0.8506
Epoch: 0102 loss_train: 0.0026 acc_train: 1.0000 loss_val: 0.6985 acc_val: 0.8487
Epoch: 0103 loss_train: 0.0026 acc_train: 1.0000 loss_val: 0.6999 acc_val: 0.8542
Epoch: 0104 loss_train: 0.0025 acc_train: 1.0000 loss_val: 0.7024 acc_val: 0.8542
Epoch: 0105 loss_train: 0.0023 acc_train: 1.0000 loss_val: 0.7055 acc_val: 0.8524
Epoch: 0106 loss_train: 0.0021 acc_train: 1.0000 loss_val: 0.7095 acc_val: 0.8524
Epoch: 0107 loss_train: 0.0021 acc_train: 1.0000 loss_val: 0.7143 acc_val: 0.8469
Optimization Finished!
Train cost: 14.8151s
Loading 54th epoch
Test set results: loss= 0.3287 accuracy= 0.8963