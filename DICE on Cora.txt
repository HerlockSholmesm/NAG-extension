  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:

	DGLGraph.adjacency_matrix(transpose, scipy_fmt="csr").

  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '
/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)
  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1448, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU(approximate='none')
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=7, bias=True)
)
total params: 2979849
Epoch: 0001 loss_train: 1.9727 acc_train: 0.1156 loss_val: 1.9686 acc_val: 0.1310
Epoch: 0002 loss_train: 1.9669 acc_train: 0.1279 loss_val: 1.9561 acc_val: 0.1384
Epoch: 0003 loss_train: 1.9547 acc_train: 0.1396 loss_val: 1.9375 acc_val: 0.1587
Epoch: 0004 loss_train: 1.9373 acc_train: 0.1525 loss_val: 1.9130 acc_val: 0.2122
Epoch: 0005 loss_train: 1.9160 acc_train: 0.1925 loss_val: 1.8830 acc_val: 0.3118
Epoch: 0006 loss_train: 1.8886 acc_train: 0.2768 loss_val: 1.8478 acc_val: 0.4428
Epoch: 0007 loss_train: 1.8561 acc_train: 0.3930 loss_val: 1.8078 acc_val: 0.4815
Epoch: 0008 loss_train: 1.8199 acc_train: 0.4736 loss_val: 1.7640 acc_val: 0.5055
Epoch: 0009 loss_train: 1.7805 acc_train: 0.4889 loss_val: 1.7170 acc_val: 0.5055
Epoch: 0010 loss_train: 1.7355 acc_train: 0.5092 loss_val: 1.6678 acc_val: 0.5166
Epoch: 0011 loss_train: 1.6880 acc_train: 0.5203 loss_val: 1.6169 acc_val: 0.5277
Epoch: 0012 loss_train: 1.6405 acc_train: 0.5295 loss_val: 1.5647 acc_val: 0.5461
Epoch: 0013 loss_train: 1.5898 acc_train: 0.5387 loss_val: 1.5112 acc_val: 0.5517
Epoch: 0014 loss_train: 1.5383 acc_train: 0.5541 loss_val: 1.4567 acc_val: 0.5627
Epoch: 0015 loss_train: 1.4837 acc_train: 0.5658 loss_val: 1.4014 acc_val: 0.5701
Epoch: 0016 loss_train: 1.4274 acc_train: 0.5824 loss_val: 1.3452 acc_val: 0.5849
Epoch: 0017 loss_train: 1.3709 acc_train: 0.5996 loss_val: 1.2880 acc_val: 0.6070
Epoch: 0018 loss_train: 1.3128 acc_train: 0.6193 loss_val: 1.2305 acc_val: 0.6494
Epoch: 0019 loss_train: 1.2546 acc_train: 0.6513 loss_val: 1.1740 acc_val: 0.6734
Epoch: 0020 loss_train: 1.1946 acc_train: 0.6845 loss_val: 1.1196 acc_val: 0.6882
Epoch: 0021 loss_train: 1.1373 acc_train: 0.7103 loss_val: 1.0686 acc_val: 0.6993
Epoch: 0022 loss_train: 1.0810 acc_train: 0.7269 loss_val: 1.0214 acc_val: 0.6974
Epoch: 0023 loss_train: 1.0270 acc_train: 0.7386 loss_val: 0.9779 acc_val: 0.7103
Epoch: 0024 loss_train: 0.9762 acc_train: 0.7491 loss_val: 0.9369 acc_val: 0.7214
Epoch: 0025 loss_train: 0.9308 acc_train: 0.7558 loss_val: 0.8978 acc_val: 0.7288
Epoch: 0026 loss_train: 0.8845 acc_train: 0.7706 loss_val: 0.8600 acc_val: 0.7306
Epoch: 0027 loss_train: 0.8421 acc_train: 0.7811 loss_val: 0.8239 acc_val: 0.7472
Epoch: 0028 loss_train: 0.8022 acc_train: 0.8007 loss_val: 0.7899 acc_val: 0.7509
Epoch: 0029 loss_train: 0.7630 acc_train: 0.8093 loss_val: 0.7580 acc_val: 0.7638
Epoch: 0030 loss_train: 0.7271 acc_train: 0.8167 loss_val: 0.7286 acc_val: 0.7768
Epoch: 0031 loss_train: 0.6939 acc_train: 0.8266 loss_val: 0.7010 acc_val: 0.7804
Epoch: 0032 loss_train: 0.6632 acc_train: 0.8284 loss_val: 0.6747 acc_val: 0.7823
Epoch: 0033 loss_train: 0.6327 acc_train: 0.8346 loss_val: 0.6496 acc_val: 0.7823
Epoch: 0034 loss_train: 0.6044 acc_train: 0.8432 loss_val: 0.6263 acc_val: 0.7878
Epoch: 0035 loss_train: 0.5759 acc_train: 0.8481 loss_val: 0.6049 acc_val: 0.8044
Epoch: 0036 loss_train: 0.5496 acc_train: 0.8524 loss_val: 0.5844 acc_val: 0.8081
Epoch: 0037 loss_train: 0.5268 acc_train: 0.8598 loss_val: 0.5635 acc_val: 0.8118
Epoch: 0038 loss_train: 0.5036 acc_train: 0.8598 loss_val: 0.5414 acc_val: 0.8192
Epoch: 0039 loss_train: 0.4787 acc_train: 0.8647 loss_val: 0.5187 acc_val: 0.8266
Epoch: 0040 loss_train: 0.4569 acc_train: 0.8684 loss_val: 0.4966 acc_val: 0.8266
Epoch: 0041 loss_train: 0.4365 acc_train: 0.8739 loss_val: 0.4760 acc_val: 0.8303
Epoch: 0042 loss_train: 0.4171 acc_train: 0.8733 loss_val: 0.4578 acc_val: 0.8413
Epoch: 0043 loss_train: 0.3960 acc_train: 0.8819 loss_val: 0.4426 acc_val: 0.8561
Epoch: 0044 loss_train: 0.3774 acc_train: 0.8887 loss_val: 0.4296 acc_val: 0.8635
Epoch: 0045 loss_train: 0.3595 acc_train: 0.8918 loss_val: 0.4170 acc_val: 0.8690
Epoch: 0046 loss_train: 0.3405 acc_train: 0.8998 loss_val: 0.4041 acc_val: 0.8727
Epoch: 0047 loss_train: 0.3229 acc_train: 0.9090 loss_val: 0.3923 acc_val: 0.8745
Epoch: 0048 loss_train: 0.3065 acc_train: 0.9157 loss_val: 0.3836 acc_val: 0.8838
Epoch: 0049 loss_train: 0.2879 acc_train: 0.9194 loss_val: 0.3782 acc_val: 0.8875
Epoch: 0050 loss_train: 0.2725 acc_train: 0.9274 loss_val: 0.3758 acc_val: 0.8893
Epoch: 0051 loss_train: 0.2565 acc_train: 0.9293 loss_val: 0.3755 acc_val: 0.8838
Epoch: 0052 loss_train: 0.2410 acc_train: 0.9342 loss_val: 0.3742 acc_val: 0.8819
Epoch: 0053 loss_train: 0.2288 acc_train: 0.9403 loss_val: 0.3732 acc_val: 0.8838
Epoch: 0054 loss_train: 0.2129 acc_train: 0.9434 loss_val: 0.3749 acc_val: 0.8819
Epoch: 0055 loss_train: 0.1947 acc_train: 0.9508 loss_val: 0.3805 acc_val: 0.8782
Epoch: 0056 loss_train: 0.1813 acc_train: 0.9533 loss_val: 0.3863 acc_val: 0.8801
Epoch: 0057 loss_train: 0.1677 acc_train: 0.9576 loss_val: 0.3874 acc_val: 0.8782
Epoch: 0058 loss_train: 0.1540 acc_train: 0.9619 loss_val: 0.3872 acc_val: 0.8782
Epoch: 0059 loss_train: 0.1432 acc_train: 0.9656 loss_val: 0.3902 acc_val: 0.8782
Epoch: 0060 loss_train: 0.1315 acc_train: 0.9686 loss_val: 0.3960 acc_val: 0.8801
Epoch: 0061 loss_train: 0.1190 acc_train: 0.9729 loss_val: 0.4023 acc_val: 0.8782
Epoch: 0062 loss_train: 0.1102 acc_train: 0.9760 loss_val: 0.4080 acc_val: 0.8764
Epoch: 0063 loss_train: 0.1000 acc_train: 0.9822 loss_val: 0.4138 acc_val: 0.8764
Epoch: 0064 loss_train: 0.0907 acc_train: 0.9859 loss_val: 0.4196 acc_val: 0.8764
Epoch: 0065 loss_train: 0.0822 acc_train: 0.9865 loss_val: 0.4272 acc_val: 0.8801
Epoch: 0066 loss_train: 0.0764 acc_train: 0.9852 loss_val: 0.4338 acc_val: 0.8745
Epoch: 0067 loss_train: 0.0687 acc_train: 0.9871 loss_val: 0.4419 acc_val: 0.8727
Epoch: 0068 loss_train: 0.0618 acc_train: 0.9895 loss_val: 0.4517 acc_val: 0.8708
Epoch: 0069 loss_train: 0.0564 acc_train: 0.9914 loss_val: 0.4612 acc_val: 0.8727
Epoch: 0070 loss_train: 0.0516 acc_train: 0.9908 loss_val: 0.4702 acc_val: 0.8708
Epoch: 0071 loss_train: 0.0470 acc_train: 0.9920 loss_val: 0.4785 acc_val: 0.8690
Epoch: 0072 loss_train: 0.0425 acc_train: 0.9926 loss_val: 0.4848 acc_val: 0.8690
Epoch: 0073 loss_train: 0.0387 acc_train: 0.9945 loss_val: 0.4902 acc_val: 0.8690
Epoch: 0074 loss_train: 0.0355 acc_train: 0.9938 loss_val: 0.4948 acc_val: 0.8672
Epoch: 0075 loss_train: 0.0312 acc_train: 0.9945 loss_val: 0.4993 acc_val: 0.8672
Epoch: 0076 loss_train: 0.0288 acc_train: 0.9963 loss_val: 0.5046 acc_val: 0.8690
Epoch: 0077 loss_train: 0.0262 acc_train: 0.9951 loss_val: 0.5107 acc_val: 0.8690
Epoch: 0078 loss_train: 0.0229 acc_train: 0.9988 loss_val: 0.5159 acc_val: 0.8708
Epoch: 0079 loss_train: 0.0213 acc_train: 0.9982 loss_val: 0.5216 acc_val: 0.8708
Epoch: 0080 loss_train: 0.0198 acc_train: 0.9975 loss_val: 0.5286 acc_val: 0.8708
Epoch: 0081 loss_train: 0.0173 acc_train: 0.9982 loss_val: 0.5363 acc_val: 0.8690
Epoch: 0082 loss_train: 0.0152 acc_train: 0.9994 loss_val: 0.5455 acc_val: 0.8708
Epoch: 0083 loss_train: 0.0139 acc_train: 0.9988 loss_val: 0.5557 acc_val: 0.8727
Epoch: 0084 loss_train: 0.0124 acc_train: 0.9994 loss_val: 0.5655 acc_val: 0.8708
Epoch: 0085 loss_train: 0.0114 acc_train: 0.9994 loss_val: 0.5740 acc_val: 0.8690
Epoch: 0086 loss_train: 0.0101 acc_train: 1.0000 loss_val: 0.5804 acc_val: 0.8690
Epoch: 0087 loss_train: 0.0090 acc_train: 1.0000 loss_val: 0.5846 acc_val: 0.8708
Epoch: 0088 loss_train: 0.0084 acc_train: 1.0000 loss_val: 0.5882 acc_val: 0.8690
Epoch: 0089 loss_train: 0.0082 acc_train: 0.9994 loss_val: 0.5925 acc_val: 0.8708
Epoch: 0090 loss_train: 0.0073 acc_train: 1.0000 loss_val: 0.5984 acc_val: 0.8708
Epoch: 0091 loss_train: 0.0067 acc_train: 1.0000 loss_val: 0.6056 acc_val: 0.8690
Epoch: 0092 loss_train: 0.0060 acc_train: 1.0000 loss_val: 0.6134 acc_val: 0.8690
Epoch: 0093 loss_train: 0.0053 acc_train: 1.0000 loss_val: 0.6213 acc_val: 0.8708
Epoch: 0094 loss_train: 0.0052 acc_train: 1.0000 loss_val: 0.6275 acc_val: 0.8690
Epoch: 0095 loss_train: 0.0047 acc_train: 1.0000 loss_val: 0.6331 acc_val: 0.8672
Epoch: 0096 loss_train: 0.0043 acc_train: 1.0000 loss_val: 0.6373 acc_val: 0.8672
Epoch: 0097 loss_train: 0.0041 acc_train: 1.0000 loss_val: 0.6404 acc_val: 0.8690
Epoch: 0098 loss_train: 0.0040 acc_train: 1.0000 loss_val: 0.6428 acc_val: 0.8672
Epoch: 0099 loss_train: 0.0035 acc_train: 1.0000 loss_val: 0.6451 acc_val: 0.8672
Epoch: 0100 loss_train: 0.0035 acc_train: 1.0000 loss_val: 0.6471 acc_val: 0.8635
Epoch: 0101 loss_train: 0.0032 acc_train: 1.0000 loss_val: 0.6496 acc_val: 0.8635
Epoch: 0102 loss_train: 0.0031 acc_train: 1.0000 loss_val: 0.6526 acc_val: 0.8635
Epoch: 0103 loss_train: 0.0029 acc_train: 1.0000 loss_val: 0.6555 acc_val: 0.8635
Optimization Finished!
Train cost: 13.5636s
Loading 50th epoch
Test set results: loss= 0.3476 accuracy= 0.9019
  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:

	DGLGraph.adjacency_matrix(transpose, scipy_fmt="csr").

  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '
/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)
  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1448, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU(approximate='none')
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=7, bias=True)
)
total params: 2979849
Epoch: 0001 loss_train: 1.9696 acc_train: 0.1089 loss_val: 1.9666 acc_val: 0.0978
Epoch: 0002 loss_train: 1.9674 acc_train: 0.1113 loss_val: 1.9559 acc_val: 0.0941
Epoch: 0003 loss_train: 1.9583 acc_train: 0.1052 loss_val: 1.9406 acc_val: 0.0978
Epoch: 0004 loss_train: 1.9446 acc_train: 0.1181 loss_val: 1.9216 acc_val: 0.1624
Epoch: 0005 loss_train: 1.9270 acc_train: 0.1531 loss_val: 1.9005 acc_val: 0.2934
Epoch: 0006 loss_train: 1.9087 acc_train: 0.2675 loss_val: 1.8791 acc_val: 0.3026
Epoch: 0007 loss_train: 1.8889 acc_train: 0.3020 loss_val: 1.8592 acc_val: 0.3026
Epoch: 0008 loss_train: 1.8706 acc_train: 0.3020 loss_val: 1.8424 acc_val: 0.3026
Epoch: 0009 loss_train: 1.8543 acc_train: 0.3020 loss_val: 1.8294 acc_val: 0.3026
Epoch: 0010 loss_train: 1.8396 acc_train: 0.3020 loss_val: 1.8207 acc_val: 0.3026
Epoch: 0011 loss_train: 1.8296 acc_train: 0.3020 loss_val: 1.8154 acc_val: 0.3026
Epoch: 0012 loss_train: 1.8234 acc_train: 0.3020 loss_val: 1.8123 acc_val: 0.3026
Epoch: 0013 loss_train: 1.8195 acc_train: 0.3020 loss_val: 1.8094 acc_val: 0.3026
Epoch: 0014 loss_train: 1.8152 acc_train: 0.3020 loss_val: 1.8054 acc_val: 0.3026
Epoch: 0015 loss_train: 1.8104 acc_train: 0.3020 loss_val: 1.7997 acc_val: 0.3026
Epoch: 0016 loss_train: 1.8053 acc_train: 0.3020 loss_val: 1.7923 acc_val: 0.3026
Epoch: 0017 loss_train: 1.7985 acc_train: 0.3020 loss_val: 1.7842 acc_val: 0.3026
Epoch: 0018 loss_train: 1.7929 acc_train: 0.3020 loss_val: 1.7759 acc_val: 0.3026
Epoch: 0019 loss_train: 1.7855 acc_train: 0.3020 loss_val: 1.7670 acc_val: 0.3026
Epoch: 0020 loss_train: 1.7805 acc_train: 0.3026 loss_val: 1.7565 acc_val: 0.3044
Epoch: 0021 loss_train: 1.7722 acc_train: 0.3063 loss_val: 1.7428 acc_val: 0.3155
Epoch: 0022 loss_train: 1.7603 acc_train: 0.3130 loss_val: 1.7235 acc_val: 0.3192
Epoch: 0023 loss_train: 1.7437 acc_train: 0.3167 loss_val: 1.6983 acc_val: 0.3192
Epoch: 0024 loss_train: 1.7244 acc_train: 0.3161 loss_val: 1.6678 acc_val: 0.3192
Epoch: 0025 loss_train: 1.6992 acc_train: 0.3180 loss_val: 1.6288 acc_val: 0.3303
Epoch: 0026 loss_train: 1.6666 acc_train: 0.3321 loss_val: 1.5743 acc_val: 0.3801
Epoch: 0027 loss_train: 1.6239 acc_train: 0.3678 loss_val: 1.5046 acc_val: 0.4649
Epoch: 0028 loss_train: 1.5685 acc_train: 0.4459 loss_val: 1.4297 acc_val: 0.5554
Epoch: 0029 loss_train: 1.5119 acc_train: 0.5135 loss_val: 1.3441 acc_val: 0.5701
Epoch: 0030 loss_train: 1.4440 acc_train: 0.5332 loss_val: 1.2657 acc_val: 0.5849
Epoch: 0031 loss_train: 1.3793 acc_train: 0.5418 loss_val: 1.1824 acc_val: 0.6439
Epoch: 0032 loss_train: 1.3154 acc_train: 0.6052 loss_val: 1.1162 acc_val: 0.6937
Epoch: 0033 loss_train: 1.2614 acc_train: 0.6513 loss_val: 1.0581 acc_val: 0.6956
Epoch: 0034 loss_train: 1.2083 acc_train: 0.6648 loss_val: 1.0089 acc_val: 0.7066
Epoch: 0035 loss_train: 1.1550 acc_train: 0.6697 loss_val: 0.9518 acc_val: 0.7509
Epoch: 0036 loss_train: 1.1059 acc_train: 0.7005 loss_val: 0.8974 acc_val: 0.7675
Epoch: 0037 loss_train: 1.0518 acc_train: 0.7134 loss_val: 0.8466 acc_val: 0.7694
Epoch: 0038 loss_train: 0.9998 acc_train: 0.7220 loss_val: 0.7953 acc_val: 0.7915
Epoch: 0039 loss_train: 0.9459 acc_train: 0.7399 loss_val: 0.7499 acc_val: 0.8081
Epoch: 0040 loss_train: 0.8996 acc_train: 0.7583 loss_val: 0.7084 acc_val: 0.7989
Epoch: 0041 loss_train: 0.8496 acc_train: 0.7558 loss_val: 0.6682 acc_val: 0.8137
Epoch: 0042 loss_train: 0.7999 acc_train: 0.7724 loss_val: 0.6334 acc_val: 0.8192
Epoch: 0043 loss_train: 0.7569 acc_train: 0.7878 loss_val: 0.6013 acc_val: 0.8266
Epoch: 0044 loss_train: 0.7147 acc_train: 0.7940 loss_val: 0.5711 acc_val: 0.8358
Epoch: 0045 loss_train: 0.6784 acc_train: 0.8044 loss_val: 0.5457 acc_val: 0.8303
Epoch: 0046 loss_train: 0.6333 acc_train: 0.8173 loss_val: 0.5234 acc_val: 0.8321
Epoch: 0047 loss_train: 0.5971 acc_train: 0.8266 loss_val: 0.5023 acc_val: 0.8376
Epoch: 0048 loss_train: 0.5597 acc_train: 0.8339 loss_val: 0.4868 acc_val: 0.8469
Epoch: 0049 loss_train: 0.5277 acc_train: 0.8462 loss_val: 0.4751 acc_val: 0.8450
Epoch: 0050 loss_train: 0.4953 acc_train: 0.8542 loss_val: 0.4610 acc_val: 0.8487
Epoch: 0051 loss_train: 0.4664 acc_train: 0.8616 loss_val: 0.4533 acc_val: 0.8524
Epoch: 0052 loss_train: 0.4391 acc_train: 0.8665 loss_val: 0.4444 acc_val: 0.8561
Epoch: 0053 loss_train: 0.4110 acc_train: 0.8733 loss_val: 0.4421 acc_val: 0.8542
Epoch: 0054 loss_train: 0.3852 acc_train: 0.8844 loss_val: 0.4376 acc_val: 0.8598
Epoch: 0055 loss_train: 0.3604 acc_train: 0.8936 loss_val: 0.4344 acc_val: 0.8616
Epoch: 0056 loss_train: 0.3358 acc_train: 0.8998 loss_val: 0.4313 acc_val: 0.8598
Epoch: 0057 loss_train: 0.3145 acc_train: 0.9065 loss_val: 0.4325 acc_val: 0.8579
Epoch: 0058 loss_train: 0.2965 acc_train: 0.9102 loss_val: 0.4356 acc_val: 0.8561
Epoch: 0059 loss_train: 0.2759 acc_train: 0.9170 loss_val: 0.4380 acc_val: 0.8598
Epoch: 0060 loss_train: 0.2562 acc_train: 0.9244 loss_val: 0.4402 acc_val: 0.8579
Epoch: 0061 loss_train: 0.2399 acc_train: 0.9287 loss_val: 0.4436 acc_val: 0.8542
Epoch: 0062 loss_train: 0.2246 acc_train: 0.9305 loss_val: 0.4511 acc_val: 0.8561
Epoch: 0063 loss_train: 0.2076 acc_train: 0.9354 loss_val: 0.4592 acc_val: 0.8598
Epoch: 0064 loss_train: 0.1943 acc_train: 0.9416 loss_val: 0.4669 acc_val: 0.8635
Epoch: 0065 loss_train: 0.1830 acc_train: 0.9434 loss_val: 0.4786 acc_val: 0.8561
Epoch: 0066 loss_train: 0.1703 acc_train: 0.9508 loss_val: 0.4891 acc_val: 0.8524
Epoch: 0067 loss_train: 0.1579 acc_train: 0.9569 loss_val: 0.5013 acc_val: 0.8469
Epoch: 0068 loss_train: 0.1456 acc_train: 0.9606 loss_val: 0.5142 acc_val: 0.8487
Epoch: 0069 loss_train: 0.1367 acc_train: 0.9625 loss_val: 0.5266 acc_val: 0.8469
Epoch: 0070 loss_train: 0.1259 acc_train: 0.9686 loss_val: 0.5417 acc_val: 0.8450
Epoch: 0071 loss_train: 0.1181 acc_train: 0.9729 loss_val: 0.5587 acc_val: 0.8469
Epoch: 0072 loss_train: 0.1073 acc_train: 0.9772 loss_val: 0.5768 acc_val: 0.8469
Epoch: 0073 loss_train: 0.0995 acc_train: 0.9828 loss_val: 0.5897 acc_val: 0.8395
Epoch: 0074 loss_train: 0.0888 acc_train: 0.9852 loss_val: 0.6052 acc_val: 0.8395
Epoch: 0075 loss_train: 0.0822 acc_train: 0.9859 loss_val: 0.6191 acc_val: 0.8413
Epoch: 0076 loss_train: 0.0747 acc_train: 0.9889 loss_val: 0.6367 acc_val: 0.8395
Epoch: 0077 loss_train: 0.0672 acc_train: 0.9908 loss_val: 0.6565 acc_val: 0.8413
Epoch: 0078 loss_train: 0.0620 acc_train: 0.9908 loss_val: 0.6687 acc_val: 0.8395
Epoch: 0079 loss_train: 0.0538 acc_train: 0.9957 loss_val: 0.6778 acc_val: 0.8358
Epoch: 0080 loss_train: 0.0488 acc_train: 0.9957 loss_val: 0.6871 acc_val: 0.8358
Epoch: 0081 loss_train: 0.0433 acc_train: 0.9969 loss_val: 0.6998 acc_val: 0.8358
Epoch: 0082 loss_train: 0.0387 acc_train: 0.9969 loss_val: 0.7137 acc_val: 0.8358
Epoch: 0083 loss_train: 0.0345 acc_train: 0.9969 loss_val: 0.7267 acc_val: 0.8376
Epoch: 0084 loss_train: 0.0313 acc_train: 0.9975 loss_val: 0.7368 acc_val: 0.8339
Epoch: 0085 loss_train: 0.0267 acc_train: 0.9994 loss_val: 0.7441 acc_val: 0.8321
Epoch: 0086 loss_train: 0.0237 acc_train: 0.9988 loss_val: 0.7503 acc_val: 0.8284
Epoch: 0087 loss_train: 0.0210 acc_train: 0.9994 loss_val: 0.7579 acc_val: 0.8303
Epoch: 0088 loss_train: 0.0190 acc_train: 0.9994 loss_val: 0.7666 acc_val: 0.8266
Epoch: 0089 loss_train: 0.0169 acc_train: 0.9994 loss_val: 0.7744 acc_val: 0.8284
Epoch: 0090 loss_train: 0.0162 acc_train: 0.9994 loss_val: 0.7842 acc_val: 0.8321
Epoch: 0091 loss_train: 0.0131 acc_train: 0.9994 loss_val: 0.7945 acc_val: 0.8303
Epoch: 0092 loss_train: 0.0117 acc_train: 0.9994 loss_val: 0.8034 acc_val: 0.8247
Epoch: 0093 loss_train: 0.0104 acc_train: 0.9994 loss_val: 0.8086 acc_val: 0.8247
Epoch: 0094 loss_train: 0.0088 acc_train: 0.9994 loss_val: 0.8122 acc_val: 0.8266
Epoch: 0095 loss_train: 0.0081 acc_train: 0.9994 loss_val: 0.8166 acc_val: 0.8247
Epoch: 0096 loss_train: 0.0072 acc_train: 0.9994 loss_val: 0.8201 acc_val: 0.8229
Epoch: 0097 loss_train: 0.0069 acc_train: 0.9994 loss_val: 0.8240 acc_val: 0.8284
Epoch: 0098 loss_train: 0.0057 acc_train: 1.0000 loss_val: 0.8280 acc_val: 0.8303
Epoch: 0099 loss_train: 0.0051 acc_train: 1.0000 loss_val: 0.8331 acc_val: 0.8266
Epoch: 0100 loss_train: 0.0046 acc_train: 1.0000 loss_val: 0.8393 acc_val: 0.8266
Epoch: 0101 loss_train: 0.0042 acc_train: 1.0000 loss_val: 0.8455 acc_val: 0.8229
Epoch: 0102 loss_train: 0.0037 acc_train: 1.0000 loss_val: 0.8533 acc_val: 0.8284
Epoch: 0103 loss_train: 0.0034 acc_train: 1.0000 loss_val: 0.8597 acc_val: 0.8247
Epoch: 0104 loss_train: 0.0030 acc_train: 1.0000 loss_val: 0.8637 acc_val: 0.8247
Epoch: 0105 loss_train: 0.0029 acc_train: 1.0000 loss_val: 0.8640 acc_val: 0.8229
Epoch: 0106 loss_train: 0.0027 acc_train: 1.0000 loss_val: 0.8658 acc_val: 0.8266
Epoch: 0107 loss_train: 0.0022 acc_train: 1.0000 loss_val: 0.8690 acc_val: 0.8284
Epoch: 0108 loss_train: 0.0023 acc_train: 1.0000 loss_val: 0.8737 acc_val: 0.8266
Epoch: 0109 loss_train: 0.0024 acc_train: 1.0000 loss_val: 0.8794 acc_val: 0.8284
Epoch: 0110 loss_train: 0.0029 acc_train: 0.9994 loss_val: 0.8948 acc_val: 0.8284
Epoch: 0111 loss_train: 0.0019 acc_train: 1.0000 loss_val: 0.9121 acc_val: 0.8266
Epoch: 0112 loss_train: 0.0019 acc_train: 1.0000 loss_val: 0.9278 acc_val: 0.8247
Epoch: 0113 loss_train: 0.0018 acc_train: 1.0000 loss_val: 0.9366 acc_val: 0.8247
Epoch: 0114 loss_train: 0.0026 acc_train: 0.9994 loss_val: 0.9374 acc_val: 0.8210
Optimization Finished!
Train cost: 15.0210s
Loading 64th epoch
Test set results: loss= 0.4508 accuracy= 0.8667
  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:

	DGLGraph.adjacency_matrix(transpose, scipy_fmt="csr").

  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '
/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)
  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1448, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU(approximate='none')
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=7, bias=True)
)
total params: 2979849
Epoch: 0001 loss_train: 1.9685 acc_train: 0.1193 loss_val: 1.9664 acc_val: 0.1162
Epoch: 0002 loss_train: 1.9660 acc_train: 0.1119 loss_val: 1.9558 acc_val: 0.1199
Epoch: 0003 loss_train: 1.9578 acc_train: 0.1169 loss_val: 1.9409 acc_val: 0.1310
Epoch: 0004 loss_train: 1.9441 acc_train: 0.1138 loss_val: 1.9224 acc_val: 0.1273
Epoch: 0005 loss_train: 1.9276 acc_train: 0.1365 loss_val: 1.9022 acc_val: 0.3026
Epoch: 0006 loss_train: 1.9099 acc_train: 0.2780 loss_val: 1.8822 acc_val: 0.3026
Epoch: 0007 loss_train: 1.8914 acc_train: 0.3020 loss_val: 1.8641 acc_val: 0.3026
Epoch: 0008 loss_train: 1.8743 acc_train: 0.3020 loss_val: 1.8486 acc_val: 0.3026
Epoch: 0009 loss_train: 1.8594 acc_train: 0.3020 loss_val: 1.8373 acc_val: 0.3026
Epoch: 0010 loss_train: 1.8467 acc_train: 0.3020 loss_val: 1.8294 acc_val: 0.3026
Epoch: 0011 loss_train: 1.8385 acc_train: 0.3020 loss_val: 1.8248 acc_val: 0.3026
Epoch: 0012 loss_train: 1.8330 acc_train: 0.3020 loss_val: 1.8229 acc_val: 0.3026
Epoch: 0013 loss_train: 1.8310 acc_train: 0.3020 loss_val: 1.8216 acc_val: 0.3026
Epoch: 0014 loss_train: 1.8283 acc_train: 0.3020 loss_val: 1.8199 acc_val: 0.3026
Epoch: 0015 loss_train: 1.8265 acc_train: 0.3020 loss_val: 1.8171 acc_val: 0.3026
Epoch: 0016 loss_train: 1.8248 acc_train: 0.3020 loss_val: 1.8131 acc_val: 0.3026
Epoch: 0017 loss_train: 1.8214 acc_train: 0.3020 loss_val: 1.8084 acc_val: 0.3026
Epoch: 0018 loss_train: 1.8194 acc_train: 0.3020 loss_val: 1.8033 acc_val: 0.3026
Epoch: 0019 loss_train: 1.8157 acc_train: 0.3020 loss_val: 1.7979 acc_val: 0.3026
Epoch: 0020 loss_train: 1.8148 acc_train: 0.3020 loss_val: 1.7918 acc_val: 0.3026
Epoch: 0021 loss_train: 1.8110 acc_train: 0.3020 loss_val: 1.7845 acc_val: 0.3026
Epoch: 0022 loss_train: 1.8061 acc_train: 0.3020 loss_val: 1.7754 acc_val: 0.3026
Epoch: 0023 loss_train: 1.8001 acc_train: 0.3020 loss_val: 1.7643 acc_val: 0.3026
Epoch: 0024 loss_train: 1.7936 acc_train: 0.3020 loss_val: 1.7518 acc_val: 0.3026
Epoch: 0025 loss_train: 1.7839 acc_train: 0.3020 loss_val: 1.7383 acc_val: 0.3026
Epoch: 0026 loss_train: 1.7745 acc_train: 0.3020 loss_val: 1.7224 acc_val: 0.3026
Epoch: 0027 loss_train: 1.7636 acc_train: 0.3020 loss_val: 1.7005 acc_val: 0.3026
Epoch: 0028 loss_train: 1.7488 acc_train: 0.3020 loss_val: 1.6703 acc_val: 0.3026
Epoch: 0029 loss_train: 1.7315 acc_train: 0.3020 loss_val: 1.6330 acc_val: 0.3026
Epoch: 0030 loss_train: 1.7087 acc_train: 0.3026 loss_val: 1.5901 acc_val: 0.3358
Epoch: 0031 loss_train: 1.6847 acc_train: 0.3376 loss_val: 1.5381 acc_val: 0.4631
Epoch: 0032 loss_train: 1.6544 acc_train: 0.3954 loss_val: 1.4799 acc_val: 0.4815
Epoch: 0033 loss_train: 1.6186 acc_train: 0.4176 loss_val: 1.4184 acc_val: 0.5000
Epoch: 0034 loss_train: 1.5794 acc_train: 0.4182 loss_val: 1.3405 acc_val: 0.5904
Epoch: 0035 loss_train: 1.5365 acc_train: 0.4619 loss_val: 1.2625 acc_val: 0.6181
Epoch: 0036 loss_train: 1.4989 acc_train: 0.4668 loss_val: 1.1899 acc_val: 0.6587
Epoch: 0037 loss_train: 1.4559 acc_train: 0.5000 loss_val: 1.1226 acc_val: 0.7122
Epoch: 0038 loss_train: 1.4132 acc_train: 0.5394 loss_val: 1.0479 acc_val: 0.7565
Epoch: 0039 loss_train: 1.3690 acc_train: 0.5523 loss_val: 0.9806 acc_val: 0.7749
Epoch: 0040 loss_train: 1.3275 acc_train: 0.5633 loss_val: 0.9274 acc_val: 0.8100
Epoch: 0041 loss_train: 1.2854 acc_train: 0.6150 loss_val: 0.8561 acc_val: 0.8395
Epoch: 0042 loss_train: 1.2411 acc_train: 0.6089 loss_val: 0.7970 acc_val: 0.8782
Epoch: 0043 loss_train: 1.1975 acc_train: 0.6482 loss_val: 0.7302 acc_val: 0.8911
Epoch: 0044 loss_train: 1.1505 acc_train: 0.6556 loss_val: 0.6599 acc_val: 0.8893
Epoch: 0045 loss_train: 1.1020 acc_train: 0.6568 loss_val: 0.6279 acc_val: 0.8893
Epoch: 0046 loss_train: 1.0643 acc_train: 0.6820 loss_val: 0.5890 acc_val: 0.8930
Epoch: 0047 loss_train: 1.0593 acc_train: 0.6427 loss_val: 0.5063 acc_val: 0.9059
Epoch: 0048 loss_train: 0.9747 acc_train: 0.6931 loss_val: 0.5784 acc_val: 0.8579
Epoch: 0049 loss_train: 0.9953 acc_train: 0.6827 loss_val: 0.4696 acc_val: 0.9059
Epoch: 0050 loss_train: 0.9324 acc_train: 0.7239 loss_val: 0.5061 acc_val: 0.8764
Epoch: 0051 loss_train: 0.9544 acc_train: 0.6507 loss_val: 0.3876 acc_val: 0.9225
Epoch: 0052 loss_train: 0.8362 acc_train: 0.7485 loss_val: 0.3963 acc_val: 0.9317
Epoch: 0053 loss_train: 0.8151 acc_train: 0.7552 loss_val: 0.3932 acc_val: 0.9354
Epoch: 0054 loss_train: 0.7994 acc_train: 0.7417 loss_val: 0.3119 acc_val: 0.9520
Epoch: 0055 loss_train: 0.7132 acc_train: 0.7915 loss_val: 0.3151 acc_val: 0.9373
Epoch: 0056 loss_train: 0.7026 acc_train: 0.7958 loss_val: 0.3032 acc_val: 0.9373
Epoch: 0057 loss_train: 0.6773 acc_train: 0.8007 loss_val: 0.2662 acc_val: 0.9446
Epoch: 0058 loss_train: 0.6202 acc_train: 0.8247 loss_val: 0.2330 acc_val: 0.9631
Epoch: 0059 loss_train: 0.5687 acc_train: 0.8481 loss_val: 0.2347 acc_val: 0.9705
Epoch: 0060 loss_train: 0.5539 acc_train: 0.8462 loss_val: 0.2110 acc_val: 0.9742
Epoch: 0061 loss_train: 0.5077 acc_train: 0.8690 loss_val: 0.1814 acc_val: 0.9815
Epoch: 0062 loss_train: 0.4586 acc_train: 0.8795 loss_val: 0.1742 acc_val: 0.9815
Epoch: 0063 loss_train: 0.4347 acc_train: 0.8795 loss_val: 0.1610 acc_val: 0.9705
Epoch: 0064 loss_train: 0.3989 acc_train: 0.8930 loss_val: 0.1514 acc_val: 0.9723
Epoch: 0065 loss_train: 0.3649 acc_train: 0.9034 loss_val: 0.1384 acc_val: 0.9815
Epoch: 0066 loss_train: 0.3299 acc_train: 0.9157 loss_val: 0.1242 acc_val: 0.9889
Epoch: 0067 loss_train: 0.2976 acc_train: 0.9268 loss_val: 0.1211 acc_val: 0.9908
Epoch: 0068 loss_train: 0.2679 acc_train: 0.9397 loss_val: 0.1099 acc_val: 0.9797
Epoch: 0069 loss_train: 0.2383 acc_train: 0.9403 loss_val: 0.0972 acc_val: 0.9797
Epoch: 0070 loss_train: 0.2056 acc_train: 0.9533 loss_val: 0.0963 acc_val: 0.9797
Epoch: 0071 loss_train: 0.1770 acc_train: 0.9631 loss_val: 0.0976 acc_val: 0.9760
Epoch: 0072 loss_train: 0.1516 acc_train: 0.9692 loss_val: 0.0919 acc_val: 0.9797
Epoch: 0073 loss_train: 0.1237 acc_train: 0.9803 loss_val: 0.0967 acc_val: 0.9723
Epoch: 0074 loss_train: 0.0997 acc_train: 0.9852 loss_val: 0.1013 acc_val: 0.9742
Epoch: 0075 loss_train: 0.0834 acc_train: 0.9914 loss_val: 0.1138 acc_val: 0.9613
Epoch: 0076 loss_train: 0.0728 acc_train: 0.9938 loss_val: 0.1215 acc_val: 0.9594
Epoch: 0077 loss_train: 0.0604 acc_train: 0.9902 loss_val: 0.1026 acc_val: 0.9613
Epoch: 0078 loss_train: 0.0438 acc_train: 0.9969 loss_val: 0.1103 acc_val: 0.9576
Epoch: 0079 loss_train: 0.0411 acc_train: 0.9975 loss_val: 0.1182 acc_val: 0.9557
Epoch: 0080 loss_train: 0.0341 acc_train: 0.9951 loss_val: 0.1192 acc_val: 0.9557
Epoch: 0081 loss_train: 0.0263 acc_train: 0.9975 loss_val: 0.1049 acc_val: 0.9631
Epoch: 0082 loss_train: 0.0214 acc_train: 1.0000 loss_val: 0.0964 acc_val: 0.9668
Epoch: 0083 loss_train: 0.0161 acc_train: 1.0000 loss_val: 0.1031 acc_val: 0.9668
Epoch: 0084 loss_train: 0.0139 acc_train: 0.9988 loss_val: 0.0981 acc_val: 0.9723
Epoch: 0085 loss_train: 0.0117 acc_train: 1.0000 loss_val: 0.0889 acc_val: 0.9668
Epoch: 0086 loss_train: 0.0092 acc_train: 0.9994 loss_val: 0.0895 acc_val: 0.9649
Epoch: 0087 loss_train: 0.0087 acc_train: 1.0000 loss_val: 0.0818 acc_val: 0.9705
Epoch: 0088 loss_train: 0.0069 acc_train: 1.0000 loss_val: 0.0749 acc_val: 0.9705
Epoch: 0089 loss_train: 0.0055 acc_train: 1.0000 loss_val: 0.0790 acc_val: 0.9797
Epoch: 0090 loss_train: 0.0055 acc_train: 1.0000 loss_val: 0.0803 acc_val: 0.9797
Epoch: 0091 loss_train: 0.0048 acc_train: 1.0000 loss_val: 0.0743 acc_val: 0.9760
Epoch: 0092 loss_train: 0.0042 acc_train: 1.0000 loss_val: 0.0672 acc_val: 0.9760
Epoch: 0093 loss_train: 0.0036 acc_train: 1.0000 loss_val: 0.0660 acc_val: 0.9779
Epoch: 0094 loss_train: 0.0036 acc_train: 1.0000 loss_val: 0.0667 acc_val: 0.9797
Epoch: 0095 loss_train: 0.0033 acc_train: 1.0000 loss_val: 0.0669 acc_val: 0.9760
Epoch: 0096 loss_train: 0.0028 acc_train: 1.0000 loss_val: 0.0693 acc_val: 0.9742
Epoch: 0097 loss_train: 0.0027 acc_train: 1.0000 loss_val: 0.0738 acc_val: 0.9797
Epoch: 0098 loss_train: 0.0022 acc_train: 1.0000 loss_val: 0.0784 acc_val: 0.9779
Epoch: 0099 loss_train: 0.0021 acc_train: 1.0000 loss_val: 0.0784 acc_val: 0.9797
Epoch: 0100 loss_train: 0.0021 acc_train: 1.0000 loss_val: 0.0738 acc_val: 0.9815
Epoch: 0101 loss_train: 0.0018 acc_train: 1.0000 loss_val: 0.0681 acc_val: 0.9815
Epoch: 0102 loss_train: 0.0016 acc_train: 1.0000 loss_val: 0.0628 acc_val: 0.9797
Epoch: 0103 loss_train: 0.0015 acc_train: 1.0000 loss_val: 0.0594 acc_val: 0.9797
Epoch: 0104 loss_train: 0.0014 acc_train: 1.0000 loss_val: 0.0574 acc_val: 0.9815
Epoch: 0105 loss_train: 0.0015 acc_train: 1.0000 loss_val: 0.0566 acc_val: 0.9852
Epoch: 0106 loss_train: 0.0014 acc_train: 1.0000 loss_val: 0.0566 acc_val: 0.9834
Epoch: 0107 loss_train: 0.0012 acc_train: 1.0000 loss_val: 0.0574 acc_val: 0.9834
Epoch: 0108 loss_train: 0.0011 acc_train: 1.0000 loss_val: 0.0593 acc_val: 0.9834
Epoch: 0109 loss_train: 0.0011 acc_train: 1.0000 loss_val: 0.0620 acc_val: 0.9834
Epoch: 0110 loss_train: 0.0011 acc_train: 1.0000 loss_val: 0.0648 acc_val: 0.9815
Epoch: 0111 loss_train: 0.0010 acc_train: 1.0000 loss_val: 0.0669 acc_val: 0.9815
Epoch: 0112 loss_train: 0.0010 acc_train: 1.0000 loss_val: 0.0682 acc_val: 0.9834
Epoch: 0113 loss_train: 0.0010 acc_train: 1.0000 loss_val: 0.0682 acc_val: 0.9834
Epoch: 0114 loss_train: 0.0009 acc_train: 1.0000 loss_val: 0.0673 acc_val: 0.9834
Epoch: 0115 loss_train: 0.0009 acc_train: 1.0000 loss_val: 0.0656 acc_val: 0.9815
Epoch: 0116 loss_train: 0.0009 acc_train: 1.0000 loss_val: 0.0631 acc_val: 0.9815
Epoch: 0117 loss_train: 0.0009 acc_train: 1.0000 loss_val: 0.0603 acc_val: 0.9834
Epoch: 0118 loss_train: 0.0008 acc_train: 1.0000 loss_val: 0.0582 acc_val: 0.9834
Epoch: 0119 loss_train: 0.0008 acc_train: 1.0000 loss_val: 0.0567 acc_val: 0.9834
Epoch: 0120 loss_train: 0.0008 acc_train: 1.0000 loss_val: 0.0561 acc_val: 0.9834
Epoch: 0121 loss_train: 0.0008 acc_train: 1.0000 loss_val: 0.0560 acc_val: 0.9834
Epoch: 0122 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0562 acc_val: 0.9834
Epoch: 0123 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0567 acc_val: 0.9834
Epoch: 0124 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0575 acc_val: 0.9834
Epoch: 0125 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0584 acc_val: 0.9834
Epoch: 0126 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0589 acc_val: 0.9834
Epoch: 0127 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0592 acc_val: 0.9834
Epoch: 0128 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0596 acc_val: 0.9834
Epoch: 0129 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0600 acc_val: 0.9834
Epoch: 0130 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0601 acc_val: 0.9834
Epoch: 0131 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0599 acc_val: 0.9852
Epoch: 0132 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0590 acc_val: 0.9834
Epoch: 0133 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0584 acc_val: 0.9834
Epoch: 0134 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0572 acc_val: 0.9834
Epoch: 0135 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0560 acc_val: 0.9834
Epoch: 0136 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0547 acc_val: 0.9834
Epoch: 0137 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0537 acc_val: 0.9834
Epoch: 0138 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0532 acc_val: 0.9834
Epoch: 0139 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0529 acc_val: 0.9834
Epoch: 0140 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0532 acc_val: 0.9834
Epoch: 0141 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0535 acc_val: 0.9834
Epoch: 0142 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0532 acc_val: 0.9834
Epoch: 0143 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0530 acc_val: 0.9834
Epoch: 0144 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0528 acc_val: 0.9834
Epoch: 0145 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0529 acc_val: 0.9834
Epoch: 0146 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0530 acc_val: 0.9834
Epoch: 0147 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0532 acc_val: 0.9834
Epoch: 0148 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0537 acc_val: 0.9834
Epoch: 0149 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0539 acc_val: 0.9834
Epoch: 0150 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0543 acc_val: 0.9834
Epoch: 0151 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0549 acc_val: 0.9852
Epoch: 0152 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0551 acc_val: 0.9852
Epoch: 0153 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0551 acc_val: 0.9852
Epoch: 0154 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0549 acc_val: 0.9852
Epoch: 0155 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0546 acc_val: 0.9852
Epoch: 0156 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0543 acc_val: 0.9834
Epoch: 0157 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0542 acc_val: 0.9834
Epoch: 0158 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0540 acc_val: 0.9834
Epoch: 0159 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0537 acc_val: 0.9834
Epoch: 0160 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0534 acc_val: 0.9834
Epoch: 0161 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0533 acc_val: 0.9834
Epoch: 0162 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0532 acc_val: 0.9834
Epoch: 0163 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0535 acc_val: 0.9852
Epoch: 0164 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0536 acc_val: 0.9852
Epoch: 0165 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0543 acc_val: 0.9852
Epoch: 0166 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0550 acc_val: 0.9871
Epoch: 0167 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0555 acc_val: 0.9871
Epoch: 0168 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0560 acc_val: 0.9871
Epoch: 0169 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0556 acc_val: 0.9871
Epoch: 0170 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0547 acc_val: 0.9871
Epoch: 0171 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0539 acc_val: 0.9852
Epoch: 0172 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0525 acc_val: 0.9852
Epoch: 0173 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0514 acc_val: 0.9852
Epoch: 0174 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0505 acc_val: 0.9852
Epoch: 0175 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0499 acc_val: 0.9852
Epoch: 0176 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0497 acc_val: 0.9852
Epoch: 0177 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0498 acc_val: 0.9852
Epoch: 0178 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0507 acc_val: 0.9852
Epoch: 0179 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0519 acc_val: 0.9852
Epoch: 0180 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0533 acc_val: 0.9871
Epoch: 0181 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0549 acc_val: 0.9871
Epoch: 0182 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0561 acc_val: 0.9871
Epoch: 0183 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0569 acc_val: 0.9871
Epoch: 0184 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0572 acc_val: 0.9871
Epoch: 0185 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0569 acc_val: 0.9871
Epoch: 0186 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0559 acc_val: 0.9871
Epoch: 0187 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0547 acc_val: 0.9871
Epoch: 0188 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0534 acc_val: 0.9871
Epoch: 0189 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0520 acc_val: 0.9871
Epoch: 0190 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0505 acc_val: 0.9871
Epoch: 0191 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0494 acc_val: 0.9852
Epoch: 0192 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0487 acc_val: 0.9852
Epoch: 0193 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0485 acc_val: 0.9852
Epoch: 0194 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0489 acc_val: 0.9852
Epoch: 0195 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0496 acc_val: 0.9852
Epoch: 0196 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0503 acc_val: 0.9852
Epoch: 0197 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0511 acc_val: 0.9852
Epoch: 0198 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0520 acc_val: 0.9871
Epoch: 0199 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0530 acc_val: 0.9889
Epoch: 0200 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0528 acc_val: 0.9889
Optimization Finished!
Train cost: 26.1063s
Loading 67th epoch
Test set results: loss= 0.1070 accuracy= 0.9907
  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:

	DGLGraph.adjacency_matrix(transpose, scipy_fmt="csr").

  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '
/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)
  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1448, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU(approximate='none')
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=7, bias=True)
)
total params: 2979849
Epoch: 0001 loss_train: 1.9684 acc_train: 0.1138 loss_val: 1.9664 acc_val: 0.1310
Epoch: 0002 loss_train: 1.9659 acc_train: 0.1138 loss_val: 1.9561 acc_val: 0.1310
Epoch: 0003 loss_train: 1.9579 acc_train: 0.1175 loss_val: 1.9414 acc_val: 0.1328
Epoch: 0004 loss_train: 1.9444 acc_train: 0.1156 loss_val: 1.9233 acc_val: 0.2214
Epoch: 0005 loss_train: 1.9278 acc_train: 0.1574 loss_val: 1.9035 acc_val: 0.3026
Epoch: 0006 loss_train: 1.9103 acc_train: 0.2891 loss_val: 1.8836 acc_val: 0.3026
Epoch: 0007 loss_train: 1.8926 acc_train: 0.3020 loss_val: 1.8660 acc_val: 0.3026
Epoch: 0008 loss_train: 1.8756 acc_train: 0.3020 loss_val: 1.8524 acc_val: 0.3026
Epoch: 0009 loss_train: 1.8614 acc_train: 0.3020 loss_val: 1.8424 acc_val: 0.3026
Epoch: 0010 loss_train: 1.8495 acc_train: 0.3020 loss_val: 1.8359 acc_val: 0.3026
Epoch: 0011 loss_train: 1.8412 acc_train: 0.3020 loss_val: 1.8324 acc_val: 0.3026
Epoch: 0012 loss_train: 1.8367 acc_train: 0.3020 loss_val: 1.8310 acc_val: 0.3026
Epoch: 0013 loss_train: 1.8350 acc_train: 0.3020 loss_val: 1.8306 acc_val: 0.3026
Epoch: 0014 loss_train: 1.8331 acc_train: 0.3020 loss_val: 1.8297 acc_val: 0.3026
Epoch: 0015 loss_train: 1.8318 acc_train: 0.3020 loss_val: 1.8280 acc_val: 0.3026
Epoch: 0016 loss_train: 1.8310 acc_train: 0.3020 loss_val: 1.8252 acc_val: 0.3026
Epoch: 0017 loss_train: 1.8285 acc_train: 0.3020 loss_val: 1.8217 acc_val: 0.3026
Epoch: 0018 loss_train: 1.8262 acc_train: 0.3020 loss_val: 1.8179 acc_val: 0.3026
Epoch: 0019 loss_train: 1.8242 acc_train: 0.3020 loss_val: 1.8138 acc_val: 0.3026
Epoch: 0020 loss_train: 1.8229 acc_train: 0.3020 loss_val: 1.8089 acc_val: 0.3026
Epoch: 0021 loss_train: 1.8214 acc_train: 0.3020 loss_val: 1.8040 acc_val: 0.3026
Epoch: 0022 loss_train: 1.8182 acc_train: 0.3020 loss_val: 1.7982 acc_val: 0.3026
Epoch: 0023 loss_train: 1.8151 acc_train: 0.3020 loss_val: 1.7911 acc_val: 0.3026
Epoch: 0024 loss_train: 1.8118 acc_train: 0.3020 loss_val: 1.7823 acc_val: 0.3026
Epoch: 0025 loss_train: 1.8057 acc_train: 0.3020 loss_val: 1.7721 acc_val: 0.3026
Epoch: 0026 loss_train: 1.7993 acc_train: 0.3020 loss_val: 1.7609 acc_val: 0.3026
Epoch: 0027 loss_train: 1.7920 acc_train: 0.3020 loss_val: 1.7487 acc_val: 0.3026
Epoch: 0028 loss_train: 1.7842 acc_train: 0.3020 loss_val: 1.7356 acc_val: 0.3026
Epoch: 0029 loss_train: 1.7772 acc_train: 0.3020 loss_val: 1.7208 acc_val: 0.3026
Epoch: 0030 loss_train: 1.7676 acc_train: 0.3020 loss_val: 1.7028 acc_val: 0.3026
Epoch: 0031 loss_train: 1.7557 acc_train: 0.3020 loss_val: 1.6809 acc_val: 0.3026
Epoch: 0032 loss_train: 1.7446 acc_train: 0.3020 loss_val: 1.6553 acc_val: 0.3026
Epoch: 0033 loss_train: 1.7305 acc_train: 0.3020 loss_val: 1.6240 acc_val: 0.3026
Epoch: 0034 loss_train: 1.7143 acc_train: 0.3020 loss_val: 1.5861 acc_val: 0.3026
Epoch: 0035 loss_train: 1.6939 acc_train: 0.3020 loss_val: 1.5474 acc_val: 0.3026
Epoch: 0036 loss_train: 1.6700 acc_train: 0.3026 loss_val: 1.5036 acc_val: 0.3026
Epoch: 0037 loss_train: 1.6416 acc_train: 0.3032 loss_val: 1.4490 acc_val: 0.3118
Epoch: 0038 loss_train: 1.6104 acc_train: 0.3155 loss_val: 1.3820 acc_val: 0.4262
Epoch: 0039 loss_train: 1.5754 acc_train: 0.3942 loss_val: 1.3090 acc_val: 0.6292
Epoch: 0040 loss_train: 1.5358 acc_train: 0.4926 loss_val: 1.2379 acc_val: 0.7196
Epoch: 0041 loss_train: 1.4946 acc_train: 0.5418 loss_val: 1.1698 acc_val: 0.7694
Epoch: 0042 loss_train: 1.4485 acc_train: 0.5732 loss_val: 1.0987 acc_val: 0.8063
Epoch: 0043 loss_train: 1.3995 acc_train: 0.5929 loss_val: 1.0201 acc_val: 0.8376
Epoch: 0044 loss_train: 1.3389 acc_train: 0.6169 loss_val: 0.9420 acc_val: 0.8469
Epoch: 0045 loss_train: 1.2745 acc_train: 0.6427 loss_val: 0.8730 acc_val: 0.8469
Epoch: 0046 loss_train: 1.2027 acc_train: 0.6617 loss_val: 0.8026 acc_val: 0.8487
Epoch: 0047 loss_train: 1.1319 acc_train: 0.6839 loss_val: 0.7338 acc_val: 0.8506
Epoch: 0048 loss_train: 1.0541 acc_train: 0.7196 loss_val: 0.6621 acc_val: 0.8635
Epoch: 0049 loss_train: 0.9788 acc_train: 0.7319 loss_val: 0.6248 acc_val: 0.8653
Epoch: 0050 loss_train: 0.9160 acc_train: 0.7743 loss_val: 0.5359 acc_val: 0.9133
Epoch: 0051 loss_train: 0.8644 acc_train: 0.7355 loss_val: 0.5768 acc_val: 0.8616
Epoch: 0052 loss_train: 0.8237 acc_train: 0.7891 loss_val: 0.5048 acc_val: 0.8690
Epoch: 0053 loss_train: 0.7446 acc_train: 0.8075 loss_val: 0.4156 acc_val: 0.9410
Epoch: 0054 loss_train: 0.6803 acc_train: 0.8413 loss_val: 0.3713 acc_val: 0.9373
Epoch: 0055 loss_train: 0.6067 acc_train: 0.8825 loss_val: 0.3557 acc_val: 0.9299
Epoch: 0056 loss_train: 0.5830 acc_train: 0.8376 loss_val: 0.3375 acc_val: 0.9391
Epoch: 0057 loss_train: 0.5352 acc_train: 0.8973 loss_val: 0.2694 acc_val: 0.9742
Epoch: 0058 loss_train: 0.4668 acc_train: 0.9139 loss_val: 0.2276 acc_val: 0.9871
Epoch: 0059 loss_train: 0.4069 acc_train: 0.9397 loss_val: 0.2158 acc_val: 0.9871
Epoch: 0060 loss_train: 0.3893 acc_train: 0.9348 loss_val: 0.1816 acc_val: 0.9797
Epoch: 0061 loss_train: 0.3367 acc_train: 0.9256 loss_val: 0.1625 acc_val: 0.9945
Epoch: 0062 loss_train: 0.3005 acc_train: 0.9619 loss_val: 0.1316 acc_val: 0.9945
Epoch: 0063 loss_train: 0.2526 acc_train: 0.9680 loss_val: 0.1229 acc_val: 0.9982
Epoch: 0064 loss_train: 0.2304 acc_train: 0.9668 loss_val: 0.0976 acc_val: 0.9963
Epoch: 0065 loss_train: 0.1862 acc_train: 0.9840 loss_val: 0.0920 acc_val: 0.9963
Epoch: 0066 loss_train: 0.1639 acc_train: 0.9760 loss_val: 0.0742 acc_val: 1.0000
Epoch: 0067 loss_train: 0.1350 acc_train: 0.9926 loss_val: 0.0661 acc_val: 1.0000
Epoch: 0068 loss_train: 0.1149 acc_train: 0.9982 loss_val: 0.0593 acc_val: 0.9963
Epoch: 0069 loss_train: 0.0955 acc_train: 0.9945 loss_val: 0.0546 acc_val: 0.9963
Epoch: 0070 loss_train: 0.0789 acc_train: 0.9969 loss_val: 0.0436 acc_val: 0.9982
Epoch: 0071 loss_train: 0.0615 acc_train: 1.0000 loss_val: 0.0387 acc_val: 1.0000
Epoch: 0072 loss_train: 0.0536 acc_train: 0.9994 loss_val: 0.0354 acc_val: 0.9982
Epoch: 0073 loss_train: 0.0433 acc_train: 0.9994 loss_val: 0.0309 acc_val: 1.0000
Epoch: 0074 loss_train: 0.0322 acc_train: 1.0000 loss_val: 0.0328 acc_val: 0.9982
Epoch: 0075 loss_train: 0.0313 acc_train: 1.0000 loss_val: 0.0311 acc_val: 0.9963
Epoch: 0076 loss_train: 0.0252 acc_train: 1.0000 loss_val: 0.0276 acc_val: 0.9963
Epoch: 0077 loss_train: 0.0202 acc_train: 1.0000 loss_val: 0.0214 acc_val: 0.9982
Epoch: 0078 loss_train: 0.0162 acc_train: 1.0000 loss_val: 0.0184 acc_val: 1.0000
Epoch: 0079 loss_train: 0.0144 acc_train: 1.0000 loss_val: 0.0175 acc_val: 0.9982
Epoch: 0080 loss_train: 0.0126 acc_train: 1.0000 loss_val: 0.0204 acc_val: 0.9963
Epoch: 0081 loss_train: 0.0112 acc_train: 1.0000 loss_val: 0.0191 acc_val: 0.9963
Epoch: 0082 loss_train: 0.0095 acc_train: 1.0000 loss_val: 0.0144 acc_val: 1.0000
Epoch: 0083 loss_train: 0.0073 acc_train: 1.0000 loss_val: 0.0129 acc_val: 1.0000
Epoch: 0084 loss_train: 0.0068 acc_train: 1.0000 loss_val: 0.0126 acc_val: 1.0000
Epoch: 0085 loss_train: 0.0062 acc_train: 1.0000 loss_val: 0.0138 acc_val: 0.9982
Epoch: 0086 loss_train: 0.0054 acc_train: 1.0000 loss_val: 0.0164 acc_val: 0.9963
Epoch: 0087 loss_train: 0.0050 acc_train: 1.0000 loss_val: 0.0163 acc_val: 0.9963
Epoch: 0088 loss_train: 0.0044 acc_train: 1.0000 loss_val: 0.0131 acc_val: 0.9982
Epoch: 0089 loss_train: 0.0038 acc_train: 1.0000 loss_val: 0.0097 acc_val: 0.9982
Epoch: 0090 loss_train: 0.0035 acc_train: 1.0000 loss_val: 0.0074 acc_val: 1.0000
Epoch: 0091 loss_train: 0.0031 acc_train: 1.0000 loss_val: 0.0065 acc_val: 1.0000
Epoch: 0092 loss_train: 0.0028 acc_train: 1.0000 loss_val: 0.0071 acc_val: 1.0000
Epoch: 0093 loss_train: 0.0025 acc_train: 1.0000 loss_val: 0.0085 acc_val: 0.9982
Epoch: 0094 loss_train: 0.0024 acc_train: 1.0000 loss_val: 0.0086 acc_val: 0.9963
Epoch: 0095 loss_train: 0.0023 acc_train: 1.0000 loss_val: 0.0072 acc_val: 0.9982
Epoch: 0096 loss_train: 0.0021 acc_train: 1.0000 loss_val: 0.0059 acc_val: 1.0000
Epoch: 0097 loss_train: 0.0019 acc_train: 1.0000 loss_val: 0.0051 acc_val: 1.0000
Epoch: 0098 loss_train: 0.0018 acc_train: 1.0000 loss_val: 0.0045 acc_val: 1.0000
Epoch: 0099 loss_train: 0.0017 acc_train: 1.0000 loss_val: 0.0042 acc_val: 1.0000
Epoch: 0100 loss_train: 0.0015 acc_train: 1.0000 loss_val: 0.0044 acc_val: 1.0000
Epoch: 0101 loss_train: 0.0014 acc_train: 1.0000 loss_val: 0.0049 acc_val: 1.0000
Epoch: 0102 loss_train: 0.0014 acc_train: 1.0000 loss_val: 0.0051 acc_val: 1.0000
Epoch: 0103 loss_train: 0.0014 acc_train: 1.0000 loss_val: 0.0047 acc_val: 1.0000
Epoch: 0104 loss_train: 0.0012 acc_train: 1.0000 loss_val: 0.0042 acc_val: 1.0000
Epoch: 0105 loss_train: 0.0012 acc_train: 1.0000 loss_val: 0.0040 acc_val: 1.0000
Epoch: 0106 loss_train: 0.0011 acc_train: 1.0000 loss_val: 0.0040 acc_val: 1.0000
Epoch: 0107 loss_train: 0.0011 acc_train: 1.0000 loss_val: 0.0042 acc_val: 1.0000
Epoch: 0108 loss_train: 0.0010 acc_train: 1.0000 loss_val: 0.0044 acc_val: 1.0000
Epoch: 0109 loss_train: 0.0010 acc_train: 1.0000 loss_val: 0.0045 acc_val: 1.0000
Epoch: 0110 loss_train: 0.0010 acc_train: 1.0000 loss_val: 0.0045 acc_val: 1.0000
Epoch: 0111 loss_train: 0.0009 acc_train: 1.0000 loss_val: 0.0043 acc_val: 1.0000
Epoch: 0112 loss_train: 0.0009 acc_train: 1.0000 loss_val: 0.0040 acc_val: 1.0000
Epoch: 0113 loss_train: 0.0008 acc_train: 1.0000 loss_val: 0.0036 acc_val: 1.0000
Epoch: 0114 loss_train: 0.0008 acc_train: 1.0000 loss_val: 0.0032 acc_val: 1.0000
Epoch: 0115 loss_train: 0.0008 acc_train: 1.0000 loss_val: 0.0030 acc_val: 1.0000
Epoch: 0116 loss_train: 0.0008 acc_train: 1.0000 loss_val: 0.0029 acc_val: 1.0000
Epoch: 0117 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0029 acc_val: 1.0000
Epoch: 0118 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0030 acc_val: 1.0000
Epoch: 0119 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0031 acc_val: 1.0000
Epoch: 0120 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0031 acc_val: 1.0000
Epoch: 0121 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0032 acc_val: 1.0000
Epoch: 0122 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0030 acc_val: 1.0000
Epoch: 0123 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0028 acc_val: 1.0000
Epoch: 0124 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.0027 acc_val: 1.0000
Epoch: 0125 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0026 acc_val: 1.0000
Epoch: 0126 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0025 acc_val: 1.0000
Epoch: 0127 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0025 acc_val: 1.0000
Epoch: 0128 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0026 acc_val: 1.0000
Epoch: 0129 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0028 acc_val: 1.0000
Epoch: 0130 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.0030 acc_val: 1.0000
Epoch: 0131 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0030 acc_val: 1.0000
Epoch: 0132 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0029 acc_val: 1.0000
Epoch: 0133 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0027 acc_val: 1.0000
Epoch: 0134 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0024 acc_val: 1.0000
Epoch: 0135 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0022 acc_val: 1.0000
Epoch: 0136 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0020 acc_val: 1.0000
Epoch: 0137 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0019 acc_val: 1.0000
Epoch: 0138 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0019 acc_val: 1.0000
Epoch: 0139 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0020 acc_val: 1.0000
Epoch: 0140 loss_train: 0.0005 acc_train: 1.0000 loss_val: 0.0021 acc_val: 1.0000
Epoch: 0141 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0022 acc_val: 1.0000
Epoch: 0142 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0022 acc_val: 1.0000
Epoch: 0143 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0022 acc_val: 1.0000
Epoch: 0144 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0020 acc_val: 1.0000
Epoch: 0145 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0019 acc_val: 1.0000
Epoch: 0146 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0018 acc_val: 1.0000
Epoch: 0147 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0017 acc_val: 1.0000
Epoch: 0148 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0017 acc_val: 1.0000
Epoch: 0149 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0016 acc_val: 1.0000
Epoch: 0150 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0016 acc_val: 1.0000
Epoch: 0151 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0016 acc_val: 1.0000
Epoch: 0152 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0017 acc_val: 1.0000
Epoch: 0153 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0017 acc_val: 1.0000
Epoch: 0154 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0017 acc_val: 1.0000
Epoch: 0155 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.0016 acc_val: 1.0000
Epoch: 0156 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0017 acc_val: 1.0000
Epoch: 0157 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0017 acc_val: 1.0000
Epoch: 0158 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0017 acc_val: 1.0000
Epoch: 0159 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0016 acc_val: 1.0000
Epoch: 0160 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0016 acc_val: 1.0000
Epoch: 0161 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0015 acc_val: 1.0000
Epoch: 0162 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0014 acc_val: 1.0000
Epoch: 0163 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0013 acc_val: 1.0000
Epoch: 0164 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0013 acc_val: 1.0000
Epoch: 0165 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0013 acc_val: 1.0000
Epoch: 0166 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0013 acc_val: 1.0000
Epoch: 0167 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0013 acc_val: 1.0000
Epoch: 0168 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0013 acc_val: 1.0000
Epoch: 0169 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0013 acc_val: 1.0000
Epoch: 0170 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0013 acc_val: 1.0000
Epoch: 0171 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0013 acc_val: 1.0000
Epoch: 0172 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0013 acc_val: 1.0000
Epoch: 0173 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0013 acc_val: 1.0000
Epoch: 0174 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0013 acc_val: 1.0000
Epoch: 0175 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0012 acc_val: 1.0000
Epoch: 0176 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0011 acc_val: 1.0000
Epoch: 0177 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0010 acc_val: 1.0000
Epoch: 0178 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0010 acc_val: 1.0000
Epoch: 0179 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0010 acc_val: 1.0000
Epoch: 0180 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0011 acc_val: 1.0000
Epoch: 0181 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0012 acc_val: 1.0000
Epoch: 0182 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0012 acc_val: 1.0000
Epoch: 0183 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0012 acc_val: 1.0000
Epoch: 0184 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0011 acc_val: 1.0000
Epoch: 0185 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0010 acc_val: 1.0000
Epoch: 0186 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0009 acc_val: 1.0000
Epoch: 0187 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0008 acc_val: 1.0000
Epoch: 0188 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0008 acc_val: 1.0000
Epoch: 0189 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.0009 acc_val: 1.0000
Epoch: 0190 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0011 acc_val: 1.0000
Epoch: 0191 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0012 acc_val: 1.0000
Epoch: 0192 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0013 acc_val: 1.0000
Epoch: 0193 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0011 acc_val: 1.0000
Epoch: 0194 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0010 acc_val: 1.0000
Epoch: 0195 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0009 acc_val: 1.0000
Epoch: 0196 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0008 acc_val: 1.0000
Epoch: 0197 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0008 acc_val: 1.0000
Epoch: 0198 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0007 acc_val: 1.0000
Epoch: 0199 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0007 acc_val: 1.0000
Epoch: 0200 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.0007 acc_val: 1.0000
Optimization Finished!
Train cost: 26.8047s
Loading 199th epoch
Test set results: loss= 0.0006 accuracy= 1.0000