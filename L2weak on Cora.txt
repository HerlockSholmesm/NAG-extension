  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:

	DGLGraph.adjacency_matrix(transpose, scipy_fmt="csr").

  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '
/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)
  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1448, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU(approximate='none')
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=7, bias=True)
)
total params: 2979849
Epoch: 0001 loss_train: 1.9727 acc_train: 0.1156 loss_val: 1.9686 acc_val: 0.1310
Epoch: 0002 loss_train: 1.9669 acc_train: 0.1279 loss_val: 1.9561 acc_val: 0.1384
Epoch: 0003 loss_train: 1.9547 acc_train: 0.1396 loss_val: 1.9375 acc_val: 0.1587
Epoch: 0004 loss_train: 1.9373 acc_train: 0.1525 loss_val: 1.9130 acc_val: 0.2122
Epoch: 0005 loss_train: 1.9160 acc_train: 0.1925 loss_val: 1.8830 acc_val: 0.3118
Epoch: 0006 loss_train: 1.8886 acc_train: 0.2768 loss_val: 1.8478 acc_val: 0.4428
Epoch: 0007 loss_train: 1.8561 acc_train: 0.3930 loss_val: 1.8078 acc_val: 0.4815
Epoch: 0008 loss_train: 1.8199 acc_train: 0.4736 loss_val: 1.7640 acc_val: 0.5055
Epoch: 0009 loss_train: 1.7805 acc_train: 0.4889 loss_val: 1.7170 acc_val: 0.5055
Epoch: 0010 loss_train: 1.7355 acc_train: 0.5092 loss_val: 1.6678 acc_val: 0.5166
Epoch: 0011 loss_train: 1.6880 acc_train: 0.5203 loss_val: 1.6169 acc_val: 0.5277
Epoch: 0012 loss_train: 1.6405 acc_train: 0.5295 loss_val: 1.5647 acc_val: 0.5461
Epoch: 0013 loss_train: 1.5898 acc_train: 0.5387 loss_val: 1.5112 acc_val: 0.5517
Epoch: 0014 loss_train: 1.5383 acc_train: 0.5541 loss_val: 1.4567 acc_val: 0.5627
Epoch: 0015 loss_train: 1.4837 acc_train: 0.5658 loss_val: 1.4014 acc_val: 0.5701
Epoch: 0016 loss_train: 1.4274 acc_train: 0.5824 loss_val: 1.3452 acc_val: 0.5849
Epoch: 0017 loss_train: 1.3709 acc_train: 0.5996 loss_val: 1.2880 acc_val: 0.6070
Epoch: 0018 loss_train: 1.3128 acc_train: 0.6193 loss_val: 1.2305 acc_val: 0.6494
Epoch: 0019 loss_train: 1.2546 acc_train: 0.6513 loss_val: 1.1740 acc_val: 0.6734
Epoch: 0020 loss_train: 1.1946 acc_train: 0.6845 loss_val: 1.1196 acc_val: 0.6882
Epoch: 0021 loss_train: 1.1373 acc_train: 0.7103 loss_val: 1.0686 acc_val: 0.6993
Epoch: 0022 loss_train: 1.0810 acc_train: 0.7269 loss_val: 1.0214 acc_val: 0.6974
Epoch: 0023 loss_train: 1.0270 acc_train: 0.7386 loss_val: 0.9779 acc_val: 0.7103
Epoch: 0024 loss_train: 0.9762 acc_train: 0.7491 loss_val: 0.9369 acc_val: 0.7214
Epoch: 0025 loss_train: 0.9308 acc_train: 0.7558 loss_val: 0.8978 acc_val: 0.7288
Epoch: 0026 loss_train: 0.8845 acc_train: 0.7706 loss_val: 0.8600 acc_val: 0.7306
Epoch: 0027 loss_train: 0.8421 acc_train: 0.7811 loss_val: 0.8239 acc_val: 0.7472
Epoch: 0028 loss_train: 0.8022 acc_train: 0.8007 loss_val: 0.7899 acc_val: 0.7509
Epoch: 0029 loss_train: 0.7630 acc_train: 0.8093 loss_val: 0.7580 acc_val: 0.7638
Epoch: 0030 loss_train: 0.7271 acc_train: 0.8167 loss_val: 0.7286 acc_val: 0.7768
Epoch: 0031 loss_train: 0.6939 acc_train: 0.8266 loss_val: 0.7010 acc_val: 0.7804
Epoch: 0032 loss_train: 0.6632 acc_train: 0.8284 loss_val: 0.6747 acc_val: 0.7823
Epoch: 0033 loss_train: 0.6327 acc_train: 0.8346 loss_val: 0.6496 acc_val: 0.7823
Epoch: 0034 loss_train: 0.6044 acc_train: 0.8432 loss_val: 0.6263 acc_val: 0.7878
Epoch: 0035 loss_train: 0.5759 acc_train: 0.8481 loss_val: 0.6049 acc_val: 0.8044
Epoch: 0036 loss_train: 0.5496 acc_train: 0.8524 loss_val: 0.5844 acc_val: 0.8081
Epoch: 0037 loss_train: 0.5268 acc_train: 0.8598 loss_val: 0.5635 acc_val: 0.8118
Epoch: 0038 loss_train: 0.5036 acc_train: 0.8598 loss_val: 0.5414 acc_val: 0.8192
Epoch: 0039 loss_train: 0.4787 acc_train: 0.8647 loss_val: 0.5187 acc_val: 0.8266
Epoch: 0040 loss_train: 0.4569 acc_train: 0.8684 loss_val: 0.4966 acc_val: 0.8266
Epoch: 0041 loss_train: 0.4365 acc_train: 0.8739 loss_val: 0.4760 acc_val: 0.8303
Epoch: 0042 loss_train: 0.4171 acc_train: 0.8733 loss_val: 0.4578 acc_val: 0.8413
Epoch: 0043 loss_train: 0.3960 acc_train: 0.8819 loss_val: 0.4426 acc_val: 0.8561
Epoch: 0044 loss_train: 0.3774 acc_train: 0.8887 loss_val: 0.4296 acc_val: 0.8635
Epoch: 0045 loss_train: 0.3595 acc_train: 0.8918 loss_val: 0.4170 acc_val: 0.8690
Epoch: 0046 loss_train: 0.3405 acc_train: 0.8998 loss_val: 0.4041 acc_val: 0.8727
Epoch: 0047 loss_train: 0.3229 acc_train: 0.9090 loss_val: 0.3923 acc_val: 0.8745
Epoch: 0048 loss_train: 0.3065 acc_train: 0.9157 loss_val: 0.3836 acc_val: 0.8838
Epoch: 0049 loss_train: 0.2879 acc_train: 0.9194 loss_val: 0.3782 acc_val: 0.8875
Epoch: 0050 loss_train: 0.2725 acc_train: 0.9274 loss_val: 0.3758 acc_val: 0.8893
Epoch: 0051 loss_train: 0.2565 acc_train: 0.9293 loss_val: 0.3755 acc_val: 0.8838
Epoch: 0052 loss_train: 0.2410 acc_train: 0.9342 loss_val: 0.3742 acc_val: 0.8819
Epoch: 0053 loss_train: 0.2288 acc_train: 0.9403 loss_val: 0.3732 acc_val: 0.8838
Epoch: 0054 loss_train: 0.2129 acc_train: 0.9434 loss_val: 0.3749 acc_val: 0.8819
Epoch: 0055 loss_train: 0.1947 acc_train: 0.9508 loss_val: 0.3805 acc_val: 0.8782
Epoch: 0056 loss_train: 0.1813 acc_train: 0.9533 loss_val: 0.3863 acc_val: 0.8801
Epoch: 0057 loss_train: 0.1677 acc_train: 0.9576 loss_val: 0.3874 acc_val: 0.8782
Epoch: 0058 loss_train: 0.1540 acc_train: 0.9619 loss_val: 0.3872 acc_val: 0.8782
Epoch: 0059 loss_train: 0.1432 acc_train: 0.9656 loss_val: 0.3902 acc_val: 0.8782
Epoch: 0060 loss_train: 0.1315 acc_train: 0.9686 loss_val: 0.3960 acc_val: 0.8801
Epoch: 0061 loss_train: 0.1190 acc_train: 0.9729 loss_val: 0.4023 acc_val: 0.8782
Epoch: 0062 loss_train: 0.1102 acc_train: 0.9760 loss_val: 0.4080 acc_val: 0.8764
Epoch: 0063 loss_train: 0.1000 acc_train: 0.9822 loss_val: 0.4138 acc_val: 0.8764
Epoch: 0064 loss_train: 0.0907 acc_train: 0.9859 loss_val: 0.4196 acc_val: 0.8764
Epoch: 0065 loss_train: 0.0822 acc_train: 0.9865 loss_val: 0.4272 acc_val: 0.8801
Epoch: 0066 loss_train: 0.0764 acc_train: 0.9852 loss_val: 0.4338 acc_val: 0.8745
Epoch: 0067 loss_train: 0.0687 acc_train: 0.9871 loss_val: 0.4419 acc_val: 0.8727
Epoch: 0068 loss_train: 0.0618 acc_train: 0.9895 loss_val: 0.4517 acc_val: 0.8708
Epoch: 0069 loss_train: 0.0564 acc_train: 0.9914 loss_val: 0.4612 acc_val: 0.8727
Epoch: 0070 loss_train: 0.0516 acc_train: 0.9908 loss_val: 0.4702 acc_val: 0.8708
Epoch: 0071 loss_train: 0.0470 acc_train: 0.9920 loss_val: 0.4785 acc_val: 0.8690
Epoch: 0072 loss_train: 0.0425 acc_train: 0.9926 loss_val: 0.4848 acc_val: 0.8690
Epoch: 0073 loss_train: 0.0387 acc_train: 0.9945 loss_val: 0.4902 acc_val: 0.8690
Epoch: 0074 loss_train: 0.0355 acc_train: 0.9938 loss_val: 0.4948 acc_val: 0.8672
Epoch: 0075 loss_train: 0.0312 acc_train: 0.9945 loss_val: 0.4993 acc_val: 0.8672
Epoch: 0076 loss_train: 0.0288 acc_train: 0.9963 loss_val: 0.5046 acc_val: 0.8690
Epoch: 0077 loss_train: 0.0262 acc_train: 0.9951 loss_val: 0.5107 acc_val: 0.8690
Epoch: 0078 loss_train: 0.0229 acc_train: 0.9988 loss_val: 0.5159 acc_val: 0.8708
Epoch: 0079 loss_train: 0.0213 acc_train: 0.9982 loss_val: 0.5216 acc_val: 0.8708
Epoch: 0080 loss_train: 0.0198 acc_train: 0.9975 loss_val: 0.5286 acc_val: 0.8708
Epoch: 0081 loss_train: 0.0173 acc_train: 0.9982 loss_val: 0.5363 acc_val: 0.8690
Epoch: 0082 loss_train: 0.0152 acc_train: 0.9994 loss_val: 0.5455 acc_val: 0.8708
Epoch: 0083 loss_train: 0.0139 acc_train: 0.9988 loss_val: 0.5557 acc_val: 0.8727
Epoch: 0084 loss_train: 0.0124 acc_train: 0.9994 loss_val: 0.5655 acc_val: 0.8708
Epoch: 0085 loss_train: 0.0114 acc_train: 0.9994 loss_val: 0.5740 acc_val: 0.8690
Epoch: 0086 loss_train: 0.0101 acc_train: 1.0000 loss_val: 0.5804 acc_val: 0.8690
Epoch: 0087 loss_train: 0.0090 acc_train: 1.0000 loss_val: 0.5846 acc_val: 0.8708
Epoch: 0088 loss_train: 0.0084 acc_train: 1.0000 loss_val: 0.5882 acc_val: 0.8690
Epoch: 0089 loss_train: 0.0082 acc_train: 0.9994 loss_val: 0.5925 acc_val: 0.8708
Epoch: 0090 loss_train: 0.0073 acc_train: 1.0000 loss_val: 0.5984 acc_val: 0.8708
Epoch: 0091 loss_train: 0.0067 acc_train: 1.0000 loss_val: 0.6056 acc_val: 0.8690
Epoch: 0092 loss_train: 0.0060 acc_train: 1.0000 loss_val: 0.6134 acc_val: 0.8690
Epoch: 0093 loss_train: 0.0053 acc_train: 1.0000 loss_val: 0.6213 acc_val: 0.8708
Epoch: 0094 loss_train: 0.0052 acc_train: 1.0000 loss_val: 0.6275 acc_val: 0.8690
Epoch: 0095 loss_train: 0.0047 acc_train: 1.0000 loss_val: 0.6331 acc_val: 0.8672
Epoch: 0096 loss_train: 0.0043 acc_train: 1.0000 loss_val: 0.6373 acc_val: 0.8672
Epoch: 0097 loss_train: 0.0041 acc_train: 1.0000 loss_val: 0.6404 acc_val: 0.8690
Epoch: 0098 loss_train: 0.0040 acc_train: 1.0000 loss_val: 0.6428 acc_val: 0.8672
Epoch: 0099 loss_train: 0.0035 acc_train: 1.0000 loss_val: 0.6451 acc_val: 0.8672
Epoch: 0100 loss_train: 0.0035 acc_train: 1.0000 loss_val: 0.6471 acc_val: 0.8635
Epoch: 0101 loss_train: 0.0032 acc_train: 1.0000 loss_val: 0.6496 acc_val: 0.8635
Epoch: 0102 loss_train: 0.0031 acc_train: 1.0000 loss_val: 0.6526 acc_val: 0.8635
Epoch: 0103 loss_train: 0.0029 acc_train: 1.0000 loss_val: 0.6555 acc_val: 0.8635
Optimization Finished!
Train cost: 13.9824s
Loading 50th epoch
Test set results: loss= 0.3476 accuracy= 0.9019
  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:354: DGLWarning: DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges
  dgl_warning("DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges")
/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:

	DGLGraph.adjacency_matrix(transpose, scipy_fmt="csr").

  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '
/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)
  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1448, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU(approximate='none')
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=7, bias=True)
)
total params: 2979849
Epoch: 0001 loss_train: 1.9680 acc_train: 0.1199 loss_val: 1.9617 acc_val: 0.1347
Epoch: 0002 loss_train: 1.9616 acc_train: 0.1292 loss_val: 1.9496 acc_val: 0.1458
Epoch: 0003 loss_train: 1.9493 acc_train: 0.1433 loss_val: 1.9316 acc_val: 0.1697
Epoch: 0004 loss_train: 1.9332 acc_train: 0.1697 loss_val: 1.9080 acc_val: 0.2269
Epoch: 0005 loss_train: 1.9117 acc_train: 0.2060 loss_val: 1.8788 acc_val: 0.2841
Epoch: 0006 loss_train: 1.8865 acc_train: 0.2854 loss_val: 1.8444 acc_val: 0.3893
Epoch: 0007 loss_train: 1.8539 acc_train: 0.3696 loss_val: 1.8052 acc_val: 0.4686
Epoch: 0008 loss_train: 1.8189 acc_train: 0.4440 loss_val: 1.7618 acc_val: 0.5092
Epoch: 0009 loss_train: 1.7789 acc_train: 0.4963 loss_val: 1.7148 acc_val: 0.5203
Epoch: 0010 loss_train: 1.7348 acc_train: 0.5068 loss_val: 1.6652 acc_val: 0.5277
Epoch: 0011 loss_train: 1.6868 acc_train: 0.5246 loss_val: 1.6132 acc_val: 0.5314
Epoch: 0012 loss_train: 1.6376 acc_train: 0.5301 loss_val: 1.5596 acc_val: 0.5443
Epoch: 0013 loss_train: 1.5869 acc_train: 0.5412 loss_val: 1.5047 acc_val: 0.5646
Epoch: 0014 loss_train: 1.5343 acc_train: 0.5572 loss_val: 1.4487 acc_val: 0.5646
Epoch: 0015 loss_train: 1.4780 acc_train: 0.5640 loss_val: 1.3915 acc_val: 0.5904
Epoch: 0016 loss_train: 1.4233 acc_train: 0.5855 loss_val: 1.3332 acc_val: 0.6199
Epoch: 0017 loss_train: 1.3650 acc_train: 0.6162 loss_val: 1.2746 acc_val: 0.6476
Epoch: 0018 loss_train: 1.3069 acc_train: 0.6470 loss_val: 1.2167 acc_val: 0.6827
Epoch: 0019 loss_train: 1.2481 acc_train: 0.6704 loss_val: 1.1606 acc_val: 0.6863
Epoch: 0020 loss_train: 1.1897 acc_train: 0.6974 loss_val: 1.1077 acc_val: 0.6845
Epoch: 0021 loss_train: 1.1361 acc_train: 0.7054 loss_val: 1.0583 acc_val: 0.6919
Epoch: 0022 loss_train: 1.0813 acc_train: 0.7196 loss_val: 1.0122 acc_val: 0.6974
Epoch: 0023 loss_train: 1.0293 acc_train: 0.7300 loss_val: 0.9689 acc_val: 0.7048
Epoch: 0024 loss_train: 0.9810 acc_train: 0.7380 loss_val: 0.9279 acc_val: 0.7214
Epoch: 0025 loss_train: 0.9358 acc_train: 0.7546 loss_val: 0.8893 acc_val: 0.7325
Epoch: 0026 loss_train: 0.8921 acc_train: 0.7626 loss_val: 0.8532 acc_val: 0.7454
Epoch: 0027 loss_train: 0.8520 acc_train: 0.7718 loss_val: 0.8198 acc_val: 0.7491
Epoch: 0028 loss_train: 0.8141 acc_train: 0.7811 loss_val: 0.7893 acc_val: 0.7601
Epoch: 0029 loss_train: 0.7789 acc_train: 0.7860 loss_val: 0.7609 acc_val: 0.7675
Epoch: 0030 loss_train: 0.7475 acc_train: 0.7891 loss_val: 0.7339 acc_val: 0.7694
Epoch: 0031 loss_train: 0.7160 acc_train: 0.8014 loss_val: 0.7081 acc_val: 0.7731
Epoch: 0032 loss_train: 0.6877 acc_train: 0.8106 loss_val: 0.6832 acc_val: 0.7749
Epoch: 0033 loss_train: 0.6595 acc_train: 0.8143 loss_val: 0.6594 acc_val: 0.7804
Epoch: 0034 loss_train: 0.6319 acc_train: 0.8198 loss_val: 0.6361 acc_val: 0.7952
Epoch: 0035 loss_train: 0.6062 acc_train: 0.8272 loss_val: 0.6129 acc_val: 0.8081
Epoch: 0036 loss_train: 0.5789 acc_train: 0.8303 loss_val: 0.5891 acc_val: 0.8118
Epoch: 0037 loss_train: 0.5552 acc_train: 0.8432 loss_val: 0.5651 acc_val: 0.8155
Epoch: 0038 loss_train: 0.5307 acc_train: 0.8401 loss_val: 0.5414 acc_val: 0.8210
Epoch: 0039 loss_train: 0.5057 acc_train: 0.8506 loss_val: 0.5186 acc_val: 0.8210
Epoch: 0040 loss_train: 0.4834 acc_train: 0.8506 loss_val: 0.4972 acc_val: 0.8247
Epoch: 0041 loss_train: 0.4620 acc_train: 0.8585 loss_val: 0.4777 acc_val: 0.8413
Epoch: 0042 loss_train: 0.4395 acc_train: 0.8641 loss_val: 0.4604 acc_val: 0.8487
Epoch: 0043 loss_train: 0.4170 acc_train: 0.8727 loss_val: 0.4449 acc_val: 0.8524
Epoch: 0044 loss_train: 0.3968 acc_train: 0.8764 loss_val: 0.4292 acc_val: 0.8598
Epoch: 0045 loss_train: 0.3759 acc_train: 0.8905 loss_val: 0.4139 acc_val: 0.8782
Epoch: 0046 loss_train: 0.3544 acc_train: 0.8961 loss_val: 0.4005 acc_val: 0.8819
Epoch: 0047 loss_train: 0.3367 acc_train: 0.9034 loss_val: 0.3897 acc_val: 0.8838
Epoch: 0048 loss_train: 0.3157 acc_train: 0.9102 loss_val: 0.3818 acc_val: 0.8875
Epoch: 0049 loss_train: 0.2973 acc_train: 0.9133 loss_val: 0.3765 acc_val: 0.8838
Epoch: 0050 loss_train: 0.2781 acc_train: 0.9200 loss_val: 0.3724 acc_val: 0.8801
Epoch: 0051 loss_train: 0.2606 acc_train: 0.9287 loss_val: 0.3700 acc_val: 0.8819
Epoch: 0052 loss_train: 0.2442 acc_train: 0.9354 loss_val: 0.3695 acc_val: 0.8838
Epoch: 0053 loss_train: 0.2295 acc_train: 0.9410 loss_val: 0.3718 acc_val: 0.8801
Epoch: 0054 loss_train: 0.2126 acc_train: 0.9490 loss_val: 0.3761 acc_val: 0.8801
Epoch: 0055 loss_train: 0.1939 acc_train: 0.9533 loss_val: 0.3789 acc_val: 0.8838
Epoch: 0056 loss_train: 0.1791 acc_train: 0.9539 loss_val: 0.3789 acc_val: 0.8838
Epoch: 0057 loss_train: 0.1636 acc_train: 0.9606 loss_val: 0.3791 acc_val: 0.8911
Epoch: 0058 loss_train: 0.1526 acc_train: 0.9613 loss_val: 0.3830 acc_val: 0.8911
Epoch: 0059 loss_train: 0.1405 acc_train: 0.9662 loss_val: 0.3920 acc_val: 0.8875
Epoch: 0060 loss_train: 0.1271 acc_train: 0.9699 loss_val: 0.4003 acc_val: 0.8801
Epoch: 0061 loss_train: 0.1149 acc_train: 0.9760 loss_val: 0.4037 acc_val: 0.8801
Epoch: 0062 loss_train: 0.1052 acc_train: 0.9797 loss_val: 0.4056 acc_val: 0.8764
Epoch: 0063 loss_train: 0.0953 acc_train: 0.9834 loss_val: 0.4115 acc_val: 0.8745
Epoch: 0064 loss_train: 0.0854 acc_train: 0.9846 loss_val: 0.4224 acc_val: 0.8782
Epoch: 0065 loss_train: 0.0769 acc_train: 0.9840 loss_val: 0.4361 acc_val: 0.8764
Epoch: 0066 loss_train: 0.0714 acc_train: 0.9859 loss_val: 0.4448 acc_val: 0.8764
Epoch: 0067 loss_train: 0.0639 acc_train: 0.9889 loss_val: 0.4496 acc_val: 0.8764
Epoch: 0068 loss_train: 0.0566 acc_train: 0.9889 loss_val: 0.4558 acc_val: 0.8727
Epoch: 0069 loss_train: 0.0525 acc_train: 0.9914 loss_val: 0.4649 acc_val: 0.8690
Epoch: 0070 loss_train: 0.0472 acc_train: 0.9920 loss_val: 0.4768 acc_val: 0.8708
Epoch: 0071 loss_train: 0.0428 acc_train: 0.9926 loss_val: 0.4881 acc_val: 0.8708
Epoch: 0072 loss_train: 0.0384 acc_train: 0.9945 loss_val: 0.4938 acc_val: 0.8727
Epoch: 0073 loss_train: 0.0351 acc_train: 0.9951 loss_val: 0.4949 acc_val: 0.8708
Epoch: 0074 loss_train: 0.0323 acc_train: 0.9938 loss_val: 0.4962 acc_val: 0.8708
Epoch: 0075 loss_train: 0.0291 acc_train: 0.9957 loss_val: 0.5000 acc_val: 0.8690
Epoch: 0076 loss_train: 0.0268 acc_train: 0.9951 loss_val: 0.5088 acc_val: 0.8690
Epoch: 0077 loss_train: 0.0242 acc_train: 0.9963 loss_val: 0.5207 acc_val: 0.8690
Epoch: 0078 loss_train: 0.0208 acc_train: 0.9982 loss_val: 0.5318 acc_val: 0.8708
Epoch: 0079 loss_train: 0.0194 acc_train: 0.9982 loss_val: 0.5404 acc_val: 0.8708
Epoch: 0080 loss_train: 0.0183 acc_train: 0.9988 loss_val: 0.5461 acc_val: 0.8708
Epoch: 0081 loss_train: 0.0169 acc_train: 0.9988 loss_val: 0.5505 acc_val: 0.8745
Epoch: 0082 loss_train: 0.0152 acc_train: 0.9988 loss_val: 0.5549 acc_val: 0.8745
Epoch: 0083 loss_train: 0.0143 acc_train: 0.9988 loss_val: 0.5609 acc_val: 0.8727
Epoch: 0084 loss_train: 0.0129 acc_train: 0.9988 loss_val: 0.5681 acc_val: 0.8708
Epoch: 0085 loss_train: 0.0116 acc_train: 0.9994 loss_val: 0.5765 acc_val: 0.8690
Epoch: 0086 loss_train: 0.0102 acc_train: 0.9994 loss_val: 0.5853 acc_val: 0.8708
Epoch: 0087 loss_train: 0.0091 acc_train: 0.9994 loss_val: 0.5937 acc_val: 0.8690
Epoch: 0088 loss_train: 0.0083 acc_train: 0.9994 loss_val: 0.6011 acc_val: 0.8690
Epoch: 0089 loss_train: 0.0079 acc_train: 1.0000 loss_val: 0.6077 acc_val: 0.8690
Epoch: 0090 loss_train: 0.0069 acc_train: 1.0000 loss_val: 0.6139 acc_val: 0.8690
Epoch: 0091 loss_train: 0.0067 acc_train: 1.0000 loss_val: 0.6196 acc_val: 0.8690
Epoch: 0092 loss_train: 0.0059 acc_train: 1.0000 loss_val: 0.6249 acc_val: 0.8690
Epoch: 0093 loss_train: 0.0054 acc_train: 1.0000 loss_val: 0.6300 acc_val: 0.8690
Epoch: 0094 loss_train: 0.0053 acc_train: 1.0000 loss_val: 0.6335 acc_val: 0.8690
Epoch: 0095 loss_train: 0.0047 acc_train: 1.0000 loss_val: 0.6375 acc_val: 0.8690
Epoch: 0096 loss_train: 0.0044 acc_train: 1.0000 loss_val: 0.6415 acc_val: 0.8690
Epoch: 0097 loss_train: 0.0042 acc_train: 1.0000 loss_val: 0.6453 acc_val: 0.8708
Epoch: 0098 loss_train: 0.0041 acc_train: 1.0000 loss_val: 0.6494 acc_val: 0.8708
Epoch: 0099 loss_train: 0.0036 acc_train: 1.0000 loss_val: 0.6533 acc_val: 0.8672
Epoch: 0100 loss_train: 0.0036 acc_train: 1.0000 loss_val: 0.6570 acc_val: 0.8672
Epoch: 0101 loss_train: 0.0033 acc_train: 1.0000 loss_val: 0.6602 acc_val: 0.8653
Epoch: 0102 loss_train: 0.0032 acc_train: 1.0000 loss_val: 0.6631 acc_val: 0.8672
Epoch: 0103 loss_train: 0.0030 acc_train: 1.0000 loss_val: 0.6652 acc_val: 0.8672
Epoch: 0104 loss_train: 0.0027 acc_train: 1.0000 loss_val: 0.6664 acc_val: 0.8672
Epoch: 0105 loss_train: 0.0025 acc_train: 1.0000 loss_val: 0.6670 acc_val: 0.8672
Epoch: 0106 loss_train: 0.0025 acc_train: 1.0000 loss_val: 0.6677 acc_val: 0.8653
Epoch: 0107 loss_train: 0.0024 acc_train: 1.0000 loss_val: 0.6682 acc_val: 0.8653
Epoch: 0108 loss_train: 0.0023 acc_train: 1.0000 loss_val: 0.6687 acc_val: 0.8653
Optimization Finished!
Train cost: 15.0464s
Loading 57th epoch
Test set results: loss= 0.3490 accuracy= 0.9000
  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:354: DGLWarning: DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges
  dgl_warning("DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges")
/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:

	DGLGraph.adjacency_matrix(transpose, scipy_fmt="csr").

  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '
/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)
  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1448, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU(approximate='none')
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=7, bias=True)
)
total params: 2979849
Epoch: 0001 loss_train: 1.9677 acc_train: 0.1384 loss_val: 1.9615 acc_val: 0.1421
Epoch: 0002 loss_train: 1.9623 acc_train: 0.1396 loss_val: 1.9501 acc_val: 0.1568
Epoch: 0003 loss_train: 1.9510 acc_train: 0.1451 loss_val: 1.9331 acc_val: 0.1808
Epoch: 0004 loss_train: 1.9359 acc_train: 0.1753 loss_val: 1.9107 acc_val: 0.2214
Epoch: 0005 loss_train: 1.9158 acc_train: 0.2042 loss_val: 1.8832 acc_val: 0.2841
Epoch: 0006 loss_train: 1.8920 acc_train: 0.2786 loss_val: 1.8508 acc_val: 0.3672
Epoch: 0007 loss_train: 1.8619 acc_train: 0.3438 loss_val: 1.8142 acc_val: 0.4649
Epoch: 0008 loss_train: 1.8300 acc_train: 0.4379 loss_val: 1.7741 acc_val: 0.4834
Epoch: 0009 loss_train: 1.7936 acc_train: 0.4643 loss_val: 1.7308 acc_val: 0.4889
Epoch: 0010 loss_train: 1.7540 acc_train: 0.4490 loss_val: 1.6854 acc_val: 0.4834
Epoch: 0011 loss_train: 1.7114 acc_train: 0.4619 loss_val: 1.6383 acc_val: 0.4908
Epoch: 0012 loss_train: 1.6676 acc_train: 0.4594 loss_val: 1.5901 acc_val: 0.4945
Epoch: 0013 loss_train: 1.6227 acc_train: 0.4705 loss_val: 1.5406 acc_val: 0.4982
Epoch: 0014 loss_train: 1.5777 acc_train: 0.4809 loss_val: 1.4899 acc_val: 0.5074
Epoch: 0015 loss_train: 1.5281 acc_train: 0.4902 loss_val: 1.4373 acc_val: 0.5148
Epoch: 0016 loss_train: 1.4778 acc_train: 0.4994 loss_val: 1.3822 acc_val: 0.5351
Epoch: 0017 loss_train: 1.4243 acc_train: 0.5160 loss_val: 1.3247 acc_val: 0.5720
Epoch: 0018 loss_train: 1.3684 acc_train: 0.5498 loss_val: 1.2661 acc_val: 0.6292
Epoch: 0019 loss_train: 1.3127 acc_train: 0.5873 loss_val: 1.2091 acc_val: 0.6734
Epoch: 0020 loss_train: 1.2549 acc_train: 0.6427 loss_val: 1.1554 acc_val: 0.6882
Epoch: 0021 loss_train: 1.2013 acc_train: 0.6759 loss_val: 1.1056 acc_val: 0.6937
Epoch: 0022 loss_train: 1.1481 acc_train: 0.6882 loss_val: 1.0593 acc_val: 0.7085
Epoch: 0023 loss_train: 1.0974 acc_train: 0.7079 loss_val: 1.0153 acc_val: 0.7159
Epoch: 0024 loss_train: 1.0487 acc_train: 0.7208 loss_val: 0.9731 acc_val: 0.7251
Epoch: 0025 loss_train: 1.0025 acc_train: 0.7325 loss_val: 0.9332 acc_val: 0.7306
Epoch: 0026 loss_train: 0.9564 acc_train: 0.7423 loss_val: 0.8963 acc_val: 0.7435
Epoch: 0027 loss_train: 0.9151 acc_train: 0.7558 loss_val: 0.8628 acc_val: 0.7417
Epoch: 0028 loss_train: 0.8774 acc_train: 0.7632 loss_val: 0.8320 acc_val: 0.7454
Epoch: 0029 loss_train: 0.8408 acc_train: 0.7706 loss_val: 0.8028 acc_val: 0.7491
Epoch: 0030 loss_train: 0.8073 acc_train: 0.7811 loss_val: 0.7748 acc_val: 0.7546
Epoch: 0031 loss_train: 0.7709 acc_train: 0.7891 loss_val: 0.7485 acc_val: 0.7638
Epoch: 0032 loss_train: 0.7391 acc_train: 0.7946 loss_val: 0.7239 acc_val: 0.7694
Epoch: 0033 loss_train: 0.7106 acc_train: 0.8032 loss_val: 0.6998 acc_val: 0.7712
Epoch: 0034 loss_train: 0.6795 acc_train: 0.8063 loss_val: 0.6754 acc_val: 0.7897
Epoch: 0035 loss_train: 0.6529 acc_train: 0.8100 loss_val: 0.6512 acc_val: 0.7897
Epoch: 0036 loss_train: 0.6214 acc_train: 0.8173 loss_val: 0.6276 acc_val: 0.7934
Epoch: 0037 loss_train: 0.5955 acc_train: 0.8235 loss_val: 0.6055 acc_val: 0.8063
Epoch: 0038 loss_train: 0.5699 acc_train: 0.8266 loss_val: 0.5848 acc_val: 0.8100
Epoch: 0039 loss_train: 0.5408 acc_train: 0.8321 loss_val: 0.5651 acc_val: 0.8118
Epoch: 0040 loss_train: 0.5149 acc_train: 0.8438 loss_val: 0.5463 acc_val: 0.8229
Epoch: 0041 loss_train: 0.4907 acc_train: 0.8462 loss_val: 0.5281 acc_val: 0.8192
Epoch: 0042 loss_train: 0.4648 acc_train: 0.8549 loss_val: 0.5106 acc_val: 0.8247
Epoch: 0043 loss_train: 0.4405 acc_train: 0.8629 loss_val: 0.4944 acc_val: 0.8303
Epoch: 0044 loss_train: 0.4203 acc_train: 0.8721 loss_val: 0.4798 acc_val: 0.8395
Epoch: 0045 loss_train: 0.3995 acc_train: 0.8819 loss_val: 0.4678 acc_val: 0.8432
Epoch: 0046 loss_train: 0.3764 acc_train: 0.8924 loss_val: 0.4594 acc_val: 0.8413
Epoch: 0047 loss_train: 0.3558 acc_train: 0.8991 loss_val: 0.4531 acc_val: 0.8487
Epoch: 0048 loss_train: 0.3365 acc_train: 0.9059 loss_val: 0.4469 acc_val: 0.8469
Epoch: 0049 loss_train: 0.3161 acc_train: 0.9084 loss_val: 0.4420 acc_val: 0.8561
Epoch: 0050 loss_train: 0.2962 acc_train: 0.9182 loss_val: 0.4399 acc_val: 0.8561
Epoch: 0051 loss_train: 0.2759 acc_train: 0.9219 loss_val: 0.4420 acc_val: 0.8598
Epoch: 0052 loss_train: 0.2582 acc_train: 0.9311 loss_val: 0.4445 acc_val: 0.8598
Epoch: 0053 loss_train: 0.2399 acc_train: 0.9348 loss_val: 0.4455 acc_val: 0.8616
Epoch: 0054 loss_train: 0.2229 acc_train: 0.9403 loss_val: 0.4450 acc_val: 0.8616
Epoch: 0055 loss_train: 0.2028 acc_train: 0.9440 loss_val: 0.4472 acc_val: 0.8635
Epoch: 0056 loss_train: 0.1866 acc_train: 0.9508 loss_val: 0.4516 acc_val: 0.8635
Epoch: 0057 loss_train: 0.1709 acc_train: 0.9557 loss_val: 0.4556 acc_val: 0.8653
Epoch: 0058 loss_train: 0.1599 acc_train: 0.9588 loss_val: 0.4586 acc_val: 0.8635
Epoch: 0059 loss_train: 0.1464 acc_train: 0.9656 loss_val: 0.4645 acc_val: 0.8653
Epoch: 0060 loss_train: 0.1341 acc_train: 0.9686 loss_val: 0.4723 acc_val: 0.8672
Epoch: 0061 loss_train: 0.1205 acc_train: 0.9748 loss_val: 0.4799 acc_val: 0.8708
Epoch: 0062 loss_train: 0.1108 acc_train: 0.9760 loss_val: 0.4849 acc_val: 0.8708
Epoch: 0063 loss_train: 0.0987 acc_train: 0.9815 loss_val: 0.4909 acc_val: 0.8653
Epoch: 0064 loss_train: 0.0877 acc_train: 0.9828 loss_val: 0.5001 acc_val: 0.8616
Epoch: 0065 loss_train: 0.0791 acc_train: 0.9877 loss_val: 0.5130 acc_val: 0.8635
Epoch: 0066 loss_train: 0.0715 acc_train: 0.9877 loss_val: 0.5219 acc_val: 0.8635
Epoch: 0067 loss_train: 0.0636 acc_train: 0.9895 loss_val: 0.5262 acc_val: 0.8616
Epoch: 0068 loss_train: 0.0569 acc_train: 0.9926 loss_val: 0.5305 acc_val: 0.8579
Epoch: 0069 loss_train: 0.0516 acc_train: 0.9920 loss_val: 0.5382 acc_val: 0.8542
Epoch: 0070 loss_train: 0.0451 acc_train: 0.9938 loss_val: 0.5499 acc_val: 0.8579
Epoch: 0071 loss_train: 0.0399 acc_train: 0.9945 loss_val: 0.5596 acc_val: 0.8598
Epoch: 0072 loss_train: 0.0364 acc_train: 0.9951 loss_val: 0.5659 acc_val: 0.8598
Epoch: 0073 loss_train: 0.0319 acc_train: 0.9963 loss_val: 0.5695 acc_val: 0.8598
Epoch: 0074 loss_train: 0.0283 acc_train: 0.9963 loss_val: 0.5746 acc_val: 0.8653
Epoch: 0075 loss_train: 0.0248 acc_train: 0.9969 loss_val: 0.5824 acc_val: 0.8635
Epoch: 0076 loss_train: 0.0224 acc_train: 0.9975 loss_val: 0.5939 acc_val: 0.8653
Epoch: 0077 loss_train: 0.0188 acc_train: 0.9994 loss_val: 0.6078 acc_val: 0.8635
Epoch: 0078 loss_train: 0.0163 acc_train: 1.0000 loss_val: 0.6191 acc_val: 0.8616
Epoch: 0079 loss_train: 0.0145 acc_train: 1.0000 loss_val: 0.6285 acc_val: 0.8598
Epoch: 0080 loss_train: 0.0139 acc_train: 1.0000 loss_val: 0.6352 acc_val: 0.8598
Epoch: 0081 loss_train: 0.0120 acc_train: 1.0000 loss_val: 0.6420 acc_val: 0.8561
Epoch: 0082 loss_train: 0.0110 acc_train: 1.0000 loss_val: 0.6501 acc_val: 0.8542
Epoch: 0083 loss_train: 0.0102 acc_train: 1.0000 loss_val: 0.6601 acc_val: 0.8579
Epoch: 0084 loss_train: 0.0093 acc_train: 1.0000 loss_val: 0.6700 acc_val: 0.8561
Epoch: 0085 loss_train: 0.0084 acc_train: 1.0000 loss_val: 0.6776 acc_val: 0.8542
Epoch: 0086 loss_train: 0.0077 acc_train: 1.0000 loss_val: 0.6836 acc_val: 0.8542
Epoch: 0087 loss_train: 0.0070 acc_train: 1.0000 loss_val: 0.6876 acc_val: 0.8561
Epoch: 0088 loss_train: 0.0063 acc_train: 1.0000 loss_val: 0.6910 acc_val: 0.8561
Epoch: 0089 loss_train: 0.0059 acc_train: 1.0000 loss_val: 0.6944 acc_val: 0.8542
Epoch: 0090 loss_train: 0.0055 acc_train: 1.0000 loss_val: 0.6999 acc_val: 0.8598
Epoch: 0091 loss_train: 0.0049 acc_train: 1.0000 loss_val: 0.7067 acc_val: 0.8598
Epoch: 0092 loss_train: 0.0046 acc_train: 1.0000 loss_val: 0.7139 acc_val: 0.8598
Epoch: 0093 loss_train: 0.0042 acc_train: 1.0000 loss_val: 0.7208 acc_val: 0.8598
Epoch: 0094 loss_train: 0.0041 acc_train: 1.0000 loss_val: 0.7261 acc_val: 0.8598
Epoch: 0095 loss_train: 0.0037 acc_train: 1.0000 loss_val: 0.7296 acc_val: 0.8579
Epoch: 0096 loss_train: 0.0034 acc_train: 1.0000 loss_val: 0.7318 acc_val: 0.8579
Epoch: 0097 loss_train: 0.0033 acc_train: 1.0000 loss_val: 0.7331 acc_val: 0.8561
Epoch: 0098 loss_train: 0.0031 acc_train: 1.0000 loss_val: 0.7340 acc_val: 0.8561
Epoch: 0099 loss_train: 0.0028 acc_train: 1.0000 loss_val: 0.7356 acc_val: 0.8561
Epoch: 0100 loss_train: 0.0028 acc_train: 1.0000 loss_val: 0.7377 acc_val: 0.8561
Epoch: 0101 loss_train: 0.0025 acc_train: 1.0000 loss_val: 0.7408 acc_val: 0.8561
Epoch: 0102 loss_train: 0.0025 acc_train: 1.0000 loss_val: 0.7450 acc_val: 0.8561
Epoch: 0103 loss_train: 0.0023 acc_train: 1.0000 loss_val: 0.7498 acc_val: 0.8561
Epoch: 0104 loss_train: 0.0022 acc_train: 1.0000 loss_val: 0.7547 acc_val: 0.8561
Epoch: 0105 loss_train: 0.0021 acc_train: 1.0000 loss_val: 0.7594 acc_val: 0.8561
Epoch: 0106 loss_train: 0.0020 acc_train: 1.0000 loss_val: 0.7637 acc_val: 0.8542
Epoch: 0107 loss_train: 0.0020 acc_train: 1.0000 loss_val: 0.7675 acc_val: 0.8542
Epoch: 0108 loss_train: 0.0019 acc_train: 1.0000 loss_val: 0.7705 acc_val: 0.8561
Epoch: 0109 loss_train: 0.0018 acc_train: 1.0000 loss_val: 0.7727 acc_val: 0.8561
Epoch: 0110 loss_train: 0.0018 acc_train: 1.0000 loss_val: 0.7740 acc_val: 0.8561
Epoch: 0111 loss_train: 0.0017 acc_train: 1.0000 loss_val: 0.7748 acc_val: 0.8579
Epoch: 0112 loss_train: 0.0016 acc_train: 1.0000 loss_val: 0.7754 acc_val: 0.8579
Optimization Finished!
Train cost: 15.5170s
Loading 61th epoch
Test set results: loss= 0.3972 accuracy= 0.8833
  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:354: DGLWarning: DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges
  dgl_warning("DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges")
/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:

	DGLGraph.adjacency_matrix(transpose, scipy_fmt="csr").

  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '
/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)
  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1448, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU(approximate='none')
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=7, bias=True)
)
total params: 2979849
Epoch: 0001 loss_train: 1.9815 acc_train: 0.1187 loss_val: 1.9746 acc_val: 0.1218
Epoch: 0002 loss_train: 1.9777 acc_train: 0.1181 loss_val: 1.9630 acc_val: 0.1421
Epoch: 0003 loss_train: 1.9653 acc_train: 0.1267 loss_val: 1.9458 acc_val: 0.1513
Epoch: 0004 loss_train: 1.9507 acc_train: 0.1365 loss_val: 1.9233 acc_val: 0.1919
Epoch: 0005 loss_train: 1.9308 acc_train: 0.1562 loss_val: 1.8959 acc_val: 0.2380
Epoch: 0006 loss_train: 1.9077 acc_train: 0.2085 loss_val: 1.8639 acc_val: 0.3155
Epoch: 0007 loss_train: 1.8783 acc_train: 0.2854 loss_val: 1.8280 acc_val: 0.3893
Epoch: 0008 loss_train: 1.8461 acc_train: 0.3622 loss_val: 1.7886 acc_val: 0.4483
Epoch: 0009 loss_train: 1.8100 acc_train: 0.4096 loss_val: 1.7467 acc_val: 0.4502
Epoch: 0010 loss_train: 1.7718 acc_train: 0.4188 loss_val: 1.7026 acc_val: 0.4428
Epoch: 0011 loss_train: 1.7304 acc_train: 0.4360 loss_val: 1.6572 acc_val: 0.4483
Epoch: 0012 loss_train: 1.6887 acc_train: 0.4397 loss_val: 1.6109 acc_val: 0.4649
Epoch: 0013 loss_train: 1.6459 acc_train: 0.4483 loss_val: 1.5638 acc_val: 0.4760
Epoch: 0014 loss_train: 1.6026 acc_train: 0.4625 loss_val: 1.5151 acc_val: 0.4945
Epoch: 0015 loss_train: 1.5561 acc_train: 0.4692 loss_val: 1.4642 acc_val: 0.5037
Epoch: 0016 loss_train: 1.5082 acc_train: 0.4852 loss_val: 1.4105 acc_val: 0.5203
Epoch: 0017 loss_train: 1.4558 acc_train: 0.5031 loss_val: 1.3540 acc_val: 0.5498
Epoch: 0018 loss_train: 1.4017 acc_train: 0.5332 loss_val: 1.2962 acc_val: 0.6052
Epoch: 0019 loss_train: 1.3457 acc_train: 0.5683 loss_val: 1.2393 acc_val: 0.6587
Epoch: 0020 loss_train: 1.2891 acc_train: 0.6218 loss_val: 1.1853 acc_val: 0.6697
Epoch: 0021 loss_train: 1.2366 acc_train: 0.6544 loss_val: 1.1346 acc_val: 0.6679
Epoch: 0022 loss_train: 1.1818 acc_train: 0.6685 loss_val: 1.0869 acc_val: 0.6808
Epoch: 0023 loss_train: 1.1295 acc_train: 0.6808 loss_val: 1.0417 acc_val: 0.6919
Epoch: 0024 loss_train: 1.0816 acc_train: 0.6913 loss_val: 0.9982 acc_val: 0.7011
Epoch: 0025 loss_train: 1.0346 acc_train: 0.7054 loss_val: 0.9577 acc_val: 0.7103
Epoch: 0026 loss_train: 0.9893 acc_train: 0.7134 loss_val: 0.9213 acc_val: 0.7196
Epoch: 0027 loss_train: 0.9470 acc_train: 0.7202 loss_val: 0.8887 acc_val: 0.7325
Epoch: 0028 loss_train: 0.9119 acc_train: 0.7355 loss_val: 0.8591 acc_val: 0.7380
Epoch: 0029 loss_train: 0.8769 acc_train: 0.7435 loss_val: 0.8310 acc_val: 0.7528
Epoch: 0030 loss_train: 0.8451 acc_train: 0.7503 loss_val: 0.8043 acc_val: 0.7565
Epoch: 0031 loss_train: 0.8105 acc_train: 0.7626 loss_val: 0.7792 acc_val: 0.7583
Epoch: 0032 loss_train: 0.7827 acc_train: 0.7718 loss_val: 0.7556 acc_val: 0.7657
Epoch: 0033 loss_train: 0.7522 acc_train: 0.7780 loss_val: 0.7326 acc_val: 0.7694
Epoch: 0034 loss_train: 0.7246 acc_train: 0.7798 loss_val: 0.7096 acc_val: 0.7657
Epoch: 0035 loss_train: 0.6980 acc_train: 0.7872 loss_val: 0.6870 acc_val: 0.7675
Epoch: 0036 loss_train: 0.6675 acc_train: 0.7927 loss_val: 0.6656 acc_val: 0.7712
Epoch: 0037 loss_train: 0.6414 acc_train: 0.7989 loss_val: 0.6453 acc_val: 0.7749
Epoch: 0038 loss_train: 0.6156 acc_train: 0.8032 loss_val: 0.6257 acc_val: 0.7804
Epoch: 0039 loss_train: 0.5870 acc_train: 0.8137 loss_val: 0.6064 acc_val: 0.7915
Epoch: 0040 loss_train: 0.5608 acc_train: 0.8216 loss_val: 0.5874 acc_val: 0.8026
Epoch: 0041 loss_train: 0.5338 acc_train: 0.8339 loss_val: 0.5686 acc_val: 0.8007
Epoch: 0042 loss_train: 0.5080 acc_train: 0.8383 loss_val: 0.5502 acc_val: 0.8081
Epoch: 0043 loss_train: 0.4827 acc_train: 0.8481 loss_val: 0.5329 acc_val: 0.8118
Epoch: 0044 loss_train: 0.4604 acc_train: 0.8573 loss_val: 0.5174 acc_val: 0.8192
Epoch: 0045 loss_train: 0.4356 acc_train: 0.8702 loss_val: 0.5049 acc_val: 0.8247
Epoch: 0046 loss_train: 0.4115 acc_train: 0.8807 loss_val: 0.4953 acc_val: 0.8321
Epoch: 0047 loss_train: 0.3886 acc_train: 0.8850 loss_val: 0.4870 acc_val: 0.8376
Epoch: 0048 loss_train: 0.3648 acc_train: 0.8985 loss_val: 0.4786 acc_val: 0.8413
Epoch: 0049 loss_train: 0.3423 acc_train: 0.9096 loss_val: 0.4715 acc_val: 0.8450
Epoch: 0050 loss_train: 0.3209 acc_train: 0.9133 loss_val: 0.4673 acc_val: 0.8469
Epoch: 0051 loss_train: 0.2981 acc_train: 0.9188 loss_val: 0.4674 acc_val: 0.8469
Epoch: 0052 loss_train: 0.2776 acc_train: 0.9262 loss_val: 0.4687 acc_val: 0.8469
Epoch: 0053 loss_train: 0.2559 acc_train: 0.9305 loss_val: 0.4690 acc_val: 0.8450
Epoch: 0054 loss_train: 0.2363 acc_train: 0.9385 loss_val: 0.4688 acc_val: 0.8450
Epoch: 0055 loss_train: 0.2140 acc_train: 0.9422 loss_val: 0.4720 acc_val: 0.8450
Epoch: 0056 loss_train: 0.1967 acc_train: 0.9496 loss_val: 0.4779 acc_val: 0.8524
Epoch: 0057 loss_train: 0.1783 acc_train: 0.9576 loss_val: 0.4828 acc_val: 0.8561
Epoch: 0058 loss_train: 0.1659 acc_train: 0.9613 loss_val: 0.4854 acc_val: 0.8542
Epoch: 0059 loss_train: 0.1511 acc_train: 0.9649 loss_val: 0.4915 acc_val: 0.8469
Epoch: 0060 loss_train: 0.1355 acc_train: 0.9699 loss_val: 0.5016 acc_val: 0.8450
Epoch: 0061 loss_train: 0.1226 acc_train: 0.9742 loss_val: 0.5119 acc_val: 0.8506
Epoch: 0062 loss_train: 0.1114 acc_train: 0.9754 loss_val: 0.5183 acc_val: 0.8450
Epoch: 0063 loss_train: 0.0992 acc_train: 0.9797 loss_val: 0.5253 acc_val: 0.8524
Epoch: 0064 loss_train: 0.0870 acc_train: 0.9815 loss_val: 0.5349 acc_val: 0.8524
Epoch: 0065 loss_train: 0.0781 acc_train: 0.9852 loss_val: 0.5477 acc_val: 0.8542
Epoch: 0066 loss_train: 0.0702 acc_train: 0.9877 loss_val: 0.5581 acc_val: 0.8469
Epoch: 0067 loss_train: 0.0624 acc_train: 0.9914 loss_val: 0.5638 acc_val: 0.8413
Epoch: 0068 loss_train: 0.0559 acc_train: 0.9932 loss_val: 0.5694 acc_val: 0.8395
Epoch: 0069 loss_train: 0.0499 acc_train: 0.9932 loss_val: 0.5789 acc_val: 0.8376
Epoch: 0070 loss_train: 0.0441 acc_train: 0.9938 loss_val: 0.5913 acc_val: 0.8358
Epoch: 0071 loss_train: 0.0398 acc_train: 0.9951 loss_val: 0.6014 acc_val: 0.8376
Epoch: 0072 loss_train: 0.0358 acc_train: 0.9963 loss_val: 0.6084 acc_val: 0.8395
Epoch: 0073 loss_train: 0.0315 acc_train: 0.9969 loss_val: 0.6136 acc_val: 0.8376
Epoch: 0074 loss_train: 0.0281 acc_train: 0.9963 loss_val: 0.6206 acc_val: 0.8358
Epoch: 0075 loss_train: 0.0257 acc_train: 0.9975 loss_val: 0.6301 acc_val: 0.8376
Epoch: 0076 loss_train: 0.0238 acc_train: 0.9969 loss_val: 0.6430 acc_val: 0.8358
Epoch: 0077 loss_train: 0.0210 acc_train: 0.9988 loss_val: 0.6566 acc_val: 0.8321
Epoch: 0078 loss_train: 0.0188 acc_train: 0.9994 loss_val: 0.6681 acc_val: 0.8303
Epoch: 0079 loss_train: 0.0172 acc_train: 0.9988 loss_val: 0.6771 acc_val: 0.8339
Epoch: 0080 loss_train: 0.0166 acc_train: 0.9988 loss_val: 0.6845 acc_val: 0.8321
Epoch: 0081 loss_train: 0.0148 acc_train: 0.9988 loss_val: 0.6923 acc_val: 0.8321
Epoch: 0082 loss_train: 0.0136 acc_train: 0.9988 loss_val: 0.7015 acc_val: 0.8321
Epoch: 0083 loss_train: 0.0129 acc_train: 0.9988 loss_val: 0.7132 acc_val: 0.8303
Epoch: 0084 loss_train: 0.0117 acc_train: 0.9988 loss_val: 0.7242 acc_val: 0.8284
Epoch: 0085 loss_train: 0.0104 acc_train: 0.9994 loss_val: 0.7336 acc_val: 0.8284
Epoch: 0086 loss_train: 0.0090 acc_train: 0.9994 loss_val: 0.7414 acc_val: 0.8266
Epoch: 0087 loss_train: 0.0083 acc_train: 0.9994 loss_val: 0.7475 acc_val: 0.8266
Epoch: 0088 loss_train: 0.0072 acc_train: 0.9994 loss_val: 0.7532 acc_val: 0.8266
Epoch: 0089 loss_train: 0.0063 acc_train: 1.0000 loss_val: 0.7590 acc_val: 0.8284
Epoch: 0090 loss_train: 0.0060 acc_train: 1.0000 loss_val: 0.7661 acc_val: 0.8284
Epoch: 0091 loss_train: 0.0053 acc_train: 1.0000 loss_val: 0.7747 acc_val: 0.8247
Epoch: 0092 loss_train: 0.0050 acc_train: 1.0000 loss_val: 0.7844 acc_val: 0.8247
Epoch: 0093 loss_train: 0.0045 acc_train: 1.0000 loss_val: 0.7934 acc_val: 0.8247
Epoch: 0094 loss_train: 0.0045 acc_train: 1.0000 loss_val: 0.7994 acc_val: 0.8247
Epoch: 0095 loss_train: 0.0041 acc_train: 1.0000 loss_val: 0.8031 acc_val: 0.8229
Epoch: 0096 loss_train: 0.0038 acc_train: 1.0000 loss_val: 0.8052 acc_val: 0.8247
Epoch: 0097 loss_train: 0.0035 acc_train: 1.0000 loss_val: 0.8066 acc_val: 0.8247
Epoch: 0098 loss_train: 0.0033 acc_train: 1.0000 loss_val: 0.8080 acc_val: 0.8247
Epoch: 0099 loss_train: 0.0031 acc_train: 1.0000 loss_val: 0.8099 acc_val: 0.8247
Epoch: 0100 loss_train: 0.0031 acc_train: 1.0000 loss_val: 0.8129 acc_val: 0.8247
Epoch: 0101 loss_train: 0.0028 acc_train: 1.0000 loss_val: 0.8166 acc_val: 0.8247
Epoch: 0102 loss_train: 0.0027 acc_train: 1.0000 loss_val: 0.8204 acc_val: 0.8247
Epoch: 0103 loss_train: 0.0027 acc_train: 1.0000 loss_val: 0.8242 acc_val: 0.8247
Epoch: 0104 loss_train: 0.0026 acc_train: 1.0000 loss_val: 0.8267 acc_val: 0.8247
Epoch: 0105 loss_train: 0.0022 acc_train: 1.0000 loss_val: 0.8292 acc_val: 0.8266
Epoch: 0106 loss_train: 0.0021 acc_train: 1.0000 loss_val: 0.8322 acc_val: 0.8247
Epoch: 0107 loss_train: 0.0023 acc_train: 1.0000 loss_val: 0.8362 acc_val: 0.8247
Optimization Finished!
Train cost: 14.4119s
Loading 57th epoch
Test set results: loss= 0.4142 accuracy= 0.8722