  NumNodes: 19717
  NumEdges: 88651
  NumFeats: 500
  NumClasses: 3
  NumTrainingSamples: 60
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:

	DGLGraph.adjacency_matrix(transpose, scipy_fmt="csr").

  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '
/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)
  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()
TransformerModel(
  (att_embeddings_nope): Linear(in_features=515, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU(approximate='none')
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=3, bias=True)
)
total params: 2501125
Epoch: 0001 loss_train: 6.5268 acc_train: 0.3937 loss_val: 2.0050 acc_val: 0.0621
Epoch: 0002 loss_train: 5.6107 acc_train: 0.6451 loss_val: 1.6404 acc_val: 0.6889
Epoch: 0003 loss_train: 4.5798 acc_train: 0.7072 loss_val: 1.2995 acc_val: 0.7518
Epoch: 0004 loss_train: 3.6574 acc_train: 0.7731 loss_val: 1.0907 acc_val: 0.7875
Epoch: 0005 loss_train: 3.1752 acc_train: 0.7946 loss_val: 1.0121 acc_val: 0.8055
Epoch: 0006 loss_train: 2.9188 acc_train: 0.8090 loss_val: 0.9214 acc_val: 0.8230
Epoch: 0007 loss_train: 2.6477 acc_train: 0.8278 loss_val: 0.8484 acc_val: 0.8347
Epoch: 0008 loss_train: 2.4033 acc_train: 0.8418 loss_val: 0.7762 acc_val: 0.8537
Epoch: 0009 loss_train: 2.1739 acc_train: 0.8575 loss_val: 0.7444 acc_val: 0.8631
Epoch: 0010 loss_train: 2.0221 acc_train: 0.8680 loss_val: 0.7093 acc_val: 0.8671
Epoch: 0011 loss_train: 1.8280 acc_train: 0.8831 loss_val: 0.6645 acc_val: 0.8765
Epoch: 0012 loss_train: 1.7502 acc_train: 0.8883 loss_val: 0.6576 acc_val: 0.8793
Epoch: 0013 loss_train: 1.5969 acc_train: 0.8994 loss_val: 0.6499 acc_val: 0.8801
Epoch: 0014 loss_train: 1.5122 acc_train: 0.9037 loss_val: 0.6885 acc_val: 0.8760
Epoch: 0015 loss_train: 1.4774 acc_train: 0.9064 loss_val: 0.6343 acc_val: 0.8867
Epoch: 0016 loss_train: 1.3783 acc_train: 0.9145 loss_val: 0.6252 acc_val: 0.8834
Epoch: 0017 loss_train: 1.2856 acc_train: 0.9230 loss_val: 0.6251 acc_val: 0.8897
Epoch: 0018 loss_train: 1.1536 acc_train: 0.9307 loss_val: 0.6529 acc_val: 0.8854
Epoch: 0019 loss_train: 1.0949 acc_train: 0.9336 loss_val: 0.6890 acc_val: 0.8851
Epoch: 0020 loss_train: 1.0392 acc_train: 0.9375 loss_val: 0.6600 acc_val: 0.8887
Epoch: 0021 loss_train: 0.9177 acc_train: 0.9467 loss_val: 0.6957 acc_val: 0.8887
Epoch: 0022 loss_train: 0.8809 acc_train: 0.9458 loss_val: 0.7379 acc_val: 0.8803
Epoch: 0023 loss_train: 0.8144 acc_train: 0.9530 loss_val: 0.7723 acc_val: 0.8826
Epoch: 0024 loss_train: 0.7890 acc_train: 0.9523 loss_val: 0.8411 acc_val: 0.8747
Epoch: 0025 loss_train: 0.7780 acc_train: 0.9509 loss_val: 0.8303 acc_val: 0.8783
Epoch: 0026 loss_train: 0.7116 acc_train: 0.9564 loss_val: 0.8051 acc_val: 0.8839
Epoch: 0027 loss_train: 0.5610 acc_train: 0.9684 loss_val: 0.8101 acc_val: 0.8869
Epoch: 0028 loss_train: 0.4670 acc_train: 0.9745 loss_val: 0.9474 acc_val: 0.8780
Epoch: 0029 loss_train: 0.4295 acc_train: 0.9751 loss_val: 0.9985 acc_val: 0.8760
Epoch: 0030 loss_train: 0.3867 acc_train: 0.9766 loss_val: 1.0297 acc_val: 0.8813
Epoch: 0031 loss_train: 0.3107 acc_train: 0.9823 loss_val: 1.0278 acc_val: 0.8841
Epoch: 0032 loss_train: 0.2900 acc_train: 0.9840 loss_val: 1.1981 acc_val: 0.8763
Epoch: 0033 loss_train: 0.7144 acc_train: 0.9576 loss_val: 1.1541 acc_val: 0.8605
Epoch: 0034 loss_train: 0.6196 acc_train: 0.9594 loss_val: 1.0990 acc_val: 0.8826
Epoch: 0035 loss_train: 0.6583 acc_train: 0.9623 loss_val: 1.1192 acc_val: 0.8770
Epoch: 0036 loss_train: 0.5769 acc_train: 0.9657 loss_val: 0.8821 acc_val: 0.8778
Epoch: 0037 loss_train: 0.3560 acc_train: 0.9807 loss_val: 0.9838 acc_val: 0.8816
Epoch: 0038 loss_train: 0.2502 acc_train: 0.9858 loss_val: 1.0691 acc_val: 0.8785
Epoch: 0039 loss_train: 0.1834 acc_train: 0.9899 loss_val: 1.1508 acc_val: 0.8803
Epoch: 0040 loss_train: 0.1176 acc_train: 0.9932 loss_val: 1.1723 acc_val: 0.8770
Epoch: 0041 loss_train: 0.0921 acc_train: 0.9954 loss_val: 1.2731 acc_val: 0.8791
Epoch: 0042 loss_train: 0.0806 acc_train: 0.9963 loss_val: 1.3150 acc_val: 0.8824
Epoch: 0043 loss_train: 0.0630 acc_train: 0.9975 loss_val: 1.3461 acc_val: 0.8851
Epoch: 0044 loss_train: 0.0423 acc_train: 0.9982 loss_val: 1.4321 acc_val: 0.8834
Epoch: 0045 loss_train: 0.0405 acc_train: 0.9981 loss_val: 1.4822 acc_val: 0.8816
Epoch: 0046 loss_train: 0.0403 acc_train: 0.9978 loss_val: 1.4735 acc_val: 0.8803
Epoch: 0047 loss_train: 0.0452 acc_train: 0.9975 loss_val: 1.5169 acc_val: 0.8818
Epoch: 0048 loss_train: 0.0573 acc_train: 0.9966 loss_val: 1.5403 acc_val: 0.8811
Epoch: 0049 loss_train: 0.0507 acc_train: 0.9970 loss_val: 1.6027 acc_val: 0.8780
Epoch: 0050 loss_train: 0.0522 acc_train: 0.9970 loss_val: 1.5824 acc_val: 0.8791
Epoch: 0051 loss_train: 0.0622 acc_train: 0.9969 loss_val: 1.6464 acc_val: 0.8753
Epoch: 0052 loss_train: 0.0543 acc_train: 0.9969 loss_val: 1.6753 acc_val: 0.8742
Epoch: 0053 loss_train: 0.0756 acc_train: 0.9961 loss_val: 1.7133 acc_val: 0.8735
Epoch: 0054 loss_train: 0.0723 acc_train: 0.9958 loss_val: 1.7299 acc_val: 0.8770
Epoch: 0055 loss_train: 0.1142 acc_train: 0.9930 loss_val: 2.0479 acc_val: 0.8514
Epoch: 0056 loss_train: 5.6391 acc_train: 0.7626 loss_val: 2.3017 acc_val: 0.5294
Epoch: 0057 loss_train: 4.7769 acc_train: 0.6653 loss_val: 1.0550 acc_val: 0.7918
Epoch: 0058 loss_train: 2.7487 acc_train: 0.8244 loss_val: 0.8637 acc_val: 0.8347
Epoch: 0059 loss_train: 3.2688 acc_train: 0.7867 loss_val: 0.9494 acc_val: 0.8012
Epoch: 0060 loss_train: 2.5691 acc_train: 0.8255 loss_val: 0.9026 acc_val: 0.8131
Epoch: 0061 loss_train: 2.3066 acc_train: 0.8536 loss_val: 0.7375 acc_val: 0.8598
Epoch: 0062 loss_train: 2.0242 acc_train: 0.8661 loss_val: 0.7294 acc_val: 0.8646
Epoch: 0063 loss_train: 1.9044 acc_train: 0.8779 loss_val: 0.6774 acc_val: 0.8687
Epoch: 0064 loss_train: 1.7796 acc_train: 0.8870 loss_val: 0.6624 acc_val: 0.8709
Epoch: 0065 loss_train: 1.6959 acc_train: 0.8922 loss_val: 0.6309 acc_val: 0.8791
Epoch: 0066 loss_train: 1.6078 acc_train: 0.8992 loss_val: 0.6236 acc_val: 0.8806
Epoch: 0067 loss_train: 1.5173 acc_train: 0.9060 loss_val: 0.6224 acc_val: 0.8806
Epoch: 0068 loss_train: 1.4677 acc_train: 0.9092 loss_val: 0.6067 acc_val: 0.8851
Epoch: 0069 loss_train: 1.3689 acc_train: 0.9154 loss_val: 0.6076 acc_val: 0.8867
Epoch: 0070 loss_train: 1.3036 acc_train: 0.9176 loss_val: 0.6205 acc_val: 0.8851
Epoch: 0071 loss_train: 1.2713 acc_train: 0.9203 loss_val: 0.6351 acc_val: 0.8905
Epoch: 0072 loss_train: 1.2381 acc_train: 0.9236 loss_val: 0.6851 acc_val: 0.8750
Epoch: 0073 loss_train: 1.2760 acc_train: 0.9182 loss_val: 0.6240 acc_val: 0.8872
Epoch: 0074 loss_train: 1.2092 acc_train: 0.9250 loss_val: 0.6453 acc_val: 0.8869
Epoch: 0075 loss_train: 1.1753 acc_train: 0.9275 loss_val: 0.6733 acc_val: 0.8796
Epoch: 0076 loss_train: 1.0985 acc_train: 0.9304 loss_val: 0.6581 acc_val: 0.8889
Epoch: 0077 loss_train: 1.0217 acc_train: 0.9390 loss_val: 0.6515 acc_val: 0.8846
Epoch: 0078 loss_train: 0.9535 acc_train: 0.9441 loss_val: 0.6804 acc_val: 0.8915
Epoch: 0079 loss_train: 0.9203 acc_train: 0.9451 loss_val: 0.7042 acc_val: 0.8895
Epoch: 0080 loss_train: 0.8812 acc_train: 0.9471 loss_val: 0.7121 acc_val: 0.8884
Epoch: 0081 loss_train: 0.8589 acc_train: 0.9501 loss_val: 0.7313 acc_val: 0.8859
Epoch: 0082 loss_train: 0.7780 acc_train: 0.9577 loss_val: 0.7680 acc_val: 0.8889
Epoch: 0083 loss_train: 0.7578 acc_train: 0.9568 loss_val: 0.7519 acc_val: 0.8864
Epoch: 0084 loss_train: 0.7270 acc_train: 0.9597 loss_val: 0.8329 acc_val: 0.8796
Epoch: 0085 loss_train: 0.9022 acc_train: 0.9493 loss_val: 0.8528 acc_val: 0.8697
Epoch: 0086 loss_train: 1.0906 acc_train: 0.9307 loss_val: 0.6755 acc_val: 0.8877
Epoch: 0087 loss_train: 0.9863 acc_train: 0.9408 loss_val: 0.6651 acc_val: 0.8912
Epoch: 0088 loss_train: 0.8798 acc_train: 0.9475 loss_val: 0.6891 acc_val: 0.8859
Epoch: 0089 loss_train: 0.7700 acc_train: 0.9553 loss_val: 0.6914 acc_val: 0.8887
Epoch: 0090 loss_train: 0.7212 acc_train: 0.9589 loss_val: 0.7566 acc_val: 0.8920
Epoch: 0091 loss_train: 0.6481 acc_train: 0.9633 loss_val: 0.7884 acc_val: 0.8846
Epoch: 0092 loss_train: 0.6038 acc_train: 0.9670 loss_val: 0.8251 acc_val: 0.8839
Epoch: 0093 loss_train: 0.5660 acc_train: 0.9691 loss_val: 0.8529 acc_val: 0.8851
Epoch: 0094 loss_train: 0.5659 acc_train: 0.9702 loss_val: 0.8750 acc_val: 0.8829
Epoch: 0095 loss_train: 0.5375 acc_train: 0.9702 loss_val: 0.8799 acc_val: 0.8859
Epoch: 0096 loss_train: 0.5349 acc_train: 0.9714 loss_val: 0.8961 acc_val: 0.8874
Epoch: 0097 loss_train: 0.4617 acc_train: 0.9767 loss_val: 0.9232 acc_val: 0.8854
Epoch: 0098 loss_train: 0.4605 acc_train: 0.9757 loss_val: 0.9660 acc_val: 0.8791
Epoch: 0099 loss_train: 0.4736 acc_train: 0.9759 loss_val: 0.9793 acc_val: 0.8829
Epoch: 0100 loss_train: 0.4557 acc_train: 0.9761 loss_val: 0.9576 acc_val: 0.8826
Epoch: 0101 loss_train: 0.4262 acc_train: 0.9791 loss_val: 0.9954 acc_val: 0.8841
Epoch: 0102 loss_train: 0.4439 acc_train: 0.9765 loss_val: 1.0609 acc_val: 0.8798
Epoch: 0103 loss_train: 0.5062 acc_train: 0.9711 loss_val: 0.9898 acc_val: 0.8834
Epoch: 0104 loss_train: 0.4448 acc_train: 0.9768 loss_val: 0.9441 acc_val: 0.8887
Epoch: 0105 loss_train: 0.4457 acc_train: 0.9773 loss_val: 0.9701 acc_val: 0.8801
Epoch: 0106 loss_train: 0.4389 acc_train: 0.9767 loss_val: 1.0631 acc_val: 0.8763
Epoch: 0107 loss_train: 0.4121 acc_train: 0.9794 loss_val: 0.9949 acc_val: 0.8793
Epoch: 0108 loss_train: 0.3538 acc_train: 0.9837 loss_val: 1.0254 acc_val: 0.8824
Epoch: 0109 loss_train: 0.3436 acc_train: 0.9836 loss_val: 1.0270 acc_val: 0.8839
Epoch: 0110 loss_train: 0.3398 acc_train: 0.9838 loss_val: 1.0582 acc_val: 0.8829
Epoch: 0111 loss_train: 0.3228 acc_train: 0.9840 loss_val: 1.0969 acc_val: 0.8798
Epoch: 0112 loss_train: 0.3675 acc_train: 0.9806 loss_val: 1.0711 acc_val: 0.8829
Epoch: 0113 loss_train: 0.3062 acc_train: 0.9853 loss_val: 1.0910 acc_val: 0.8816
Epoch: 0114 loss_train: 0.2863 acc_train: 0.9860 loss_val: 1.1241 acc_val: 0.8775
Epoch: 0115 loss_train: 0.2794 acc_train: 0.9877 loss_val: 1.1985 acc_val: 0.8798
Epoch: 0116 loss_train: 0.2628 acc_train: 0.9877 loss_val: 1.1814 acc_val: 0.8780
Epoch: 0117 loss_train: 0.2706 acc_train: 0.9870 loss_val: 1.2116 acc_val: 0.8811
Epoch: 0118 loss_train: 0.2647 acc_train: 0.9873 loss_val: 1.1897 acc_val: 0.8811
Epoch: 0119 loss_train: 0.2516 acc_train: 0.9891 loss_val: 1.1783 acc_val: 0.8824
Epoch: 0120 loss_train: 0.2204 acc_train: 0.9901 loss_val: 1.2201 acc_val: 0.8798
Epoch: 0121 loss_train: 0.2276 acc_train: 0.9901 loss_val: 1.2666 acc_val: 0.8793
Epoch: 0122 loss_train: 0.2196 acc_train: 0.9899 loss_val: 1.2593 acc_val: 0.8801
Epoch: 0123 loss_train: 0.1977 acc_train: 0.9910 loss_val: 1.2743 acc_val: 0.8818
Epoch: 0124 loss_train: 0.1905 acc_train: 0.9919 loss_val: 1.3226 acc_val: 0.8773
Epoch: 0125 loss_train: 0.1880 acc_train: 0.9926 loss_val: 1.3028 acc_val: 0.8801
Epoch: 0126 loss_train: 0.1949 acc_train: 0.9920 loss_val: 1.3192 acc_val: 0.8780
Epoch: 0127 loss_train: 0.2209 acc_train: 0.9902 loss_val: 1.3070 acc_val: 0.8758
Epoch: 0128 loss_train: 0.2622 acc_train: 0.9866 loss_val: 1.2770 acc_val: 0.8829
Epoch: 0129 loss_train: 0.2576 acc_train: 0.9884 loss_val: 1.2361 acc_val: 0.8785
Epoch: 0130 loss_train: 0.2407 acc_train: 0.9892 loss_val: 1.3049 acc_val: 0.8765
Epoch: 0131 loss_train: 0.2296 acc_train: 0.9901 loss_val: 1.2518 acc_val: 0.8791
Epoch: 0132 loss_train: 0.2209 acc_train: 0.9899 loss_val: 1.2675 acc_val: 0.8803
Epoch: 0133 loss_train: 0.2049 acc_train: 0.9909 loss_val: 1.2856 acc_val: 0.8796
Epoch: 0134 loss_train: 0.1963 acc_train: 0.9913 loss_val: 1.2263 acc_val: 0.8803
Epoch: 0135 loss_train: 0.2195 acc_train: 0.9897 loss_val: 1.2534 acc_val: 0.8826
Epoch: 0136 loss_train: 0.1919 acc_train: 0.9917 loss_val: 1.3379 acc_val: 0.8778
Epoch: 0137 loss_train: 0.1735 acc_train: 0.9931 loss_val: 1.2871 acc_val: 0.8829
Epoch: 0138 loss_train: 0.1707 acc_train: 0.9933 loss_val: 1.3179 acc_val: 0.8818
Epoch: 0139 loss_train: 0.1558 acc_train: 0.9936 loss_val: 1.3507 acc_val: 0.8831
Epoch: 0140 loss_train: 0.1588 acc_train: 0.9929 loss_val: 1.3458 acc_val: 0.8806
Optimization Finished!
Train cost: 114.7153s
Loading 90th epoch
Test set results: loss= 0.7947 accuracy= 0.8813
DGL backend not selected or invalid.  Assuming PyTorch for now.
Setting the default backend to "pytorch". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)
Downloading /root/.dgl/pubmed.zip from https://data.dgl.ai/dataset/pubmed.zip...
Extracting file to /root/.dgl/pubmed
Finished data loading and preprocessing.
  NumNodes: 19717
  NumEdges: 88651
  NumFeats: 500
  NumClasses: 3
  NumTrainingSamples: 60
  NumValidationSamples: 500
  NumTestSamples: 1000
Done saving data into cached files.
/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:354: DGLWarning: DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges
  dgl_warning("DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges")
/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:

	DGLGraph.adjacency_matrix(transpose, scipy_fmt="csr").

  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '
/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)
  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()
TransformerModel(
  (att_embeddings_nope): Linear(in_features=515, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU(approximate='none')
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=3, bias=True)
)
total params: 2501125
Epoch: 0001 loss_train: 6.4541 acc_train: 0.4269 loss_val: 1.9895 acc_val: 0.0920
Epoch: 0002 loss_train: 5.5849 acc_train: 0.6452 loss_val: 1.6297 acc_val: 0.6848
Epoch: 0003 loss_train: 4.5642 acc_train: 0.7033 loss_val: 1.3014 acc_val: 0.7515
Epoch: 0004 loss_train: 3.6853 acc_train: 0.7670 loss_val: 1.1068 acc_val: 0.7842
Epoch: 0005 loss_train: 3.2232 acc_train: 0.7882 loss_val: 1.0260 acc_val: 0.7977
Epoch: 0006 loss_train: 2.9591 acc_train: 0.7998 loss_val: 0.9305 acc_val: 0.8215
Epoch: 0007 loss_train: 2.6816 acc_train: 0.8231 loss_val: 0.8531 acc_val: 0.8339
Epoch: 0008 loss_train: 2.4217 acc_train: 0.8399 loss_val: 0.7822 acc_val: 0.8509
Epoch: 0009 loss_train: 2.2013 acc_train: 0.8547 loss_val: 0.7494 acc_val: 0.8608
Epoch: 0010 loss_train: 2.0429 acc_train: 0.8635 loss_val: 0.7086 acc_val: 0.8692
Epoch: 0011 loss_train: 1.8785 acc_train: 0.8754 loss_val: 0.6752 acc_val: 0.8722
Epoch: 0012 loss_train: 1.7319 acc_train: 0.8893 loss_val: 0.6640 acc_val: 0.8768
Epoch: 0013 loss_train: 1.6005 acc_train: 0.8985 loss_val: 0.6472 acc_val: 0.8826
Epoch: 0014 loss_train: 1.5224 acc_train: 0.9048 loss_val: 0.6528 acc_val: 0.8844
Epoch: 0015 loss_train: 1.5214 acc_train: 0.9014 loss_val: 0.7041 acc_val: 0.8742
Epoch: 0016 loss_train: 1.4393 acc_train: 0.9066 loss_val: 0.6359 acc_val: 0.8841
Epoch: 0017 loss_train: 1.3206 acc_train: 0.9170 loss_val: 0.6133 acc_val: 0.8877
Epoch: 0018 loss_train: 1.1863 acc_train: 0.9269 loss_val: 0.6587 acc_val: 0.8846
Epoch: 0019 loss_train: 1.1557 acc_train: 0.9262 loss_val: 0.6676 acc_val: 0.8831
Epoch: 0020 loss_train: 1.0563 acc_train: 0.9342 loss_val: 0.6800 acc_val: 0.8851
Epoch: 0021 loss_train: 0.9631 acc_train: 0.9432 loss_val: 0.6842 acc_val: 0.8900
Epoch: 0022 loss_train: 0.9375 acc_train: 0.9409 loss_val: 0.7017 acc_val: 0.8910
Epoch: 0023 loss_train: 0.8967 acc_train: 0.9448 loss_val: 0.7113 acc_val: 0.8872
Epoch: 0024 loss_train: 1.0552 acc_train: 0.9325 loss_val: 0.6880 acc_val: 0.8872
Epoch: 0025 loss_train: 0.9134 acc_train: 0.9416 loss_val: 0.7344 acc_val: 0.8808
Epoch: 0026 loss_train: 0.7897 acc_train: 0.9500 loss_val: 0.7336 acc_val: 0.8862
Epoch: 0027 loss_train: 0.6379 acc_train: 0.9639 loss_val: 0.7793 acc_val: 0.8862
Epoch: 0028 loss_train: 0.5527 acc_train: 0.9675 loss_val: 0.8403 acc_val: 0.8821
Epoch: 0029 loss_train: 0.4766 acc_train: 0.9742 loss_val: 0.8603 acc_val: 0.8864
Epoch: 0030 loss_train: 0.4146 acc_train: 0.9772 loss_val: 0.9487 acc_val: 0.8844
Epoch: 0031 loss_train: 0.3544 acc_train: 0.9805 loss_val: 1.0001 acc_val: 0.8811
Epoch: 0032 loss_train: 0.2882 acc_train: 0.9835 loss_val: 1.0792 acc_val: 0.8775
Epoch: 0033 loss_train: 0.2920 acc_train: 0.9835 loss_val: 1.1571 acc_val: 0.8765
Epoch: 0034 loss_train: 0.4328 acc_train: 0.9721 loss_val: 1.2567 acc_val: 0.8732
Epoch: 0035 loss_train: 0.4426 acc_train: 0.9735 loss_val: 1.1263 acc_val: 0.8844
Epoch: 0036 loss_train: 0.3413 acc_train: 0.9788 loss_val: 1.1305 acc_val: 0.8811
Epoch: 0037 loss_train: 0.2355 acc_train: 0.9858 loss_val: 1.0992 acc_val: 0.8788
Epoch: 0038 loss_train: 0.1636 acc_train: 0.9915 loss_val: 1.2547 acc_val: 0.8806
Epoch: 0039 loss_train: 0.1506 acc_train: 0.9911 loss_val: 1.3046 acc_val: 0.8780
Epoch: 0040 loss_train: 0.1387 acc_train: 0.9915 loss_val: 1.3813 acc_val: 0.8806
Epoch: 0041 loss_train: 0.1333 acc_train: 0.9935 loss_val: 1.3895 acc_val: 0.8770
Epoch: 0042 loss_train: 0.1142 acc_train: 0.9939 loss_val: 1.4463 acc_val: 0.8834
Epoch: 0043 loss_train: 0.1107 acc_train: 0.9927 loss_val: 1.5184 acc_val: 0.8763
Epoch: 0044 loss_train: 0.0951 acc_train: 0.9950 loss_val: 1.5492 acc_val: 0.8753
Epoch: 0045 loss_train: 0.0927 acc_train: 0.9950 loss_val: 1.5712 acc_val: 0.8783
Epoch: 0046 loss_train: 0.1260 acc_train: 0.9924 loss_val: 1.5838 acc_val: 0.8763
Epoch: 0047 loss_train: 0.1548 acc_train: 0.9919 loss_val: 1.7073 acc_val: 0.8656
Epoch: 0048 loss_train: 0.2139 acc_train: 0.9872 loss_val: 1.5707 acc_val: 0.8725
Epoch: 0049 loss_train: 0.2478 acc_train: 0.9866 loss_val: 1.5039 acc_val: 0.8775
Epoch: 0050 loss_train: 0.2309 acc_train: 0.9861 loss_val: 1.3982 acc_val: 0.8780
Epoch: 0051 loss_train: 0.2149 acc_train: 0.9880 loss_val: 1.3446 acc_val: 0.8712
Epoch: 0052 loss_train: 0.1672 acc_train: 0.9902 loss_val: 1.4230 acc_val: 0.8740
Epoch: 0053 loss_train: 0.1465 acc_train: 0.9921 loss_val: 1.3990 acc_val: 0.8803
Epoch: 0054 loss_train: 0.1034 acc_train: 0.9943 loss_val: 1.4059 acc_val: 0.8791
Epoch: 0055 loss_train: 0.0760 acc_train: 0.9961 loss_val: 1.4440 acc_val: 0.8778
Epoch: 0056 loss_train: 0.0586 acc_train: 0.9970 loss_val: 1.6051 acc_val: 0.8750
Epoch: 0057 loss_train: 0.0729 acc_train: 0.9955 loss_val: 1.6664 acc_val: 0.8775
Epoch: 0058 loss_train: 0.1116 acc_train: 0.9930 loss_val: 1.6312 acc_val: 0.8745
Epoch: 0059 loss_train: 0.2178 acc_train: 0.9881 loss_val: 1.9535 acc_val: 0.8451
Epoch: 0060 loss_train: 0.4945 acc_train: 0.9735 loss_val: 1.6471 acc_val: 0.8636
Epoch: 0061 loss_train: 0.3522 acc_train: 0.9788 loss_val: 1.1348 acc_val: 0.8732
Epoch: 0062 loss_train: 0.3408 acc_train: 0.9807 loss_val: 1.1913 acc_val: 0.8818
Epoch: 0063 loss_train: 0.2537 acc_train: 0.9848 loss_val: 1.1722 acc_val: 0.8725
Epoch: 0064 loss_train: 0.1675 acc_train: 0.9908 loss_val: 1.1968 acc_val: 0.8834
Epoch: 0065 loss_train: 0.0957 acc_train: 0.9954 loss_val: 1.2707 acc_val: 0.8813
Epoch: 0066 loss_train: 0.0746 acc_train: 0.9964 loss_val: 1.4198 acc_val: 0.8811
Epoch: 0067 loss_train: 0.0550 acc_train: 0.9964 loss_val: 1.4792 acc_val: 0.8778
Epoch: 0068 loss_train: 0.0414 acc_train: 0.9977 loss_val: 1.5503 acc_val: 0.8854
Epoch: 0069 loss_train: 0.0393 acc_train: 0.9980 loss_val: 1.5880 acc_val: 0.8765
Epoch: 0070 loss_train: 0.0252 acc_train: 0.9988 loss_val: 1.5868 acc_val: 0.8851
Epoch: 0071 loss_train: 0.0212 acc_train: 0.9990 loss_val: 1.6438 acc_val: 0.8824
Epoch: 0072 loss_train: 0.0308 acc_train: 0.9984 loss_val: 1.8025 acc_val: 0.8717
Optimization Finished!
Train cost: 57.9187s
Loading 22th epoch
Test set results: loss= 0.7247 accuracy= 0.8841
  NumNodes: 19717
  NumEdges: 88651
  NumFeats: 500
  NumClasses: 3
  NumTrainingSamples: 60
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:354: DGLWarning: DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges
  dgl_warning("DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges")
/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:

	DGLGraph.adjacency_matrix(transpose, scipy_fmt="csr").

  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '
/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)
  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()
TransformerModel(
  (att_embeddings_nope): Linear(in_features=515, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU(approximate='none')
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=3, bias=True)
)
total params: 2501125
Epoch: 0001 loss_train: 6.4406 acc_train: 0.4287 loss_val: 1.9885 acc_val: 0.0892
Epoch: 0002 loss_train: 5.6149 acc_train: 0.6323 loss_val: 1.6443 acc_val: 0.6767
Epoch: 0003 loss_train: 4.6597 acc_train: 0.6860 loss_val: 1.3244 acc_val: 0.7459
Epoch: 0004 loss_train: 3.8341 acc_train: 0.7515 loss_val: 1.1463 acc_val: 0.7754
Epoch: 0005 loss_train: 3.3929 acc_train: 0.7735 loss_val: 1.0611 acc_val: 0.7893
Epoch: 0006 loss_train: 3.1099 acc_train: 0.7896 loss_val: 0.9666 acc_val: 0.8106
Epoch: 0007 loss_train: 2.8246 acc_train: 0.8119 loss_val: 0.8885 acc_val: 0.8235
Epoch: 0008 loss_train: 2.5580 acc_train: 0.8292 loss_val: 0.8178 acc_val: 0.8413
Epoch: 0009 loss_train: 2.3220 acc_train: 0.8447 loss_val: 0.7651 acc_val: 0.8555
Epoch: 0010 loss_train: 2.1406 acc_train: 0.8585 loss_val: 0.7289 acc_val: 0.8618
Epoch: 0011 loss_train: 1.9611 acc_train: 0.8709 loss_val: 0.6913 acc_val: 0.8707
Epoch: 0012 loss_train: 1.8167 acc_train: 0.8819 loss_val: 0.6735 acc_val: 0.8755
Epoch: 0013 loss_train: 1.6737 acc_train: 0.8942 loss_val: 0.6613 acc_val: 0.8801
Epoch: 0014 loss_train: 1.5667 acc_train: 0.9004 loss_val: 0.6620 acc_val: 0.8854
Epoch: 0015 loss_train: 1.4752 acc_train: 0.9079 loss_val: 0.6371 acc_val: 0.8846
Epoch: 0016 loss_train: 1.3863 acc_train: 0.9123 loss_val: 0.6500 acc_val: 0.8826
Epoch: 0017 loss_train: 1.2990 acc_train: 0.9210 loss_val: 0.7068 acc_val: 0.8722
Epoch: 0018 loss_train: 1.3910 acc_train: 0.9085 loss_val: 0.6809 acc_val: 0.8763
Epoch: 0019 loss_train: 1.3450 acc_train: 0.9157 loss_val: 0.6689 acc_val: 0.8737
Epoch: 0020 loss_train: 1.1988 acc_train: 0.9222 loss_val: 0.6534 acc_val: 0.8915
Epoch: 0021 loss_train: 1.0823 acc_train: 0.9317 loss_val: 0.7230 acc_val: 0.8829
Epoch: 0022 loss_train: 0.9991 acc_train: 0.9385 loss_val: 0.6924 acc_val: 0.8867
Epoch: 0023 loss_train: 0.9064 acc_train: 0.9460 loss_val: 0.7059 acc_val: 0.8877
Epoch: 0024 loss_train: 0.9205 acc_train: 0.9411 loss_val: 0.7509 acc_val: 0.8816
Epoch: 0025 loss_train: 0.7504 acc_train: 0.9557 loss_val: 0.7492 acc_val: 0.8907
Epoch: 0026 loss_train: 0.6335 acc_train: 0.9664 loss_val: 0.7851 acc_val: 0.8887
Epoch: 0027 loss_train: 0.5496 acc_train: 0.9693 loss_val: 0.8479 acc_val: 0.8895
Epoch: 0028 loss_train: 0.4719 acc_train: 0.9746 loss_val: 0.9471 acc_val: 0.8841
Epoch: 0029 loss_train: 0.4674 acc_train: 0.9735 loss_val: 0.9378 acc_val: 0.8806
Epoch: 0030 loss_train: 0.5571 acc_train: 0.9647 loss_val: 1.3244 acc_val: 0.8438
Epoch: 0031 loss_train: 1.5416 acc_train: 0.9075 loss_val: 0.8898 acc_val: 0.8702
Epoch: 0032 loss_train: 1.0276 acc_train: 0.9381 loss_val: 0.8546 acc_val: 0.8684
Epoch: 0033 loss_train: 0.7790 acc_train: 0.9508 loss_val: 0.8513 acc_val: 0.8824
Epoch: 0034 loss_train: 0.6046 acc_train: 0.9651 loss_val: 0.8170 acc_val: 0.8869
Epoch: 0035 loss_train: 0.4240 acc_train: 0.9773 loss_val: 0.9151 acc_val: 0.8831
Epoch: 0036 loss_train: 0.3078 acc_train: 0.9829 loss_val: 1.0144 acc_val: 0.8849
Epoch: 0037 loss_train: 0.2113 acc_train: 0.9894 loss_val: 1.0881 acc_val: 0.8839
Epoch: 0038 loss_train: 0.1856 acc_train: 0.9907 loss_val: 1.1737 acc_val: 0.8803
Epoch: 0039 loss_train: 0.1712 acc_train: 0.9919 loss_val: 1.2500 acc_val: 0.8778
Epoch: 0040 loss_train: 0.1401 acc_train: 0.9925 loss_val: 1.2707 acc_val: 0.8824
Epoch: 0041 loss_train: 0.1342 acc_train: 0.9925 loss_val: 1.3495 acc_val: 0.8826
Epoch: 0042 loss_train: 0.1021 acc_train: 0.9943 loss_val: 1.3954 acc_val: 0.8798
Epoch: 0043 loss_train: 0.0876 acc_train: 0.9950 loss_val: 1.4366 acc_val: 0.8780
Epoch: 0044 loss_train: 0.0750 acc_train: 0.9951 loss_val: 1.4803 acc_val: 0.8722
Epoch: 0045 loss_train: 0.0761 acc_train: 0.9964 loss_val: 1.5631 acc_val: 0.8760
Epoch: 0046 loss_train: 0.0659 acc_train: 0.9963 loss_val: 1.5505 acc_val: 0.8785
Epoch: 0047 loss_train: 0.0706 acc_train: 0.9959 loss_val: 1.6459 acc_val: 0.8808
Epoch: 0048 loss_train: 0.1193 acc_train: 0.9941 loss_val: 1.7435 acc_val: 0.8684
Epoch: 0049 loss_train: 0.1788 acc_train: 0.9896 loss_val: 1.6557 acc_val: 0.8656
Epoch: 0050 loss_train: 0.2048 acc_train: 0.9878 loss_val: 1.5470 acc_val: 0.8737
Epoch: 0051 loss_train: 0.2190 acc_train: 0.9867 loss_val: 1.5409 acc_val: 0.8656
Epoch: 0052 loss_train: 0.2026 acc_train: 0.9880 loss_val: 1.3996 acc_val: 0.8659
Epoch: 0053 loss_train: 0.1851 acc_train: 0.9891 loss_val: 1.5478 acc_val: 0.8740
Epoch: 0054 loss_train: 0.1589 acc_train: 0.9908 loss_val: 1.5006 acc_val: 0.8697
Epoch: 0055 loss_train: 0.2432 acc_train: 0.9864 loss_val: 1.5211 acc_val: 0.8727
Epoch: 0056 loss_train: 0.2309 acc_train: 0.9865 loss_val: 1.3800 acc_val: 0.8727
Epoch: 0057 loss_train: 0.1824 acc_train: 0.9896 loss_val: 1.4773 acc_val: 0.8676
Epoch: 0058 loss_train: 0.1893 acc_train: 0.9890 loss_val: 1.3975 acc_val: 0.8737
Epoch: 0059 loss_train: 0.1490 acc_train: 0.9920 loss_val: 1.4555 acc_val: 0.8798
Epoch: 0060 loss_train: 0.0989 acc_train: 0.9947 loss_val: 1.4341 acc_val: 0.8737
Epoch: 0061 loss_train: 0.0593 acc_train: 0.9971 loss_val: 1.5361 acc_val: 0.8768
Epoch: 0062 loss_train: 0.0647 acc_train: 0.9972 loss_val: 1.6024 acc_val: 0.8687
Epoch: 0063 loss_train: 0.0347 acc_train: 0.9984 loss_val: 1.6840 acc_val: 0.8704
Epoch: 0064 loss_train: 0.0299 acc_train: 0.9988 loss_val: 1.7686 acc_val: 0.8755
Epoch: 0065 loss_train: 0.0317 acc_train: 0.9982 loss_val: 1.7636 acc_val: 0.8765
Epoch: 0066 loss_train: 0.0323 acc_train: 0.9983 loss_val: 1.9067 acc_val: 0.8755
Epoch: 0067 loss_train: 0.0454 acc_train: 0.9975 loss_val: 1.8713 acc_val: 0.8725
Epoch: 0068 loss_train: 0.0406 acc_train: 0.9979 loss_val: 1.9212 acc_val: 0.8684
Epoch: 0069 loss_train: 0.0463 acc_train: 0.9971 loss_val: 1.9371 acc_val: 0.8699
Epoch: 0070 loss_train: 0.0588 acc_train: 0.9972 loss_val: 1.8549 acc_val: 0.8760
Optimization Finished!
Train cost: 52.2059s
Loading 20th epoch
Test set results: loss= 0.6964 accuracy= 0.8805
  NumNodes: 19717
  NumEdges: 88651
  NumFeats: 500
  NumClasses: 3
  NumTrainingSamples: 60
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:354: DGLWarning: DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges
  dgl_warning("DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges")
/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:

	DGLGraph.adjacency_matrix(transpose, scipy_fmt="csr").

  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '
/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)
  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()
TransformerModel(
  (att_embeddings_nope): Linear(in_features=515, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU(approximate='none')
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=3, bias=True)
)
total params: 2501125
Epoch: 0001 loss_train: 6.4417 acc_train: 0.4259 loss_val: 1.9890 acc_val: 0.0075
Epoch: 0002 loss_train: 5.6272 acc_train: 0.6318 loss_val: 1.6520 acc_val: 0.6717
Epoch: 0003 loss_train: 4.7118 acc_train: 0.6775 loss_val: 1.3394 acc_val: 0.7391
Epoch: 0004 loss_train: 3.9044 acc_train: 0.7432 loss_val: 1.1601 acc_val: 0.7723
Epoch: 0005 loss_train: 3.4605 acc_train: 0.7685 loss_val: 1.0742 acc_val: 0.7858
Epoch: 0006 loss_train: 3.1755 acc_train: 0.7837 loss_val: 0.9821 acc_val: 0.8038
Epoch: 0007 loss_train: 2.8977 acc_train: 0.8067 loss_val: 0.9024 acc_val: 0.8210
Epoch: 0008 loss_train: 2.6179 acc_train: 0.8249 loss_val: 0.8305 acc_val: 0.8372
Epoch: 0009 loss_train: 2.3692 acc_train: 0.8393 loss_val: 0.7737 acc_val: 0.8547
Epoch: 0010 loss_train: 2.1781 acc_train: 0.8552 loss_val: 0.7377 acc_val: 0.8608
Epoch: 0011 loss_train: 1.9740 acc_train: 0.8675 loss_val: 0.6991 acc_val: 0.8689
Epoch: 0012 loss_train: 1.8384 acc_train: 0.8810 loss_val: 0.6858 acc_val: 0.8720
Epoch: 0013 loss_train: 1.6818 acc_train: 0.8922 loss_val: 0.6627 acc_val: 0.8801
Epoch: 0014 loss_train: 1.5782 acc_train: 0.9018 loss_val: 0.6708 acc_val: 0.8806
Epoch: 0015 loss_train: 1.5347 acc_train: 0.8997 loss_val: 0.6717 acc_val: 0.8783
Epoch: 0016 loss_train: 1.4419 acc_train: 0.9070 loss_val: 0.6371 acc_val: 0.8836
Epoch: 0017 loss_train: 1.3051 acc_train: 0.9193 loss_val: 0.6514 acc_val: 0.8829
Epoch: 0018 loss_train: 1.2726 acc_train: 0.9202 loss_val: 0.7487 acc_val: 0.8669
Epoch: 0019 loss_train: 1.2901 acc_train: 0.9156 loss_val: 0.7216 acc_val: 0.8740
Epoch: 0020 loss_train: 1.1848 acc_train: 0.9273 loss_val: 0.6975 acc_val: 0.8758
Epoch: 0021 loss_train: 1.1341 acc_train: 0.9276 loss_val: 0.6960 acc_val: 0.8844
Epoch: 0022 loss_train: 1.0309 acc_train: 0.9347 loss_val: 0.6921 acc_val: 0.8882
Epoch: 0023 loss_train: 0.9499 acc_train: 0.9439 loss_val: 0.7158 acc_val: 0.8818
Epoch: 0024 loss_train: 0.8538 acc_train: 0.9496 loss_val: 0.7248 acc_val: 0.8849
Epoch: 0025 loss_train: 0.7351 acc_train: 0.9581 loss_val: 0.7631 acc_val: 0.8798
Epoch: 0026 loss_train: 0.6172 acc_train: 0.9677 loss_val: 0.8216 acc_val: 0.8831
Epoch: 0027 loss_train: 0.5615 acc_train: 0.9691 loss_val: 0.8991 acc_val: 0.8818
Epoch: 0028 loss_train: 0.4852 acc_train: 0.9735 loss_val: 0.9509 acc_val: 0.8780
Epoch: 0029 loss_train: 0.4331 acc_train: 0.9745 loss_val: 1.0207 acc_val: 0.8778
Epoch: 0030 loss_train: 0.4475 acc_train: 0.9745 loss_val: 1.0186 acc_val: 0.8773
Epoch: 0031 loss_train: 0.3916 acc_train: 0.9780 loss_val: 1.1614 acc_val: 0.8755
Epoch: 0032 loss_train: 0.3870 acc_train: 0.9741 loss_val: 1.4577 acc_val: 0.8611
Epoch: 0033 loss_train: 1.0781 acc_train: 0.9378 loss_val: 1.1499 acc_val: 0.8684
Epoch: 0034 loss_train: 0.6947 acc_train: 0.9594 loss_val: 0.9311 acc_val: 0.8656
Epoch: 0035 loss_train: 0.5278 acc_train: 0.9665 loss_val: 1.0336 acc_val: 0.8687
Epoch: 0036 loss_train: 0.4250 acc_train: 0.9765 loss_val: 1.0608 acc_val: 0.8735
Epoch: 0037 loss_train: 0.2996 acc_train: 0.9832 loss_val: 1.1202 acc_val: 0.8737
Epoch: 0038 loss_train: 0.2217 acc_train: 0.9882 loss_val: 1.2450 acc_val: 0.8692
Epoch: 0039 loss_train: 0.1865 acc_train: 0.9894 loss_val: 1.3008 acc_val: 0.8709
Epoch: 0040 loss_train: 0.1721 acc_train: 0.9914 loss_val: 1.3493 acc_val: 0.8742
Epoch: 0041 loss_train: 0.1433 acc_train: 0.9927 loss_val: 1.5077 acc_val: 0.8730
Epoch: 0042 loss_train: 0.1414 acc_train: 0.9920 loss_val: 1.5194 acc_val: 0.8699
Epoch: 0043 loss_train: 0.1383 acc_train: 0.9921 loss_val: 1.5798 acc_val: 0.8742
Epoch: 0044 loss_train: 0.1376 acc_train: 0.9918 loss_val: 1.6024 acc_val: 0.8763
Epoch: 0045 loss_train: 0.1484 acc_train: 0.9921 loss_val: 1.5817 acc_val: 0.8631
Epoch: 0046 loss_train: 0.1328 acc_train: 0.9922 loss_val: 1.5749 acc_val: 0.8760
Epoch: 0047 loss_train: 0.1270 acc_train: 0.9923 loss_val: 1.5784 acc_val: 0.8715
Epoch: 0048 loss_train: 0.1068 acc_train: 0.9939 loss_val: 1.6249 acc_val: 0.8715
Epoch: 0049 loss_train: 0.1132 acc_train: 0.9941 loss_val: 1.7187 acc_val: 0.8758
Epoch: 0050 loss_train: 0.1048 acc_train: 0.9939 loss_val: 1.7130 acc_val: 0.8715
Epoch: 0051 loss_train: 0.0796 acc_train: 0.9948 loss_val: 1.7050 acc_val: 0.8715
Epoch: 0052 loss_train: 0.0782 acc_train: 0.9951 loss_val: 1.6908 acc_val: 0.8682
Epoch: 0053 loss_train: 0.0780 acc_train: 0.9955 loss_val: 1.7922 acc_val: 0.8758
Epoch: 0054 loss_train: 0.1090 acc_train: 0.9943 loss_val: 1.9216 acc_val: 0.8735
Epoch: 0055 loss_train: 0.1562 acc_train: 0.9911 loss_val: 2.1557 acc_val: 0.8438
Epoch: 0056 loss_train: 6.1454 acc_train: 0.8131 loss_val: 1.1749 acc_val: 0.7875
Epoch: 0057 loss_train: 3.0493 acc_train: 0.8292 loss_val: 1.7827 acc_val: 0.7234
Epoch: 0058 loss_train: 3.1142 acc_train: 0.8128 loss_val: 0.7884 acc_val: 0.8545
Epoch: 0059 loss_train: 2.2006 acc_train: 0.8580 loss_val: 0.8578 acc_val: 0.8463
Epoch: 0060 loss_train: 2.0337 acc_train: 0.8728 loss_val: 0.7436 acc_val: 0.8651
Epoch: 0061 loss_train: 1.8408 acc_train: 0.8844 loss_val: 0.7254 acc_val: 0.8618
Epoch: 0062 loss_train: 1.7569 acc_train: 0.8900 loss_val: 0.6703 acc_val: 0.8737
Epoch: 0063 loss_train: 1.6010 acc_train: 0.9019 loss_val: 0.6877 acc_val: 0.8760
Epoch: 0064 loss_train: 1.4899 acc_train: 0.9098 loss_val: 0.6483 acc_val: 0.8829
Epoch: 0065 loss_train: 1.3990 acc_train: 0.9161 loss_val: 0.6507 acc_val: 0.8824
Epoch: 0066 loss_train: 1.3086 acc_train: 0.9196 loss_val: 0.6570 acc_val: 0.8851
Epoch: 0067 loss_train: 1.2320 acc_train: 0.9244 loss_val: 0.6740 acc_val: 0.8851
Epoch: 0068 loss_train: 1.1792 acc_train: 0.9286 loss_val: 0.7206 acc_val: 0.8750
Epoch: 0069 loss_train: 1.1334 acc_train: 0.9331 loss_val: 0.7133 acc_val: 0.8818
Epoch: 0070 loss_train: 1.0730 acc_train: 0.9351 loss_val: 0.7102 acc_val: 0.8824
Epoch: 0071 loss_train: 0.9536 acc_train: 0.9445 loss_val: 0.7357 acc_val: 0.8811
Epoch: 0072 loss_train: 0.9017 acc_train: 0.9484 loss_val: 0.8360 acc_val: 0.8753
Optimization Finished!
Train cost: 52.4891s
Loading 22th epoch
Test set results: loss= 0.7415 accuracy= 0.8783