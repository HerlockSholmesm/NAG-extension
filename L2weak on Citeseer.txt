  NumNodes: 3327
  NumEdges: 9228
  NumFeats: 3703
  NumClasses: 6
  NumTrainingSamples: 120
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:

	DGLGraph.adjacency_matrix(transpose, scipy_fmt="csr").

  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '
/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)
  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()
TransformerModel(
  (att_embeddings_nope): Linear(in_features=3743, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU(approximate='none')
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=6, bias=True)
)
total params: 4154632
Epoch: 0001 loss_train: 1.8342 acc_train: 0.1412 loss_val: 1.8246 acc_val: 0.1757
Epoch: 0002 loss_train: 1.8278 acc_train: 0.1527 loss_val: 1.8145 acc_val: 0.1892
Epoch: 0003 loss_train: 1.8206 acc_train: 0.1592 loss_val: 1.7996 acc_val: 0.1877
Epoch: 0004 loss_train: 1.8050 acc_train: 0.1738 loss_val: 1.7804 acc_val: 0.1982
Epoch: 0005 loss_train: 1.7892 acc_train: 0.1948 loss_val: 1.7574 acc_val: 0.2598
Epoch: 0006 loss_train: 1.7646 acc_train: 0.2323 loss_val: 1.7310 acc_val: 0.3273
Epoch: 0007 loss_train: 1.7381 acc_train: 0.2969 loss_val: 1.7019 acc_val: 0.4054
Epoch: 0008 loss_train: 1.7107 acc_train: 0.3796 loss_val: 1.6705 acc_val: 0.4745
Epoch: 0009 loss_train: 1.6795 acc_train: 0.4687 loss_val: 1.6371 acc_val: 0.5180
Epoch: 0010 loss_train: 1.6443 acc_train: 0.5238 loss_val: 1.6021 acc_val: 0.5450
Epoch: 0011 loss_train: 1.6093 acc_train: 0.5633 loss_val: 1.5659 acc_val: 0.5631
Epoch: 0012 loss_train: 1.5712 acc_train: 0.5779 loss_val: 1.5283 acc_val: 0.5751
Epoch: 0013 loss_train: 1.5338 acc_train: 0.5939 loss_val: 1.4895 acc_val: 0.5856
Epoch: 0014 loss_train: 1.4936 acc_train: 0.6134 loss_val: 1.4499 acc_val: 0.6006
Epoch: 0015 loss_train: 1.4514 acc_train: 0.6254 loss_val: 1.4097 acc_val: 0.6126
Epoch: 0016 loss_train: 1.4086 acc_train: 0.6304 loss_val: 1.3691 acc_val: 0.6201
Epoch: 0017 loss_train: 1.3665 acc_train: 0.6485 loss_val: 1.3284 acc_val: 0.6276
Epoch: 0018 loss_train: 1.3224 acc_train: 0.6550 loss_val: 1.2880 acc_val: 0.6396
Epoch: 0019 loss_train: 1.2805 acc_train: 0.6660 loss_val: 1.2483 acc_val: 0.6456
Epoch: 0020 loss_train: 1.2373 acc_train: 0.6730 loss_val: 1.2094 acc_val: 0.6562
Epoch: 0021 loss_train: 1.1942 acc_train: 0.6790 loss_val: 1.1717 acc_val: 0.6682
Epoch: 0022 loss_train: 1.1514 acc_train: 0.6960 loss_val: 1.1356 acc_val: 0.6892
Epoch: 0023 loss_train: 1.1101 acc_train: 0.7031 loss_val: 1.1012 acc_val: 0.6922
Epoch: 0024 loss_train: 1.0701 acc_train: 0.7111 loss_val: 1.0689 acc_val: 0.6937
Epoch: 0025 loss_train: 1.0313 acc_train: 0.7171 loss_val: 1.0388 acc_val: 0.6982
Epoch: 0026 loss_train: 0.9938 acc_train: 0.7266 loss_val: 1.0108 acc_val: 0.7042
Epoch: 0027 loss_train: 0.9575 acc_train: 0.7276 loss_val: 0.9850 acc_val: 0.7042
Epoch: 0028 loss_train: 0.9233 acc_train: 0.7336 loss_val: 0.9612 acc_val: 0.7132
Epoch: 0029 loss_train: 0.8901 acc_train: 0.7391 loss_val: 0.9397 acc_val: 0.7162
Epoch: 0030 loss_train: 0.8563 acc_train: 0.7456 loss_val: 0.9205 acc_val: 0.7192
Epoch: 0031 loss_train: 0.8228 acc_train: 0.7531 loss_val: 0.9037 acc_val: 0.7207
Epoch: 0032 loss_train: 0.7948 acc_train: 0.7616 loss_val: 0.8892 acc_val: 0.7192
Epoch: 0033 loss_train: 0.7652 acc_train: 0.7682 loss_val: 0.8765 acc_val: 0.7207
Epoch: 0034 loss_train: 0.7386 acc_train: 0.7722 loss_val: 0.8653 acc_val: 0.7207
Epoch: 0035 loss_train: 0.7101 acc_train: 0.7777 loss_val: 0.8560 acc_val: 0.7237
Epoch: 0036 loss_train: 0.6839 acc_train: 0.7837 loss_val: 0.8485 acc_val: 0.7267
Epoch: 0037 loss_train: 0.6595 acc_train: 0.7867 loss_val: 0.8427 acc_val: 0.7237
Epoch: 0038 loss_train: 0.6345 acc_train: 0.7972 loss_val: 0.8381 acc_val: 0.7357
Epoch: 0039 loss_train: 0.6103 acc_train: 0.8067 loss_val: 0.8349 acc_val: 0.7372
Epoch: 0040 loss_train: 0.5898 acc_train: 0.8137 loss_val: 0.8334 acc_val: 0.7462
Epoch: 0041 loss_train: 0.5698 acc_train: 0.8167 loss_val: 0.8337 acc_val: 0.7462
Epoch: 0042 loss_train: 0.5483 acc_train: 0.8212 loss_val: 0.8358 acc_val: 0.7477
Epoch: 0043 loss_train: 0.5265 acc_train: 0.8247 loss_val: 0.8397 acc_val: 0.7508
Epoch: 0044 loss_train: 0.5079 acc_train: 0.8292 loss_val: 0.8456 acc_val: 0.7492
Epoch: 0045 loss_train: 0.4877 acc_train: 0.8322 loss_val: 0.8525 acc_val: 0.7447
Epoch: 0046 loss_train: 0.4702 acc_train: 0.8388 loss_val: 0.8603 acc_val: 0.7477
Epoch: 0047 loss_train: 0.4494 acc_train: 0.8478 loss_val: 0.8698 acc_val: 0.7462
Epoch: 0048 loss_train: 0.4337 acc_train: 0.8533 loss_val: 0.8809 acc_val: 0.7417
Epoch: 0049 loss_train: 0.4156 acc_train: 0.8578 loss_val: 0.8936 acc_val: 0.7402
Epoch: 0050 loss_train: 0.3995 acc_train: 0.8653 loss_val: 0.9074 acc_val: 0.7417
Epoch: 0051 loss_train: 0.3842 acc_train: 0.8643 loss_val: 0.9222 acc_val: 0.7372
Epoch: 0052 loss_train: 0.3657 acc_train: 0.8773 loss_val: 0.9390 acc_val: 0.7357
Epoch: 0053 loss_train: 0.3496 acc_train: 0.8808 loss_val: 0.9587 acc_val: 0.7372
Epoch: 0054 loss_train: 0.3342 acc_train: 0.8848 loss_val: 0.9803 acc_val: 0.7312
Epoch: 0055 loss_train: 0.3183 acc_train: 0.8918 loss_val: 1.0022 acc_val: 0.7282
Epoch: 0056 loss_train: 0.2997 acc_train: 0.9004 loss_val: 1.0245 acc_val: 0.7297
Epoch: 0057 loss_train: 0.2826 acc_train: 0.8998 loss_val: 1.0471 acc_val: 0.7282
Epoch: 0058 loss_train: 0.2705 acc_train: 0.9014 loss_val: 1.0700 acc_val: 0.7312
Epoch: 0059 loss_train: 0.2493 acc_train: 0.9119 loss_val: 1.0930 acc_val: 0.7342
Epoch: 0060 loss_train: 0.2338 acc_train: 0.9204 loss_val: 1.1163 acc_val: 0.7342
Epoch: 0061 loss_train: 0.2192 acc_train: 0.9274 loss_val: 1.1416 acc_val: 0.7357
Epoch: 0062 loss_train: 0.2034 acc_train: 0.9359 loss_val: 1.1693 acc_val: 0.7387
Epoch: 0063 loss_train: 0.1880 acc_train: 0.9439 loss_val: 1.1976 acc_val: 0.7432
Epoch: 0064 loss_train: 0.1752 acc_train: 0.9464 loss_val: 1.2253 acc_val: 0.7387
Epoch: 0065 loss_train: 0.1605 acc_train: 0.9549 loss_val: 1.2564 acc_val: 0.7402
Epoch: 0066 loss_train: 0.1493 acc_train: 0.9514 loss_val: 1.2885 acc_val: 0.7387
Epoch: 0067 loss_train: 0.1369 acc_train: 0.9629 loss_val: 1.3184 acc_val: 0.7402
Epoch: 0068 loss_train: 0.1237 acc_train: 0.9619 loss_val: 1.3475 acc_val: 0.7387
Epoch: 0069 loss_train: 0.1141 acc_train: 0.9670 loss_val: 1.3774 acc_val: 0.7372
Epoch: 0070 loss_train: 0.1046 acc_train: 0.9685 loss_val: 1.4064 acc_val: 0.7372
Epoch: 0071 loss_train: 0.0957 acc_train: 0.9710 loss_val: 1.4324 acc_val: 0.7327
Epoch: 0072 loss_train: 0.0864 acc_train: 0.9725 loss_val: 1.4580 acc_val: 0.7282
Epoch: 0073 loss_train: 0.0794 acc_train: 0.9745 loss_val: 1.4830 acc_val: 0.7297
Epoch: 0074 loss_train: 0.0719 acc_train: 0.9775 loss_val: 1.5067 acc_val: 0.7282
Epoch: 0075 loss_train: 0.0650 acc_train: 0.9810 loss_val: 1.5305 acc_val: 0.7297
Epoch: 0076 loss_train: 0.0593 acc_train: 0.9825 loss_val: 1.5557 acc_val: 0.7282
Epoch: 0077 loss_train: 0.0555 acc_train: 0.9840 loss_val: 1.5821 acc_val: 0.7252
Epoch: 0078 loss_train: 0.0480 acc_train: 0.9870 loss_val: 1.6092 acc_val: 0.7267
Epoch: 0079 loss_train: 0.0456 acc_train: 0.9875 loss_val: 1.6336 acc_val: 0.7282
Epoch: 0080 loss_train: 0.0405 acc_train: 0.9875 loss_val: 1.6592 acc_val: 0.7267
Epoch: 0081 loss_train: 0.0359 acc_train: 0.9900 loss_val: 1.6857 acc_val: 0.7282
Epoch: 0082 loss_train: 0.0342 acc_train: 0.9925 loss_val: 1.7128 acc_val: 0.7267
Epoch: 0083 loss_train: 0.0307 acc_train: 0.9915 loss_val: 1.7375 acc_val: 0.7252
Epoch: 0084 loss_train: 0.0291 acc_train: 0.9915 loss_val: 1.7580 acc_val: 0.7237
Epoch: 0085 loss_train: 0.0253 acc_train: 0.9950 loss_val: 1.7781 acc_val: 0.7252
Epoch: 0086 loss_train: 0.0240 acc_train: 0.9915 loss_val: 1.8004 acc_val: 0.7267
Epoch: 0087 loss_train: 0.0223 acc_train: 0.9945 loss_val: 1.8231 acc_val: 0.7222
Epoch: 0088 loss_train: 0.0203 acc_train: 0.9950 loss_val: 1.8434 acc_val: 0.7237
Epoch: 0089 loss_train: 0.0180 acc_train: 0.9960 loss_val: 1.8600 acc_val: 0.7237
Epoch: 0090 loss_train: 0.0178 acc_train: 0.9945 loss_val: 1.8726 acc_val: 0.7267
Epoch: 0091 loss_train: 0.0173 acc_train: 0.9950 loss_val: 1.8876 acc_val: 0.7297
Epoch: 0092 loss_train: 0.0151 acc_train: 0.9950 loss_val: 1.9051 acc_val: 0.7282
Epoch: 0093 loss_train: 0.0131 acc_train: 0.9965 loss_val: 1.9223 acc_val: 0.7282
Optimization Finished!
Train cost: 29.9250s
Loading 43th epoch
Test set results: loss= 0.7103 accuracy= 0.7831
  NumNodes: 3327
  NumEdges: 9228
  NumFeats: 3703
  NumClasses: 6
  NumTrainingSamples: 120
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:354: DGLWarning: DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges
  dgl_warning("DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges")
/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:

	DGLGraph.adjacency_matrix(transpose, scipy_fmt="csr").

  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '
/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)
  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()
TransformerModel(
  (att_embeddings_nope): Linear(in_features=3743, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU(approximate='none')
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=6, bias=True)
)
total params: 4154632
Epoch: 0001 loss_train: 1.8415 acc_train: 0.1127 loss_val: 1.8348 acc_val: 0.0976
Epoch: 0002 loss_train: 1.8342 acc_train: 0.1162 loss_val: 1.8249 acc_val: 0.1096
Epoch: 0003 loss_train: 1.8256 acc_train: 0.1237 loss_val: 1.8104 acc_val: 0.1246
Epoch: 0004 loss_train: 1.8108 acc_train: 0.1447 loss_val: 1.7913 acc_val: 0.1471
Epoch: 0005 loss_train: 1.7936 acc_train: 0.1557 loss_val: 1.7681 acc_val: 0.2177
Epoch: 0006 loss_train: 1.7703 acc_train: 0.2158 loss_val: 1.7412 acc_val: 0.3138
Epoch: 0007 loss_train: 1.7459 acc_train: 0.2994 loss_val: 1.7111 acc_val: 0.3754
Epoch: 0008 loss_train: 1.7187 acc_train: 0.3771 loss_val: 1.6783 acc_val: 0.4414
Epoch: 0009 loss_train: 1.6874 acc_train: 0.4352 loss_val: 1.6435 acc_val: 0.4985
Epoch: 0010 loss_train: 1.6548 acc_train: 0.4922 loss_val: 1.6073 acc_val: 0.5330
Epoch: 0011 loss_train: 1.6184 acc_train: 0.5278 loss_val: 1.5704 acc_val: 0.5480
Epoch: 0012 loss_train: 1.5825 acc_train: 0.5538 loss_val: 1.5330 acc_val: 0.5631
Epoch: 0013 loss_train: 1.5473 acc_train: 0.5749 loss_val: 1.4951 acc_val: 0.5676
Epoch: 0014 loss_train: 1.5069 acc_train: 0.5879 loss_val: 1.4567 acc_val: 0.5781
Epoch: 0015 loss_train: 1.4678 acc_train: 0.6069 loss_val: 1.4178 acc_val: 0.5991
Epoch: 0016 loss_train: 1.4282 acc_train: 0.6154 loss_val: 1.3786 acc_val: 0.6126
Epoch: 0017 loss_train: 1.3884 acc_train: 0.6279 loss_val: 1.3390 acc_val: 0.6186
Epoch: 0018 loss_train: 1.3485 acc_train: 0.6380 loss_val: 1.2994 acc_val: 0.6276
Epoch: 0019 loss_train: 1.3061 acc_train: 0.6470 loss_val: 1.2599 acc_val: 0.6336
Epoch: 0020 loss_train: 1.2658 acc_train: 0.6560 loss_val: 1.2210 acc_val: 0.6426
Epoch: 0021 loss_train: 1.2266 acc_train: 0.6580 loss_val: 1.1829 acc_val: 0.6532
Epoch: 0022 loss_train: 1.1814 acc_train: 0.6715 loss_val: 1.1460 acc_val: 0.6607
Epoch: 0023 loss_train: 1.1430 acc_train: 0.6770 loss_val: 1.1105 acc_val: 0.6697
Epoch: 0024 loss_train: 1.1046 acc_train: 0.6820 loss_val: 1.0767 acc_val: 0.6772
Epoch: 0025 loss_train: 1.0663 acc_train: 0.6870 loss_val: 1.0448 acc_val: 0.6847
Epoch: 0026 loss_train: 1.0288 acc_train: 0.7011 loss_val: 1.0149 acc_val: 0.6967
Epoch: 0027 loss_train: 0.9933 acc_train: 0.7126 loss_val: 0.9872 acc_val: 0.6967
Epoch: 0028 loss_train: 0.9594 acc_train: 0.7156 loss_val: 0.9616 acc_val: 0.7027
Epoch: 0029 loss_train: 0.9246 acc_train: 0.7201 loss_val: 0.9382 acc_val: 0.7012
Epoch: 0030 loss_train: 0.8904 acc_train: 0.7246 loss_val: 0.9172 acc_val: 0.7027
Epoch: 0031 loss_train: 0.8570 acc_train: 0.7301 loss_val: 0.8988 acc_val: 0.7072
Epoch: 0032 loss_train: 0.8281 acc_train: 0.7386 loss_val: 0.8828 acc_val: 0.7057
Epoch: 0033 loss_train: 0.7946 acc_train: 0.7496 loss_val: 0.8689 acc_val: 0.7087
Epoch: 0034 loss_train: 0.7643 acc_train: 0.7536 loss_val: 0.8568 acc_val: 0.7132
Epoch: 0035 loss_train: 0.7383 acc_train: 0.7631 loss_val: 0.8468 acc_val: 0.7177
Epoch: 0036 loss_train: 0.7065 acc_train: 0.7732 loss_val: 0.8388 acc_val: 0.7252
Epoch: 0037 loss_train: 0.6788 acc_train: 0.7782 loss_val: 0.8323 acc_val: 0.7297
Epoch: 0038 loss_train: 0.6515 acc_train: 0.7837 loss_val: 0.8275 acc_val: 0.7387
Epoch: 0039 loss_train: 0.6258 acc_train: 0.7962 loss_val: 0.8250 acc_val: 0.7447
Epoch: 0040 loss_train: 0.6015 acc_train: 0.8062 loss_val: 0.8252 acc_val: 0.7477
Epoch: 0041 loss_train: 0.5780 acc_train: 0.8162 loss_val: 0.8282 acc_val: 0.7432
Epoch: 0042 loss_train: 0.5536 acc_train: 0.8207 loss_val: 0.8340 acc_val: 0.7432
Epoch: 0043 loss_train: 0.5290 acc_train: 0.8272 loss_val: 0.8414 acc_val: 0.7492
Epoch: 0044 loss_train: 0.5095 acc_train: 0.8343 loss_val: 0.8496 acc_val: 0.7417
Epoch: 0045 loss_train: 0.4866 acc_train: 0.8408 loss_val: 0.8590 acc_val: 0.7447
Epoch: 0046 loss_train: 0.4655 acc_train: 0.8468 loss_val: 0.8701 acc_val: 0.7462
Epoch: 0047 loss_train: 0.4442 acc_train: 0.8518 loss_val: 0.8825 acc_val: 0.7387
Epoch: 0048 loss_train: 0.4277 acc_train: 0.8558 loss_val: 0.8950 acc_val: 0.7417
Epoch: 0049 loss_train: 0.4043 acc_train: 0.8628 loss_val: 0.9079 acc_val: 0.7417
Epoch: 0050 loss_train: 0.3865 acc_train: 0.8668 loss_val: 0.9240 acc_val: 0.7447
Epoch: 0051 loss_train: 0.3662 acc_train: 0.8763 loss_val: 0.9433 acc_val: 0.7477
Epoch: 0052 loss_train: 0.3444 acc_train: 0.8843 loss_val: 0.9642 acc_val: 0.7477
Epoch: 0053 loss_train: 0.3272 acc_train: 0.8888 loss_val: 0.9859 acc_val: 0.7447
Epoch: 0054 loss_train: 0.3105 acc_train: 0.8903 loss_val: 1.0079 acc_val: 0.7402
Epoch: 0055 loss_train: 0.2931 acc_train: 0.8998 loss_val: 1.0311 acc_val: 0.7417
Epoch: 0056 loss_train: 0.2742 acc_train: 0.9069 loss_val: 1.0564 acc_val: 0.7402
Epoch: 0057 loss_train: 0.2545 acc_train: 0.9169 loss_val: 1.0821 acc_val: 0.7372
Epoch: 0058 loss_train: 0.2420 acc_train: 0.9194 loss_val: 1.1090 acc_val: 0.7327
Epoch: 0059 loss_train: 0.2218 acc_train: 0.9289 loss_val: 1.1372 acc_val: 0.7357
Epoch: 0060 loss_train: 0.2082 acc_train: 0.9364 loss_val: 1.1665 acc_val: 0.7282
Epoch: 0061 loss_train: 0.1952 acc_train: 0.9414 loss_val: 1.1949 acc_val: 0.7312
Epoch: 0062 loss_train: 0.1804 acc_train: 0.9464 loss_val: 1.2231 acc_val: 0.7327
Epoch: 0063 loss_train: 0.1662 acc_train: 0.9509 loss_val: 1.2510 acc_val: 0.7312
Epoch: 0064 loss_train: 0.1532 acc_train: 0.9569 loss_val: 1.2785 acc_val: 0.7297
Epoch: 0065 loss_train: 0.1369 acc_train: 0.9614 loss_val: 1.3058 acc_val: 0.7327
Epoch: 0066 loss_train: 0.1269 acc_train: 0.9634 loss_val: 1.3340 acc_val: 0.7342
Epoch: 0067 loss_train: 0.1143 acc_train: 0.9680 loss_val: 1.3639 acc_val: 0.7327
Epoch: 0068 loss_train: 0.1069 acc_train: 0.9670 loss_val: 1.3910 acc_val: 0.7327
Epoch: 0069 loss_train: 0.0975 acc_train: 0.9695 loss_val: 1.4196 acc_val: 0.7312
Epoch: 0070 loss_train: 0.0884 acc_train: 0.9750 loss_val: 1.4502 acc_val: 0.7357
Epoch: 0071 loss_train: 0.0793 acc_train: 0.9790 loss_val: 1.4793 acc_val: 0.7297
Epoch: 0072 loss_train: 0.0703 acc_train: 0.9800 loss_val: 1.5082 acc_val: 0.7312
Epoch: 0073 loss_train: 0.0639 acc_train: 0.9815 loss_val: 1.5380 acc_val: 0.7252
Epoch: 0074 loss_train: 0.0567 acc_train: 0.9825 loss_val: 1.5673 acc_val: 0.7282
Epoch: 0075 loss_train: 0.0513 acc_train: 0.9855 loss_val: 1.5962 acc_val: 0.7327
Epoch: 0076 loss_train: 0.0483 acc_train: 0.9865 loss_val: 1.6259 acc_val: 0.7297
Epoch: 0077 loss_train: 0.0438 acc_train: 0.9865 loss_val: 1.6546 acc_val: 0.7267
Epoch: 0078 loss_train: 0.0373 acc_train: 0.9905 loss_val: 1.6819 acc_val: 0.7252
Epoch: 0079 loss_train: 0.0359 acc_train: 0.9900 loss_val: 1.7072 acc_val: 0.7267
Epoch: 0080 loss_train: 0.0308 acc_train: 0.9950 loss_val: 1.7306 acc_val: 0.7252
Epoch: 0081 loss_train: 0.0280 acc_train: 0.9925 loss_val: 1.7545 acc_val: 0.7282
Epoch: 0082 loss_train: 0.0261 acc_train: 0.9935 loss_val: 1.7808 acc_val: 0.7267
Epoch: 0083 loss_train: 0.0230 acc_train: 0.9960 loss_val: 1.8057 acc_val: 0.7267
Epoch: 0084 loss_train: 0.0211 acc_train: 0.9970 loss_val: 1.8267 acc_val: 0.7237
Epoch: 0085 loss_train: 0.0190 acc_train: 0.9965 loss_val: 1.8473 acc_val: 0.7222
Epoch: 0086 loss_train: 0.0174 acc_train: 0.9970 loss_val: 1.8689 acc_val: 0.7267
Epoch: 0087 loss_train: 0.0159 acc_train: 0.9970 loss_val: 1.8889 acc_val: 0.7222
Epoch: 0088 loss_train: 0.0146 acc_train: 0.9970 loss_val: 1.9072 acc_val: 0.7192
Epoch: 0089 loss_train: 0.0137 acc_train: 0.9975 loss_val: 1.9253 acc_val: 0.7192
Epoch: 0090 loss_train: 0.0124 acc_train: 0.9970 loss_val: 1.9404 acc_val: 0.7252
Epoch: 0091 loss_train: 0.0121 acc_train: 0.9970 loss_val: 1.9510 acc_val: 0.7252
Epoch: 0092 loss_train: 0.0104 acc_train: 0.9975 loss_val: 1.9629 acc_val: 0.7252
Epoch: 0093 loss_train: 0.0095 acc_train: 0.9980 loss_val: 1.9786 acc_val: 0.7222
Optimization Finished!
Train cost: 29.8348s
Loading 43th epoch
Test set results: loss= 0.7246 accuracy= 0.7711
  NumNodes: 3327
  NumEdges: 9228
  NumFeats: 3703
  NumClasses: 6
  NumTrainingSamples: 120
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:354: DGLWarning: DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges
  dgl_warning("DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges")
/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:

	DGLGraph.adjacency_matrix(transpose, scipy_fmt="csr").

  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '
/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)
  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()
TransformerModel(
  (att_embeddings_nope): Linear(in_features=3743, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU(approximate='none')
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=6, bias=True)
)
total params: 4154632
Epoch: 0001 loss_train: 1.8184 acc_train: 0.1352 loss_val: 1.8107 acc_val: 0.1607
Epoch: 0002 loss_train: 1.8123 acc_train: 0.1487 loss_val: 1.8006 acc_val: 0.1682
Epoch: 0003 loss_train: 1.8031 acc_train: 0.1567 loss_val: 1.7859 acc_val: 0.1907
Epoch: 0004 loss_train: 1.7885 acc_train: 0.1748 loss_val: 1.7669 acc_val: 0.2538
Epoch: 0005 loss_train: 1.7739 acc_train: 0.2148 loss_val: 1.7442 acc_val: 0.3303
Epoch: 0006 loss_train: 1.7531 acc_train: 0.2974 loss_val: 1.7184 acc_val: 0.3859
Epoch: 0007 loss_train: 1.7291 acc_train: 0.3520 loss_val: 1.6900 acc_val: 0.4294
Epoch: 0008 loss_train: 1.7053 acc_train: 0.3951 loss_val: 1.6595 acc_val: 0.4565
Epoch: 0009 loss_train: 1.6788 acc_train: 0.4367 loss_val: 1.6275 acc_val: 0.4835
Epoch: 0010 loss_train: 1.6484 acc_train: 0.4802 loss_val: 1.5945 acc_val: 0.5180
Epoch: 0011 loss_train: 1.6182 acc_train: 0.4932 loss_val: 1.5605 acc_val: 0.5330
Epoch: 0012 loss_train: 1.5858 acc_train: 0.5013 loss_val: 1.5258 acc_val: 0.5375
Epoch: 0013 loss_train: 1.5558 acc_train: 0.5193 loss_val: 1.4905 acc_val: 0.5495
Epoch: 0014 loss_train: 1.5224 acc_train: 0.5253 loss_val: 1.4546 acc_val: 0.5586
Epoch: 0015 loss_train: 1.4882 acc_train: 0.5363 loss_val: 1.4178 acc_val: 0.5616
Epoch: 0016 loss_train: 1.4544 acc_train: 0.5508 loss_val: 1.3800 acc_val: 0.5661
Epoch: 0017 loss_train: 1.4178 acc_train: 0.5653 loss_val: 1.3417 acc_val: 0.5811
Epoch: 0018 loss_train: 1.3827 acc_train: 0.5679 loss_val: 1.3029 acc_val: 0.5916
Epoch: 0019 loss_train: 1.3422 acc_train: 0.5924 loss_val: 1.2645 acc_val: 0.6036
Epoch: 0020 loss_train: 1.3065 acc_train: 0.6004 loss_val: 1.2274 acc_val: 0.6231
Epoch: 0021 loss_train: 1.2680 acc_train: 0.6104 loss_val: 1.1923 acc_val: 0.6291
Epoch: 0022 loss_train: 1.2313 acc_train: 0.6209 loss_val: 1.1594 acc_val: 0.6366
Epoch: 0023 loss_train: 1.1958 acc_train: 0.6319 loss_val: 1.1288 acc_val: 0.6381
Epoch: 0024 loss_train: 1.1618 acc_train: 0.6415 loss_val: 1.1002 acc_val: 0.6562
Epoch: 0025 loss_train: 1.1263 acc_train: 0.6520 loss_val: 1.0730 acc_val: 0.6757
Epoch: 0026 loss_train: 1.0907 acc_train: 0.6605 loss_val: 1.0471 acc_val: 0.6772
Epoch: 0027 loss_train: 1.0550 acc_train: 0.6710 loss_val: 1.0225 acc_val: 0.6877
Epoch: 0028 loss_train: 1.0172 acc_train: 0.6830 loss_val: 0.9990 acc_val: 0.6922
Epoch: 0029 loss_train: 0.9839 acc_train: 0.6975 loss_val: 0.9766 acc_val: 0.6937
Epoch: 0030 loss_train: 0.9474 acc_train: 0.7061 loss_val: 0.9549 acc_val: 0.6967
Epoch: 0031 loss_train: 0.9108 acc_train: 0.7186 loss_val: 0.9344 acc_val: 0.7042
Epoch: 0032 loss_train: 0.8744 acc_train: 0.7291 loss_val: 0.9153 acc_val: 0.7057
Epoch: 0033 loss_train: 0.8383 acc_train: 0.7441 loss_val: 0.8976 acc_val: 0.7057
Epoch: 0034 loss_train: 0.8035 acc_train: 0.7471 loss_val: 0.8814 acc_val: 0.7042
Epoch: 0035 loss_train: 0.7688 acc_train: 0.7556 loss_val: 0.8682 acc_val: 0.7117
Epoch: 0036 loss_train: 0.7331 acc_train: 0.7636 loss_val: 0.8586 acc_val: 0.7162
Epoch: 0037 loss_train: 0.6979 acc_train: 0.7812 loss_val: 0.8521 acc_val: 0.7192
Epoch: 0038 loss_train: 0.6655 acc_train: 0.7872 loss_val: 0.8481 acc_val: 0.7207
Epoch: 0039 loss_train: 0.6302 acc_train: 0.8047 loss_val: 0.8458 acc_val: 0.7327
Epoch: 0040 loss_train: 0.6027 acc_train: 0.8082 loss_val: 0.8460 acc_val: 0.7312
Epoch: 0041 loss_train: 0.5753 acc_train: 0.8137 loss_val: 0.8499 acc_val: 0.7327
Epoch: 0042 loss_train: 0.5475 acc_train: 0.8192 loss_val: 0.8563 acc_val: 0.7357
Epoch: 0043 loss_train: 0.5167 acc_train: 0.8302 loss_val: 0.8631 acc_val: 0.7357
Epoch: 0044 loss_train: 0.4923 acc_train: 0.8373 loss_val: 0.8719 acc_val: 0.7372
Epoch: 0045 loss_train: 0.4649 acc_train: 0.8448 loss_val: 0.8830 acc_val: 0.7372
Epoch: 0046 loss_train: 0.4394 acc_train: 0.8538 loss_val: 0.8965 acc_val: 0.7387
Epoch: 0047 loss_train: 0.4118 acc_train: 0.8613 loss_val: 0.9103 acc_val: 0.7402
Epoch: 0048 loss_train: 0.3902 acc_train: 0.8643 loss_val: 0.9251 acc_val: 0.7387
Epoch: 0049 loss_train: 0.3620 acc_train: 0.8773 loss_val: 0.9431 acc_val: 0.7387
Epoch: 0050 loss_train: 0.3417 acc_train: 0.8803 loss_val: 0.9641 acc_val: 0.7417
Epoch: 0051 loss_train: 0.3155 acc_train: 0.8963 loss_val: 0.9850 acc_val: 0.7402
Epoch: 0052 loss_train: 0.2915 acc_train: 0.9044 loss_val: 1.0058 acc_val: 0.7447
Epoch: 0053 loss_train: 0.2722 acc_train: 0.9114 loss_val: 1.0289 acc_val: 0.7477
Epoch: 0054 loss_train: 0.2525 acc_train: 0.9164 loss_val: 1.0534 acc_val: 0.7417
Epoch: 0055 loss_train: 0.2333 acc_train: 0.9274 loss_val: 1.0762 acc_val: 0.7432
Epoch: 0056 loss_train: 0.2127 acc_train: 0.9339 loss_val: 1.0986 acc_val: 0.7372
Epoch: 0057 loss_train: 0.1950 acc_train: 0.9399 loss_val: 1.1244 acc_val: 0.7357
Epoch: 0058 loss_train: 0.1810 acc_train: 0.9424 loss_val: 1.1508 acc_val: 0.7387
Epoch: 0059 loss_train: 0.1620 acc_train: 0.9519 loss_val: 1.1739 acc_val: 0.7357
Epoch: 0060 loss_train: 0.1478 acc_train: 0.9554 loss_val: 1.1999 acc_val: 0.7297
Epoch: 0061 loss_train: 0.1361 acc_train: 0.9584 loss_val: 1.2281 acc_val: 0.7267
Epoch: 0062 loss_train: 0.1242 acc_train: 0.9624 loss_val: 1.2533 acc_val: 0.7327
Epoch: 0063 loss_train: 0.1160 acc_train: 0.9629 loss_val: 1.2807 acc_val: 0.7357
Epoch: 0064 loss_train: 0.1030 acc_train: 0.9700 loss_val: 1.3082 acc_val: 0.7357
Epoch: 0065 loss_train: 0.0915 acc_train: 0.9715 loss_val: 1.3351 acc_val: 0.7342
Epoch: 0066 loss_train: 0.0847 acc_train: 0.9775 loss_val: 1.3596 acc_val: 0.7357
Epoch: 0067 loss_train: 0.0746 acc_train: 0.9810 loss_val: 1.3862 acc_val: 0.7312
Epoch: 0068 loss_train: 0.0675 acc_train: 0.9830 loss_val: 1.4113 acc_val: 0.7327
Epoch: 0069 loss_train: 0.0608 acc_train: 0.9845 loss_val: 1.4387 acc_val: 0.7327
Epoch: 0070 loss_train: 0.0564 acc_train: 0.9845 loss_val: 1.4668 acc_val: 0.7312
Epoch: 0071 loss_train: 0.0502 acc_train: 0.9835 loss_val: 1.4937 acc_val: 0.7297
Epoch: 0072 loss_train: 0.0439 acc_train: 0.9880 loss_val: 1.5206 acc_val: 0.7327
Epoch: 0073 loss_train: 0.0406 acc_train: 0.9885 loss_val: 1.5464 acc_val: 0.7372
Epoch: 0074 loss_train: 0.0360 acc_train: 0.9900 loss_val: 1.5708 acc_val: 0.7342
Epoch: 0075 loss_train: 0.0334 acc_train: 0.9915 loss_val: 1.5955 acc_val: 0.7327
Epoch: 0076 loss_train: 0.0314 acc_train: 0.9915 loss_val: 1.6214 acc_val: 0.7327
Epoch: 0077 loss_train: 0.0298 acc_train: 0.9900 loss_val: 1.6479 acc_val: 0.7327
Epoch: 0078 loss_train: 0.0261 acc_train: 0.9925 loss_val: 1.6734 acc_val: 0.7312
Epoch: 0079 loss_train: 0.0250 acc_train: 0.9950 loss_val: 1.6943 acc_val: 0.7327
Epoch: 0080 loss_train: 0.0214 acc_train: 0.9965 loss_val: 1.7124 acc_val: 0.7327
Epoch: 0081 loss_train: 0.0198 acc_train: 0.9945 loss_val: 1.7299 acc_val: 0.7327
Epoch: 0082 loss_train: 0.0180 acc_train: 0.9950 loss_val: 1.7472 acc_val: 0.7312
Epoch: 0083 loss_train: 0.0170 acc_train: 0.9965 loss_val: 1.7652 acc_val: 0.7312
Epoch: 0084 loss_train: 0.0154 acc_train: 0.9975 loss_val: 1.7849 acc_val: 0.7312
Epoch: 0085 loss_train: 0.0147 acc_train: 0.9970 loss_val: 1.8045 acc_val: 0.7297
Epoch: 0086 loss_train: 0.0131 acc_train: 0.9975 loss_val: 1.8231 acc_val: 0.7327
Epoch: 0087 loss_train: 0.0127 acc_train: 0.9975 loss_val: 1.8412 acc_val: 0.7312
Epoch: 0088 loss_train: 0.0111 acc_train: 0.9975 loss_val: 1.8588 acc_val: 0.7327
Epoch: 0089 loss_train: 0.0104 acc_train: 0.9980 loss_val: 1.8742 acc_val: 0.7297
Epoch: 0090 loss_train: 0.0097 acc_train: 0.9985 loss_val: 1.8893 acc_val: 0.7297
Epoch: 0091 loss_train: 0.0090 acc_train: 0.9980 loss_val: 1.9039 acc_val: 0.7297
Epoch: 0092 loss_train: 0.0079 acc_train: 0.9985 loss_val: 1.9177 acc_val: 0.7282
Epoch: 0093 loss_train: 0.0077 acc_train: 0.9980 loss_val: 1.9306 acc_val: 0.7267
Epoch: 0094 loss_train: 0.0075 acc_train: 0.9985 loss_val: 1.9435 acc_val: 0.7267
Epoch: 0095 loss_train: 0.0066 acc_train: 0.9985 loss_val: 1.9557 acc_val: 0.7297
Epoch: 0096 loss_train: 0.0060 acc_train: 0.9990 loss_val: 1.9668 acc_val: 0.7297
Epoch: 0097 loss_train: 0.0055 acc_train: 0.9985 loss_val: 1.9761 acc_val: 0.7267
Epoch: 0098 loss_train: 0.0052 acc_train: 0.9995 loss_val: 1.9844 acc_val: 0.7312
Epoch: 0099 loss_train: 0.0047 acc_train: 0.9995 loss_val: 1.9925 acc_val: 0.7327
Epoch: 0100 loss_train: 0.0047 acc_train: 0.9990 loss_val: 2.0022 acc_val: 0.7327
Epoch: 0101 loss_train: 0.0038 acc_train: 0.9995 loss_val: 2.0112 acc_val: 0.7297
Epoch: 0102 loss_train: 0.0037 acc_train: 0.9995 loss_val: 2.0193 acc_val: 0.7282
Epoch: 0103 loss_train: 0.0039 acc_train: 0.9990 loss_val: 2.0280 acc_val: 0.7297
Optimization Finished!
Train cost: 32.4745s
Loading 53th epoch
Test set results: loss= 0.7563 accuracy= 0.7892
  NumNodes: 3327
  NumEdges: 9228
  NumFeats: 3703
  NumClasses: 6
  NumTrainingSamples: 120
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:354: DGLWarning: DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges
  dgl_warning("DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges")
/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:

	DGLGraph.adjacency_matrix(transpose, scipy_fmt="csr").

  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '
/content/gdrive/MyDrive/FinalProjectGNN/NAGphormer/utils.py:163: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)
  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()
TransformerModel(
  (att_embeddings_nope): Linear(in_features=3743, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU(approximate='none')
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=6, bias=True)
)
total params: 4154632
Epoch: 0001 loss_train: 1.8282 acc_train: 0.1302 loss_val: 1.8213 acc_val: 0.1471
Epoch: 0002 loss_train: 1.8214 acc_train: 0.1417 loss_val: 1.8113 acc_val: 0.1502
Epoch: 0003 loss_train: 1.8145 acc_train: 0.1457 loss_val: 1.7968 acc_val: 0.1622
Epoch: 0004 loss_train: 1.8013 acc_train: 0.1607 loss_val: 1.7779 acc_val: 0.1952
Epoch: 0005 loss_train: 1.7862 acc_train: 0.1768 loss_val: 1.7553 acc_val: 0.2297
Epoch: 0006 loss_train: 1.7666 acc_train: 0.2083 loss_val: 1.7296 acc_val: 0.3348
Epoch: 0007 loss_train: 1.7462 acc_train: 0.2604 loss_val: 1.7013 acc_val: 0.4099
Epoch: 0008 loss_train: 1.7227 acc_train: 0.3365 loss_val: 1.6711 acc_val: 0.4760
Epoch: 0009 loss_train: 1.6996 acc_train: 0.3991 loss_val: 1.6397 acc_val: 0.5090
Epoch: 0010 loss_train: 1.6732 acc_train: 0.4337 loss_val: 1.6071 acc_val: 0.5195
Epoch: 0011 loss_train: 1.6463 acc_train: 0.4567 loss_val: 1.5734 acc_val: 0.5360
Epoch: 0012 loss_train: 1.6169 acc_train: 0.4807 loss_val: 1.5387 acc_val: 0.5330
Epoch: 0013 loss_train: 1.5905 acc_train: 0.4892 loss_val: 1.5027 acc_val: 0.5450
Epoch: 0014 loss_train: 1.5607 acc_train: 0.5018 loss_val: 1.4652 acc_val: 0.5541
Epoch: 0015 loss_train: 1.5275 acc_train: 0.5178 loss_val: 1.4261 acc_val: 0.5706
Epoch: 0016 loss_train: 1.4939 acc_train: 0.5358 loss_val: 1.3859 acc_val: 0.5856
Epoch: 0017 loss_train: 1.4580 acc_train: 0.5388 loss_val: 1.3453 acc_val: 0.5991
Epoch: 0018 loss_train: 1.4222 acc_train: 0.5518 loss_val: 1.3051 acc_val: 0.6111
Epoch: 0019 loss_train: 1.3816 acc_train: 0.5689 loss_val: 1.2662 acc_val: 0.6231
Epoch: 0020 loss_train: 1.3444 acc_train: 0.5859 loss_val: 1.2294 acc_val: 0.6291
Epoch: 0021 loss_train: 1.3074 acc_train: 0.5894 loss_val: 1.1948 acc_val: 0.6351
Epoch: 0022 loss_train: 1.2693 acc_train: 0.6014 loss_val: 1.1620 acc_val: 0.6502
Epoch: 0023 loss_train: 1.2303 acc_train: 0.6149 loss_val: 1.1310 acc_val: 0.6577
Epoch: 0024 loss_train: 1.1962 acc_train: 0.6234 loss_val: 1.1014 acc_val: 0.6667
Epoch: 0025 loss_train: 1.1567 acc_train: 0.6370 loss_val: 1.0732 acc_val: 0.6727
Epoch: 0026 loss_train: 1.1189 acc_train: 0.6485 loss_val: 1.0459 acc_val: 0.6802
Epoch: 0027 loss_train: 1.0813 acc_train: 0.6600 loss_val: 1.0195 acc_val: 0.6847
Epoch: 0028 loss_train: 1.0410 acc_train: 0.6760 loss_val: 0.9942 acc_val: 0.6907
Epoch: 0029 loss_train: 0.9983 acc_train: 0.6925 loss_val: 0.9703 acc_val: 0.6952
Epoch: 0030 loss_train: 0.9609 acc_train: 0.7061 loss_val: 0.9475 acc_val: 0.7042
Epoch: 0031 loss_train: 0.9159 acc_train: 0.7181 loss_val: 0.9257 acc_val: 0.7147
Epoch: 0032 loss_train: 0.8753 acc_train: 0.7291 loss_val: 0.9055 acc_val: 0.7177
Epoch: 0033 loss_train: 0.8338 acc_train: 0.7426 loss_val: 0.8878 acc_val: 0.7237
Epoch: 0034 loss_train: 0.7966 acc_train: 0.7516 loss_val: 0.8735 acc_val: 0.7207
Epoch: 0035 loss_train: 0.7589 acc_train: 0.7656 loss_val: 0.8623 acc_val: 0.7207
Epoch: 0036 loss_train: 0.7183 acc_train: 0.7742 loss_val: 0.8521 acc_val: 0.7297
Epoch: 0037 loss_train: 0.6841 acc_train: 0.7822 loss_val: 0.8443 acc_val: 0.7282
Epoch: 0038 loss_train: 0.6499 acc_train: 0.7942 loss_val: 0.8397 acc_val: 0.7342
Epoch: 0039 loss_train: 0.6154 acc_train: 0.8032 loss_val: 0.8386 acc_val: 0.7357
Epoch: 0040 loss_train: 0.5855 acc_train: 0.8122 loss_val: 0.8397 acc_val: 0.7357
Epoch: 0041 loss_train: 0.5552 acc_train: 0.8242 loss_val: 0.8428 acc_val: 0.7327
Epoch: 0042 loss_train: 0.5295 acc_train: 0.8292 loss_val: 0.8473 acc_val: 0.7312
Epoch: 0043 loss_train: 0.4977 acc_train: 0.8368 loss_val: 0.8535 acc_val: 0.7327
Epoch: 0044 loss_train: 0.4730 acc_train: 0.8423 loss_val: 0.8613 acc_val: 0.7372
Epoch: 0045 loss_train: 0.4441 acc_train: 0.8518 loss_val: 0.8714 acc_val: 0.7372
Epoch: 0046 loss_train: 0.4164 acc_train: 0.8628 loss_val: 0.8828 acc_val: 0.7357
Epoch: 0047 loss_train: 0.3905 acc_train: 0.8733 loss_val: 0.8953 acc_val: 0.7357
Epoch: 0048 loss_train: 0.3684 acc_train: 0.8783 loss_val: 0.9101 acc_val: 0.7357
Epoch: 0049 loss_train: 0.3421 acc_train: 0.8863 loss_val: 0.9280 acc_val: 0.7312
Epoch: 0050 loss_train: 0.3201 acc_train: 0.8953 loss_val: 0.9482 acc_val: 0.7327
Epoch: 0051 loss_train: 0.2956 acc_train: 0.9024 loss_val: 0.9685 acc_val: 0.7312
Epoch: 0052 loss_train: 0.2727 acc_train: 0.9129 loss_val: 0.9915 acc_val: 0.7327
Epoch: 0053 loss_train: 0.2528 acc_train: 0.9179 loss_val: 1.0164 acc_val: 0.7342
Epoch: 0054 loss_train: 0.2348 acc_train: 0.9219 loss_val: 1.0383 acc_val: 0.7387
Epoch: 0055 loss_train: 0.2149 acc_train: 0.9324 loss_val: 1.0613 acc_val: 0.7387
Epoch: 0056 loss_train: 0.1997 acc_train: 0.9374 loss_val: 1.0862 acc_val: 0.7402
Epoch: 0057 loss_train: 0.1824 acc_train: 0.9419 loss_val: 1.1117 acc_val: 0.7492
Epoch: 0058 loss_train: 0.1681 acc_train: 0.9489 loss_val: 1.1367 acc_val: 0.7523
Epoch: 0059 loss_train: 0.1526 acc_train: 0.9544 loss_val: 1.1622 acc_val: 0.7477
Epoch: 0060 loss_train: 0.1378 acc_train: 0.9624 loss_val: 1.1918 acc_val: 0.7432
Epoch: 0061 loss_train: 0.1276 acc_train: 0.9604 loss_val: 1.2229 acc_val: 0.7387
Epoch: 0062 loss_train: 0.1161 acc_train: 0.9690 loss_val: 1.2501 acc_val: 0.7402
Epoch: 0063 loss_train: 0.1082 acc_train: 0.9710 loss_val: 1.2793 acc_val: 0.7372
Epoch: 0064 loss_train: 0.0976 acc_train: 0.9730 loss_val: 1.3113 acc_val: 0.7297
Epoch: 0065 loss_train: 0.0875 acc_train: 0.9770 loss_val: 1.3430 acc_val: 0.7327
Epoch: 0066 loss_train: 0.0799 acc_train: 0.9800 loss_val: 1.3731 acc_val: 0.7312
Epoch: 0067 loss_train: 0.0707 acc_train: 0.9845 loss_val: 1.4054 acc_val: 0.7327
Epoch: 0068 loss_train: 0.0646 acc_train: 0.9835 loss_val: 1.4342 acc_val: 0.7342
Epoch: 0069 loss_train: 0.0591 acc_train: 0.9825 loss_val: 1.4620 acc_val: 0.7327
Epoch: 0070 loss_train: 0.0550 acc_train: 0.9845 loss_val: 1.4922 acc_val: 0.7357
Epoch: 0071 loss_train: 0.0510 acc_train: 0.9830 loss_val: 1.5213 acc_val: 0.7372
Epoch: 0072 loss_train: 0.0460 acc_train: 0.9845 loss_val: 1.5507 acc_val: 0.7342
Epoch: 0073 loss_train: 0.0427 acc_train: 0.9855 loss_val: 1.5797 acc_val: 0.7357
Epoch: 0074 loss_train: 0.0377 acc_train: 0.9905 loss_val: 1.6050 acc_val: 0.7357
Epoch: 0075 loss_train: 0.0346 acc_train: 0.9910 loss_val: 1.6284 acc_val: 0.7342
Epoch: 0076 loss_train: 0.0325 acc_train: 0.9920 loss_val: 1.6526 acc_val: 0.7297
Epoch: 0077 loss_train: 0.0299 acc_train: 0.9915 loss_val: 1.6792 acc_val: 0.7312
Epoch: 0078 loss_train: 0.0269 acc_train: 0.9915 loss_val: 1.7054 acc_val: 0.7327
Epoch: 0079 loss_train: 0.0250 acc_train: 0.9935 loss_val: 1.7290 acc_val: 0.7342
Epoch: 0080 loss_train: 0.0227 acc_train: 0.9925 loss_val: 1.7514 acc_val: 0.7312
Epoch: 0081 loss_train: 0.0207 acc_train: 0.9940 loss_val: 1.7734 acc_val: 0.7327
Epoch: 0082 loss_train: 0.0192 acc_train: 0.9945 loss_val: 1.7957 acc_val: 0.7327
Epoch: 0083 loss_train: 0.0178 acc_train: 0.9955 loss_val: 1.8177 acc_val: 0.7327
Epoch: 0084 loss_train: 0.0172 acc_train: 0.9940 loss_val: 1.8382 acc_val: 0.7327
Epoch: 0085 loss_train: 0.0154 acc_train: 0.9960 loss_val: 1.8553 acc_val: 0.7312
Epoch: 0086 loss_train: 0.0136 acc_train: 0.9965 loss_val: 1.8711 acc_val: 0.7327
Epoch: 0087 loss_train: 0.0131 acc_train: 0.9970 loss_val: 1.8890 acc_val: 0.7327
Epoch: 0088 loss_train: 0.0123 acc_train: 0.9970 loss_val: 1.9065 acc_val: 0.7342
Epoch: 0089 loss_train: 0.0113 acc_train: 0.9975 loss_val: 1.9253 acc_val: 0.7312
Epoch: 0090 loss_train: 0.0109 acc_train: 0.9975 loss_val: 1.9422 acc_val: 0.7327
Epoch: 0091 loss_train: 0.0103 acc_train: 0.9985 loss_val: 1.9536 acc_val: 0.7327
Epoch: 0092 loss_train: 0.0086 acc_train: 0.9990 loss_val: 1.9641 acc_val: 0.7312
Epoch: 0093 loss_train: 0.0087 acc_train: 0.9980 loss_val: 1.9804 acc_val: 0.7312
Epoch: 0094 loss_train: 0.0085 acc_train: 0.9975 loss_val: 1.9963 acc_val: 0.7312
Epoch: 0095 loss_train: 0.0069 acc_train: 0.9985 loss_val: 2.0102 acc_val: 0.7282
Epoch: 0096 loss_train: 0.0067 acc_train: 0.9985 loss_val: 2.0180 acc_val: 0.7312
Epoch: 0097 loss_train: 0.0062 acc_train: 0.9980 loss_val: 2.0253 acc_val: 0.7357
Epoch: 0098 loss_train: 0.0062 acc_train: 0.9985 loss_val: 2.0356 acc_val: 0.7357
Epoch: 0099 loss_train: 0.0054 acc_train: 0.9980 loss_val: 2.0469 acc_val: 0.7357
Epoch: 0100 loss_train: 0.0051 acc_train: 0.9985 loss_val: 2.0550 acc_val: 0.7327
Epoch: 0101 loss_train: 0.0047 acc_train: 0.9990 loss_val: 2.0645 acc_val: 0.7327
Epoch: 0102 loss_train: 0.0042 acc_train: 0.9990 loss_val: 2.0754 acc_val: 0.7342
Epoch: 0103 loss_train: 0.0045 acc_train: 0.9985 loss_val: 2.0867 acc_val: 0.7327
Epoch: 0104 loss_train: 0.0040 acc_train: 0.9990 loss_val: 2.0959 acc_val: 0.7327
Epoch: 0105 loss_train: 0.0038 acc_train: 0.9985 loss_val: 2.1048 acc_val: 0.7297
Epoch: 0106 loss_train: 0.0036 acc_train: 0.9985 loss_val: 2.1136 acc_val: 0.7297
Epoch: 0107 loss_train: 0.0035 acc_train: 0.9995 loss_val: 2.1189 acc_val: 0.7312
Epoch: 0108 loss_train: 0.0033 acc_train: 0.9990 loss_val: 2.1215 acc_val: 0.7297
Optimization Finished!
Train cost: 34.7586s
Loading 58th epoch
Test set results: loss= 0.8748 accuracy= 0.7696